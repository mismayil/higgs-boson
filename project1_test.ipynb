{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y_train, X_train, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import preprocess\n",
    "tX_train, ty_train, tX_test, ty_test, cont_cols = preprocess(X_train, y_train, X_test, encodable_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 35), (568238, 35))"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train.shape, tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy : 71.3685% reached at epoch 99\n",
      "Best Accuracy : 71.11800000000001% reached at epoch 99\n",
      "Best Accuracy : 70.49499999999999% reached at epoch 99\n",
      "Best Accuracy : 71.662% reached at epoch 99\n",
      "Best Accuracy : 69.922% reached at epoch 99\n",
      "Best Accuracy : 72.0095% reached at epoch 99\n",
      "Best Accuracy : 70.56949999999999% reached at epoch 99\n",
      "Best Accuracy : 71.816% reached at epoch 99\n",
      "Best Accuracy : 71.58250000000001% reached at epoch 99\n",
      "Best Accuracy : 71.456% reached at epoch 99\n",
      "Best Accuracy : 69.896% reached at epoch 99\n",
      "Best Accuracy : 71.316% reached at epoch 99\n",
      "Best Accuracy : 69.51899999999999% reached at epoch 99\n",
      "Best Accuracy : 71.119% reached at epoch 99\n",
      "Best Accuracy : 70.487% reached at epoch 99\n",
      "Best Accuracy : 71.7565% reached at epoch 81\n",
      "Best Accuracy : 71.1895% reached at epoch 99\n",
      "Best Accuracy : 71.143% reached at epoch 98\n",
      "Best Accuracy : 71.185% reached at epoch 99\n",
      "Best Accuracy : 71.5565% reached at epoch 69\n",
      "Best Accuracy : 69.45599999999999% reached at epoch 45\n",
      "Best Accuracy : 69.5205% reached at epoch 22\n",
      "Best Accuracy : 69.369% reached at epoch 28\n",
      "Best Accuracy : 69.3965% reached at epoch 31\n",
      "Best Accuracy : 69.4885% reached at epoch 41\n",
      "Best Accuracy : 76.067% reached at epoch 99\n",
      "Best Accuracy : 76.7515% reached at epoch 99\n",
      "Best Accuracy : 75.41600000000001% reached at epoch 99\n",
      "Best Accuracy : 76.2295% reached at epoch 99\n",
      "Best Accuracy : 76.8645% reached at epoch 99\n",
      "Best Accuracy : 77.4645% reached at epoch 99\n",
      "Best Accuracy : 75.4745% reached at epoch 99\n",
      "Best Accuracy : 75.35849999999999% reached at epoch 99\n",
      "Best Accuracy : 76.31700000000001% reached at epoch 99\n",
      "Best Accuracy : 75.213% reached at epoch 99\n",
      "Best Accuracy : 77.6405% reached at epoch 99\n",
      "Best Accuracy : 76.452% reached at epoch 99\n",
      "Best Accuracy : 76.786% reached at epoch 99\n",
      "Best Accuracy : 76.7385% reached at epoch 99\n",
      "Best Accuracy : 75.921% reached at epoch 99\n",
      "Best Accuracy : 76.952% reached at epoch 97\n",
      "Best Accuracy : 76.838% reached at epoch 99\n",
      "Best Accuracy : 76.776% reached at epoch 96\n",
      "Best Accuracy : 77.0185% reached at epoch 87\n",
      "Best Accuracy : 76.941% reached at epoch 85\n",
      "Best Accuracy : 71.69% reached at epoch 12\n",
      "Best Accuracy : 70.5035% reached at epoch 58\n",
      "Best Accuracy : 70.9025% reached at epoch 12\n",
      "Best Accuracy : 71.1395% reached at epoch 12\n",
      "Best Accuracy : 71.0505% reached at epoch 12\n",
      "Best Accuracy : 74.878% reached at epoch 95\n",
      "Best Accuracy : 75.351% reached at epoch 96\n",
      "Best Accuracy : 75.93299999999999% reached at epoch 97\n",
      "Best Accuracy : 75.15650000000001% reached at epoch 97\n",
      "Best Accuracy : 75.734% reached at epoch 93\n",
      "Best Accuracy : 74.4175% reached at epoch 98\n",
      "Best Accuracy : 75.6065% reached at epoch 97\n",
      "Best Accuracy : 75.9505% reached at epoch 98\n",
      "Best Accuracy : 76.653% reached at epoch 96\n",
      "Best Accuracy : 76.54950000000001% reached at epoch 98\n",
      "Best Accuracy : 75.85499999999999% reached at epoch 96\n",
      "Best Accuracy : 76.604% reached at epoch 92\n",
      "Best Accuracy : 76.58500000000001% reached at epoch 99\n",
      "Best Accuracy : 75.872% reached at epoch 92\n",
      "Best Accuracy : 75.464% reached at epoch 96\n",
      "Best Accuracy : 75.5945% reached at epoch 88\n",
      "Best Accuracy : 75.75% reached at epoch 94\n",
      "Best Accuracy : 75.67099999999999% reached at epoch 78\n",
      "Best Accuracy : 76.05% reached at epoch 73\n",
      "Best Accuracy : 75.9305% reached at epoch 86\n",
      "Best Accuracy : 70.73899999999999% reached at epoch 52\n",
      "Best Accuracy : 71.016% reached at epoch 44\n",
      "Best Accuracy : 72.039% reached at epoch 29\n",
      "Best Accuracy : 71.1015% reached at epoch 44\n",
      "Best Accuracy : 70.532% reached at epoch 74\n",
      "Best Accuracy : 67.8995% reached at epoch 99\n",
      "Best Accuracy : 67.7065% reached at epoch 99\n",
      "Best Accuracy : 74.694% reached at epoch 83\n",
      "Best Accuracy : 68.0615% reached at epoch 98\n",
      "Best Accuracy : 67.3765% reached at epoch 97\n",
      "Best Accuracy : 68.246% reached at epoch 99\n",
      "Best Accuracy : 68.0395% reached at epoch 99\n",
      "Best Accuracy : 75.302% reached at epoch 94\n",
      "Best Accuracy : 68.32350000000001% reached at epoch 99\n",
      "Best Accuracy : 67.23649999999999% reached at epoch 99\n",
      "Best Accuracy : 69.4915% reached at epoch 99\n",
      "Best Accuracy : 68.4675% reached at epoch 98\n",
      "Best Accuracy : 74.8235% reached at epoch 95\n",
      "Best Accuracy : 69.428% reached at epoch 99\n",
      "Best Accuracy : 68.827% reached at epoch 98\n",
      "Best Accuracy : 73.0155% reached at epoch 75\n",
      "Best Accuracy : 74.20949999999999% reached at epoch 81\n",
      "Best Accuracy : 73.9575% reached at epoch 75\n",
      "Best Accuracy : 74.33800000000001% reached at epoch 79\n",
      "Best Accuracy : 72.481% reached at epoch 81\n",
      "Best Accuracy : 66.9855% reached at epoch 84\n",
      "Best Accuracy : 68.71350000000001% reached at epoch 32\n",
      "Best Accuracy : 68.928% reached at epoch 94\n",
      "Best Accuracy : 68.4365% reached at epoch 64\n",
      "Best Accuracy : 68.681% reached at epoch 80\n",
      "Best Accuracy : 56.413000000000004% reached at epoch 35\n",
      "Best Accuracy : 56.46099999999999% reached at epoch 35\n",
      "Best Accuracy : 71.8955% reached at epoch 33\n",
      "Best Accuracy : 56.269499999999994% reached at epoch 29\n",
      "Best Accuracy : 56.668% reached at epoch 32\n",
      "Best Accuracy : 56.3825% reached at epoch 36\n",
      "Best Accuracy : 56.462500000000006% reached at epoch 26\n",
      "Best Accuracy : 71.9755% reached at epoch 77\n",
      "Best Accuracy : 56.2355% reached at epoch 27\n",
      "Best Accuracy : 56.6805% reached at epoch 32\n",
      "Best Accuracy : 56.41199999999999% reached at epoch 35\n",
      "Best Accuracy : 56.413000000000004% reached at epoch 26\n",
      "Best Accuracy : 71.825% reached at epoch 50\n",
      "Best Accuracy : 56.255% reached at epoch 30\n",
      "Best Accuracy : 56.647000000000006% reached at epoch 31\n",
      "Best Accuracy : 56.836% reached at epoch 97\n",
      "Best Accuracy : 56.50150000000001% reached at epoch 28\n",
      "Best Accuracy : 71.745% reached at epoch 74\n",
      "Best Accuracy : 56.3715% reached at epoch 41\n",
      "Best Accuracy : 56.799% reached at epoch 98\n",
      "Best Accuracy : 58.957% reached at epoch 50\n",
      "Best Accuracy : 66.5795% reached at epoch 84\n",
      "Best Accuracy : 66.6125% reached at epoch 95\n",
      "Best Accuracy : 63.4265% reached at epoch 27\n",
      "Best Accuracy : 66.9225% reached at epoch 84\n",
      "Best Accuracy : 65.602% reached at epoch 1\n",
      "Best Accuracy : 65.7255% reached at epoch 1\n",
      "Best Accuracy : 66.5875% reached at epoch 2\n",
      "Best Accuracy : 65.956% reached at epoch 1\n",
      "Best Accuracy : 65.866% reached at epoch 1\n",
      "Best Accuracy : 65.6525% reached at epoch 1\n",
      "Best Accuracy : 65.6395% reached at epoch 1\n",
      "Best Accuracy : 66.384% reached at epoch 2\n",
      "Best Accuracy : 65.87349999999999% reached at epoch 1\n",
      "Best Accuracy : 66.025% reached at epoch 1\n",
      "Best Accuracy : 65.7285% reached at epoch 1\n",
      "Best Accuracy : 65.6215% reached at epoch 1\n",
      "Best Accuracy : 67.22399999999999% reached at epoch 99\n",
      "Best Accuracy : 65.82050000000001% reached at epoch 1\n",
      "Best Accuracy : 65.79650000000001% reached at epoch 1\n",
      "Best Accuracy : 65.716% reached at epoch 1\n",
      "Best Accuracy : 65.9455% reached at epoch 1\n",
      "Best Accuracy : 67.484% reached at epoch 36\n",
      "Best Accuracy : 65.794% reached at epoch 1\n",
      "Best Accuracy : 65.759% reached at epoch 1\n",
      "Best Accuracy : 67.3525% reached at epoch 39\n",
      "Best Accuracy : 67.262% reached at epoch 39\n",
      "Best Accuracy : 67.8545% reached at epoch 10\n",
      "Best Accuracy : 65.755% reached at epoch 1\n",
      "Best Accuracy : 66.9005% reached at epoch 39\n"
     ]
    }
   ],
   "source": [
    "from implementations import logistic_regression_cv\n",
    "weights, loss, lambda_, degree, accuracy, f1 = logistic_regression_cv(ty_train, tX_train, cont_cols=cont_cols, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.24183977],\n",
       "        [-0.26544805],\n",
       "        [ 0.37138584],\n",
       "        [ 0.30515328],\n",
       "        [ 0.91327956],\n",
       "        [ 0.17158336],\n",
       "        [ 0.53986589],\n",
       "        [-0.36120854],\n",
       "        [ 0.19929079],\n",
       "        [ 0.35773507],\n",
       "        [-0.06072972],\n",
       "        [ 0.29187704],\n",
       "        [ 0.12951111],\n",
       "        [ 0.11767485],\n",
       "        [ 0.33113117],\n",
       "        [ 0.20211105],\n",
       "        [ 0.23329362],\n",
       "        [-0.09106707],\n",
       "        [ 0.21601779],\n",
       "        [-0.04322265],\n",
       "        [-0.05206696],\n",
       "        [ 0.23535913],\n",
       "        [ 0.1280109 ],\n",
       "        [ 0.27909552],\n",
       "        [-0.22777135],\n",
       "        [ 0.03490579],\n",
       "        [-0.60286866],\n",
       "        [ 0.16207475],\n",
       "        [-0.32180454],\n",
       "        [-0.01908622],\n",
       "        [-0.02063272],\n",
       "        [-0.01574469],\n",
       "        [ 0.31183612],\n",
       "        [-0.03238049],\n",
       "        [-0.04039736],\n",
       "        [ 0.07057688],\n",
       "        [ 0.02495367],\n",
       "        [-0.17392018],\n",
       "        [ 0.02440933],\n",
       "        [-0.00554544],\n",
       "        [-0.10049804],\n",
       "        [-0.03536644],\n",
       "        [-0.26242361],\n",
       "        [-0.11389735],\n",
       "        [-0.24725652],\n",
       "        [-0.08161609],\n",
       "        [ 0.17138845],\n",
       "        [-0.14844606]]),\n",
       " 0.48650376615578345,\n",
       " 0.01,\n",
       " 2,\n",
       " 76.80879999999999,\n",
       " 0.7134537923821604)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, loss, lambda_, degree, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/1000\n",
      "Accuracy = 34.28%\n",
      "Loss = 6.553596071063578\n",
      "\n",
      "\n",
      "Iteration 10/1000\n",
      "Accuracy = 59.205200000000005%\n",
      "Loss = 1.4241444661980058\n",
      "\n",
      "\n",
      "Iteration 20/1000\n",
      "Accuracy = 66.754%\n",
      "Loss = 0.8678075379235216\n",
      "\n",
      "\n",
      "Iteration 30/1000\n",
      "Accuracy = 68.7688%\n",
      "Loss = 0.7323285444073929\n",
      "\n",
      "\n",
      "Iteration 40/1000\n",
      "Accuracy = 70.456%\n",
      "Loss = 0.6423609852661826\n",
      "\n",
      "\n",
      "Iteration 50/1000\n",
      "Accuracy = 72.1308%\n",
      "Loss = 0.5831094012354116\n",
      "\n",
      "\n",
      "Iteration 60/1000\n",
      "Accuracy = 73.66120000000001%\n",
      "Loss = 0.5442413431628939\n",
      "\n",
      "\n",
      "Iteration 70/1000\n",
      "Accuracy = 74.99040000000001%\n",
      "Loss = 0.5183459331903096\n",
      "\n",
      "\n",
      "Iteration 80/1000\n",
      "Accuracy = 76.0272%\n",
      "Loss = 0.5006872535966626\n",
      "\n",
      "\n",
      "Iteration 90/1000\n",
      "Accuracy = 76.8432%\n",
      "Loss = 0.48844500430059234\n",
      "\n",
      "\n",
      "Iteration 100/1000\n",
      "Accuracy = 77.422%\n",
      "Loss = 0.4798351283071654\n",
      "\n",
      "\n",
      "Iteration 110/1000\n",
      "Accuracy = 77.8452%\n",
      "Loss = 0.4737060900011065\n",
      "\n",
      "\n",
      "Iteration 120/1000\n",
      "Accuracy = 78.2252%\n",
      "Loss = 0.4693243317826708\n",
      "\n",
      "\n",
      "Iteration 130/1000\n",
      "Accuracy = 78.4676%\n",
      "Loss = 0.4661643137899054\n",
      "\n",
      "\n",
      "Iteration 140/1000\n",
      "Accuracy = 78.5768%\n",
      "Loss = 0.4638500933659703\n",
      "\n",
      "\n",
      "Iteration 150/1000\n",
      "Accuracy = 78.6912%\n",
      "Loss = 0.46213661353502256\n",
      "\n",
      "\n",
      "Iteration 160/1000\n",
      "Accuracy = 78.7804%\n",
      "Loss = 0.46085837968841703\n",
      "\n",
      "\n",
      "Iteration 170/1000\n",
      "Accuracy = 78.836%\n",
      "Loss = 0.4598999182151839\n",
      "\n",
      "\n",
      "Iteration 180/1000\n",
      "Accuracy = 78.8732%\n",
      "Loss = 0.4591773589019407\n",
      "\n",
      "\n",
      "Iteration 190/1000\n",
      "Accuracy = 78.92479999999999%\n",
      "Loss = 0.4586290259109501\n",
      "\n",
      "\n",
      "Iteration 200/1000\n",
      "Accuracy = 78.946%\n",
      "Loss = 0.45821041341616464\n",
      "\n",
      "\n",
      "Iteration 210/1000\n",
      "Accuracy = 78.9628%\n",
      "Loss = 0.4578890683054508\n",
      "\n",
      "\n",
      "Iteration 220/1000\n",
      "Accuracy = 78.97800000000001%\n",
      "Loss = 0.4576425360537488\n",
      "\n",
      "\n",
      "Iteration 230/1000\n",
      "Accuracy = 78.9828%\n",
      "Loss = 0.45745404004707496\n",
      "\n",
      "\n",
      "Iteration 240/1000\n",
      "Accuracy = 79.008%\n",
      "Loss = 0.4573101930734343\n",
      "\n",
      "\n",
      "Iteration 250/1000\n",
      "Accuracy = 79.002%\n",
      "Loss = 0.45720078538510167\n",
      "\n",
      "\n",
      "Iteration 260/1000\n",
      "Accuracy = 79.0164%\n",
      "Loss = 0.4571151783849646\n",
      "\n",
      "\n",
      "Iteration 270/1000\n",
      "Accuracy = 79.02120000000001%\n",
      "Loss = 0.4570487614948407\n",
      "\n",
      "\n",
      "Iteration 280/1000\n",
      "Accuracy = 79.0164%\n",
      "Loss = 0.45699798305335837\n",
      "\n",
      "\n",
      "Iteration 290/1000\n",
      "Accuracy = 79.008%\n",
      "Loss = 0.45695900496200553\n",
      "\n",
      "\n",
      "Iteration 300/1000\n",
      "Accuracy = 79.006%\n",
      "Loss = 0.4569286979013683\n",
      "\n",
      "\n",
      "Iteration 310/1000\n",
      "Accuracy = 79.0076%\n",
      "Loss = 0.45690348880544124\n",
      "\n",
      "\n",
      "Iteration 320/1000\n",
      "Accuracy = 79.0072%\n",
      "Loss = 0.45688338253844146\n",
      "\n",
      "\n",
      "Iteration 330/1000\n",
      "Accuracy = 79.0028%\n",
      "Loss = 0.45686726066973066\n",
      "\n",
      "\n",
      "Iteration 340/1000\n",
      "Accuracy = 79.0096%\n",
      "Loss = 0.4568543078238424\n",
      "\n",
      "\n",
      "Iteration 350/1000\n",
      "Accuracy = 79.0056%\n",
      "Loss = 0.4568437173372246\n",
      "\n",
      "\n",
      "Iteration 360/1000\n",
      "Accuracy = 79.0076%\n",
      "Loss = 0.4568350342086955\n",
      "\n",
      "\n",
      "Iteration 370/1000\n",
      "Accuracy = 79.002%\n",
      "Loss = 0.4568277417390936\n",
      "\n",
      "\n",
      "Iteration 380/1000\n",
      "Accuracy = 79.0136%\n",
      "Loss = 0.45682126269562784\n",
      "\n",
      "\n",
      "Iteration 390/1000\n",
      "Accuracy = 79.0152%\n",
      "Loss = 0.4568153053563841\n",
      "\n",
      "\n",
      "Iteration 400/1000\n",
      "Accuracy = 79.0148%\n",
      "Loss = 0.4568098138917241\n",
      "\n",
      "\n",
      "Iteration 410/1000\n",
      "Accuracy = 79.0148%\n",
      "Loss = 0.4568046050675555\n",
      "\n",
      "\n",
      "Iteration 420/1000\n",
      "Accuracy = 79.0172%\n",
      "Loss = 0.45679954046889715\n",
      "\n",
      "\n",
      "Iteration 430/1000\n",
      "Accuracy = 79.02080000000001%\n",
      "Loss = 0.45679451696247664\n",
      "\n",
      "\n",
      "Iteration 440/1000\n",
      "Accuracy = 79.0216%\n",
      "Loss = 0.45678945913626023\n",
      "\n",
      "\n",
      "Iteration 450/1000\n",
      "Accuracy = 79.0292%\n",
      "Loss = 0.45678421737625857\n",
      "\n",
      "\n",
      "Iteration 460/1000\n",
      "Accuracy = 79.0288%\n",
      "Loss = 0.45677876115386423\n",
      "\n",
      "\n",
      "Iteration 470/1000\n",
      "Accuracy = 79.02799999999999%\n",
      "Loss = 0.4567730737137044\n",
      "\n",
      "\n",
      "Iteration 480/1000\n",
      "Accuracy = 79.02799999999999%\n",
      "Loss = 0.4567671466261867\n",
      "\n",
      "\n",
      "Iteration 490/1000\n",
      "Accuracy = 79.03559999999999%\n",
      "Loss = 0.45676068017813115\n",
      "\n",
      "\n",
      "Iteration 500/1000\n",
      "Accuracy = 79.03320000000001%\n",
      "Loss = 0.45675377397981887\n",
      "\n",
      "\n",
      "Iteration 510/1000\n",
      "Accuracy = 79.0392%\n",
      "Loss = 0.45674676336903275\n",
      "\n",
      "\n",
      "Iteration 520/1000\n",
      "Accuracy = 79.0388%\n",
      "Loss = 0.45673965309070785\n",
      "\n",
      "\n",
      "Iteration 530/1000\n",
      "Accuracy = 79.0336%\n",
      "Loss = 0.45673245040705546\n",
      "\n",
      "\n",
      "Iteration 540/1000\n",
      "Accuracy = 79.0368%\n",
      "Loss = 0.4567251643524251\n",
      "\n",
      "\n",
      "Iteration 550/1000\n",
      "Accuracy = 79.0428%\n",
      "Loss = 0.4567178051475048\n",
      "\n",
      "\n",
      "Iteration 560/1000\n",
      "Accuracy = 79.0412%\n",
      "Loss = 0.45671038374128753\n",
      "\n",
      "\n",
      "Iteration 570/1000\n",
      "Accuracy = 79.0412%\n",
      "Loss = 0.4567029114548833\n",
      "\n",
      "\n",
      "Iteration 580/1000\n",
      "Accuracy = 79.038%\n",
      "Loss = 0.45669539970645334\n",
      "\n",
      "\n",
      "Iteration 590/1000\n",
      "Accuracy = 79.0384%\n",
      "Loss = 0.45668785980172244\n",
      "\n",
      "\n",
      "Iteration 600/1000\n",
      "Accuracy = 79.0416%\n",
      "Loss = 0.45668030277630567\n",
      "\n",
      "\n",
      "Iteration 610/1000\n",
      "Accuracy = 79.0432%\n",
      "Loss = 0.45667273927978635\n",
      "\n",
      "\n",
      "Iteration 620/1000\n",
      "Accuracy = 79.0464%\n",
      "Loss = 0.4566651794915801\n",
      "\n",
      "\n",
      "Iteration 630/1000\n",
      "Accuracy = 79.0484%\n",
      "Loss = 0.45665763306474766\n",
      "\n",
      "\n",
      "Iteration 640/1000\n",
      "Accuracy = 79.04759999999999%\n",
      "Loss = 0.4566501090884826\n",
      "\n",
      "\n",
      "Iteration 650/1000\n",
      "Accuracy = 79.04480000000001%\n",
      "Loss = 0.45664261606808265\n",
      "\n",
      "\n",
      "Iteration 660/1000\n",
      "Accuracy = 79.0484%\n",
      "Loss = 0.45663516191753695\n",
      "\n",
      "\n",
      "Iteration 670/1000\n",
      "Accuracy = 79.05%\n",
      "Loss = 0.45662775396020483\n",
      "\n",
      "\n",
      "Iteration 680/1000\n",
      "Accuracy = 79.04759999999999%\n",
      "Loss = 0.45662039894009016\n",
      "\n",
      "\n",
      "Iteration 690/1000\n",
      "Accuracy = 79.0508%\n",
      "Loss = 0.4566131030348706\n",
      "\n",
      "\n",
      "Iteration 700/1000\n",
      "Accuracy = 79.05%\n",
      "Loss = 0.4566058718764602\n",
      "\n",
      "\n",
      "Iteration 710/1000\n",
      "Accuracy = 79.0496%\n",
      "Loss = 0.4565987105722404\n",
      "\n",
      "\n",
      "Iteration 720/1000\n",
      "Accuracy = 79.0492%\n",
      "Loss = 0.4565916160451824\n",
      "\n",
      "\n",
      "Iteration 730/1000\n",
      "Accuracy = 79.046%\n",
      "Loss = 0.4565844984234918\n",
      "\n",
      "\n",
      "Iteration 740/1000\n",
      "Accuracy = 79.0488%\n",
      "Loss = 0.4565774675804958\n",
      "\n",
      "\n",
      "Iteration 750/1000\n",
      "Accuracy = 79.05%\n",
      "Loss = 0.4565705265628872\n",
      "\n",
      "\n",
      "Iteration 760/1000\n",
      "Accuracy = 79.0504%\n",
      "Loss = 0.45656357786453405\n",
      "\n",
      "\n",
      "Iteration 770/1000\n",
      "Accuracy = 79.0516%\n",
      "Loss = 0.4565566240455394\n",
      "\n",
      "\n",
      "Iteration 780/1000\n",
      "Accuracy = 79.0532%\n",
      "Loss = 0.45654977716588063\n",
      "\n",
      "\n",
      "Iteration 790/1000\n",
      "Accuracy = 79.0528%\n",
      "Loss = 0.4565430383310169\n",
      "\n",
      "\n",
      "Iteration 800/1000\n",
      "Accuracy = 79.05720000000001%\n",
      "Loss = 0.45653640838507026\n",
      "\n",
      "\n",
      "Iteration 810/1000\n",
      "Accuracy = 79.05720000000001%\n",
      "Loss = 0.456529887930652\n",
      "\n",
      "\n",
      "Iteration 820/1000\n",
      "Accuracy = 79.05799999999999%\n",
      "Loss = 0.4565234773477606\n",
      "\n",
      "\n",
      "Iteration 830/1000\n",
      "Accuracy = 79.0576%\n",
      "Loss = 0.45651717681172227\n",
      "\n",
      "\n",
      "Iteration 840/1000\n",
      "Accuracy = 79.05640000000001%\n",
      "Loss = 0.45651098631016385\n",
      "\n",
      "\n",
      "Iteration 850/1000\n",
      "Accuracy = 79.0612%\n",
      "Loss = 0.45650490565901997\n",
      "\n",
      "\n",
      "Iteration 860/1000\n",
      "Accuracy = 79.0644%\n",
      "Loss = 0.45649893451758655\n",
      "\n",
      "\n",
      "Iteration 870/1000\n",
      "Accuracy = 79.0648%\n",
      "Loss = 0.4564930724026471\n",
      "\n",
      "\n",
      "Iteration 880/1000\n",
      "Accuracy = 79.0668%\n",
      "Loss = 0.4564873187016947\n",
      "\n",
      "\n",
      "Iteration 890/1000\n",
      "Accuracy = 79.0688%\n",
      "Loss = 0.45648167268528755\n",
      "\n",
      "\n",
      "Iteration 900/1000\n",
      "Accuracy = 79.0652%\n",
      "Loss = 0.456476133518569\n",
      "\n",
      "\n",
      "Iteration 910/1000\n",
      "Accuracy = 79.0624%\n",
      "Loss = 0.4564707002719933\n",
      "\n",
      "\n",
      "Iteration 920/1000\n",
      "Accuracy = 79.0636%\n",
      "Loss = 0.45646537193129477\n",
      "\n",
      "\n",
      "Iteration 930/1000\n",
      "Accuracy = 79.0612%\n",
      "Loss = 0.4564601474067383\n",
      "\n",
      "\n",
      "Iteration 940/1000\n",
      "Accuracy = 79.062%\n",
      "Loss = 0.4564550255416938\n",
      "\n",
      "\n",
      "Iteration 950/1000\n",
      "Accuracy = 79.0644%\n",
      "Loss = 0.456450005120569\n",
      "\n",
      "\n",
      "Iteration 960/1000\n",
      "Accuracy = 79.0672%\n",
      "Loss = 0.4564450848761417\n",
      "\n",
      "\n",
      "Iteration 970/1000\n",
      "Accuracy = 79.0668%\n",
      "Loss = 0.4564402634963274\n",
      "\n",
      "\n",
      "Iteration 980/1000\n",
      "Accuracy = 79.0676%\n",
      "Loss = 0.4564355396304144\n",
      "\n",
      "\n",
      "Iteration 990/1000\n",
      "Accuracy = 79.07039999999999%\n",
      "Loss = 0.4564309118948066\n",
      "\n",
      "\n",
      "Best Accuracy : 79.0736% reached at epoch 998\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from implementations import logistic_regression, build_poly\n",
    "tX_train_poly = build_poly(tX_train, degree, cont_cols)\n",
    "loss, weights = logistic_regression(ty_train, tX_train_poly, max_iters=1000, lambda_=lambda_, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 34)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import build_poly\n",
    "tX_test = build_poly(tX_test, degree, cont_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 48)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from implementations import predict\n",
    "method = 'logistic_regression'\n",
    "time = datetime.now().strftime('%Y%m%dH%H%M%S')\n",
    "OUTPUT_PATH = f'submissions/submission_{method}_{time}' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict(weights, tX_test)\n",
    "y_pred[np.where(y_pred == 0)] = -1\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = []\n",
    "for i in range(tX.shape[1]):\n",
    "    corrs.append(np.corrcoef(tX[:, i], y)[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.3514279558616751,\n",
       " -0.19539789618287817,\n",
       " -0.031947586805348205,\n",
       " -0.015287426687781413,\n",
       " -0.014055273784852506,\n",
       " -0.0044025386863883985,\n",
       " -0.0009432510582117487,\n",
       " 0.0015162353770597236,\n",
       " 0.0041254474115248515,\n",
       " 0.0074753421885902374,\n",
       " 0.012245481285482902,\n",
       " 0.02246575151078583,\n",
       " 0.13354912308169134,\n",
       " 0.13429572666925302,\n",
       " 0.1355202615226846,\n",
       " 0.14055440046509557,\n",
       " 0.1407143669504368,\n",
       " 0.14125568650533774,\n",
       " 0.1412649137760153,\n",
       " 0.14134598859646297,\n",
       " 0.1416459925656641,\n",
       " 0.15046803779292678,\n",
       " 0.15046926004977906,\n",
       " 0.1532359324758135,\n",
       " 0.15760414567634926,\n",
       " 0.19176608807477077,\n",
       " 0.19252632856874796,\n",
       " 0.23523797587836723,\n",
       " 0.23914905789154473,\n",
       " 0.2717518770516493]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34e9dc8c9cd4c2e3341692c7f5472da17e27c062a0f2ac63648b60e63867ef4a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
