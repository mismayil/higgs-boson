{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "This notebook contains the code for running multiple models on the ML Higgs boson data using Ridge regression with normal equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y_train, X_train, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model using raw data\n",
    "First let's run Ridge regression algorithm on our raw data without doing any preprocessing. We will use K-fold cross validation to report the metrics on the test data and grid search to tune our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lambda_': np.logspace(-4,0,5) ,\n",
    "    'max_iters': 1000,\n",
    "    'gamma': [0.01, 0.1]\n",
    "}\n",
    "metrics, params = ridge_regression_cv(y_train, X_train, param_grid=param_grid, transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss': 0.3397855566120538,\n",
       "  'accuracy': 74.42840000000001,\n",
       "  'f1_score': 0.5688096990110691},\n",
       " {'lambda_': 0.0001, 'max_iters': 1000, 'gamma': 0.01})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression seems to be working decently with raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model using lightly feature engineered data\n",
    "Now let's now preprocess our data a bit to handle the missing values (-999) in various ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features with NaN values imputed\n",
    "First let's impute all missing values with median of their respective columns. So we will set the `imputable_th` to `1` which means impute all columns with a nan value ratio less than 1, or in other words all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train, ty_train, tX_test, ty_test, cont_features = preprocess(X_train, y_train, X_test, imputable_th=1, encodable_th=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((236483, 31), (568238, 31))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train.shape, tX_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now all the columns imputed and plus one more column for the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lambda_': np.logspace(-4,0,5) ,\n",
    "    'max_iters': 1000,\n",
    "    'gamma': [0.01, 0.1]\n",
    "}\n",
    "metrics, params = ridge_regression_cv(ty_train, tX_train, param_grid=param_grid, transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss': 0.3256406318235878,\n",
       "  'accuracy': 76.44029093369419,\n",
       "  'f1_score': 0.6129394851260843},\n",
       " {'lambda_': 0.0001, 'max_iters': 1000, 'gamma': 0.01})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, f1_score and loss have gone up, compared to the raw values, indicating that this pre-processing is useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features with NaN values encoded\n",
    "Now let's instead encode these features with NaN values into new indicator features where the new feature takes on a value of 1 if the value for the feature is missing, otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train, ty_train, tX_test, ty_test, cont_features = preprocess(X_train, y_train, X_test, imputable_th=0, encodable_th=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((243430, 31), (568238, 31))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train.shape, tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lambda_': np.logspace(-4,0,5) ,\n",
    "    'max_iters': 1000,\n",
    "    'gamma': [0.01, 0.1]\n",
    "}\n",
    "metrics, params = ridge_regression_cv(ty_train, tX_train, param_grid=param_grid, transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss': 0.33508274817893047,\n",
       "  'accuracy': 75.14151912254036,\n",
       "  'f1_score': 0.604671339546958},\n",
       " {'lambda_': 0.0001, 'max_iters': 1000, 'gamma': 0.01})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, f1_score and loss have gone up, compared to the raw values, indicating that this pre-processing is useful but it is lower than only imputing values. This indicates that a mix of the two might be more useful "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed imputing and encoding approach\n",
    "Finally,  let's try a more reasonable approach to the imputing and encoding scheme. As we saw in the exploration notebook, we have some features that have less than 15% of them missing, some around 40% and some more than 70%. Let's impute the columns in the first group, encode the ones in the second group and drop completely the ones in the third group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train, ty_train, tX_test, ty_test, cont_features = preprocess(X_train, y_train, X_test, imputable_th=0.3, encodable_th=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((242240, 24), (568238, 24))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train.shape, tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lambda_': np.logspace(-4,0,5) ,\n",
    "    'max_iters': 1000,\n",
    "    'gamma': [0.01, 0.1]\n",
    "}\n",
    "metrics, params = ridge_regression_cv(ty_train, tX_train, param_grid=param_grid, transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss': 0.3360989798895204,\n",
       "  'accuracy': 75.44129788639366,\n",
       "  'f1_score': 0.61151010562385},\n",
       " {'lambda_': 0.0001, 'max_iters': 1000, 'gamma': 0.01})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we get the best performance when we impute all of the NaN values. Let's continue our feature engineering with these preprocessing thresholds fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model using heavily feature engineered data\n",
    "In this step, we are going to apply more feature engineering. First, we will apply polynomial features of some degree that we will tune through grid search and cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train, ty_train, tX_test, ty_test, cont_features = preprocess(X_train, y_train, X_test, imputable_th=1, encodable_th=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((236483, 31), (568238, 31))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train.shape, tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lambda_': np.logspace(-4,0,5) ,\n",
    "    'degree': list(range(1, 4)),\n",
    "    'max_iters': 1000,\n",
    "    'gamma': [0.01, 0.1]\n",
    "}\n",
    "metrics, params = ridge_regression_cv(ty_train, tX_train, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss': 0.2940684456343238,\n",
       "  'accuracy': 79.8680649526387,\n",
       "  'f1_score': 0.6785882304073018},\n",
       " {'lambda_': 0.0001, 'degree': 3, 'max_iters': 1000, 'gamma': 0.01})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like polynomial feature expansion is useful for ridge regression, as the degree =3 was chosen and accuracy has gone up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to split our datasets based on the number of jets (`PRI_jet_num`) and create 3 subsets of the data for observations with 0, 1 and more than 1 jet respectively. Each subset will also only have the relevant columns (based on the original paper) All other missing values in the new subsets will be imputed with median values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_zero, y_train_zero, X_train_one, y_train_one, X_train_many, y_train_many = split_by_jet_num(DATA_TRAIN_PATH, X_train, y_train)\n",
    "X_test_zero, ids_test_zero, X_test_one, ids_test_one, X_test_many, ids_test_many = split_by_jet_num(DATA_TRAIN_PATH, X_test, ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99913, 15), (77544, 22), (72543, 29))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_zero.shape, X_train_one.shape, X_train_many.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, X_test):\n",
    "    tX_train, ty_train, tX_test, ty_test, cont_features = preprocess(X_train, y_train, X_test, imputable_th=1, encodable_th=0)\n",
    "    \n",
    "    param_grid = {\n",
    "    'lambda_': np.logspace(-4,0,5) ,\n",
    "    'degree': list(range(1, 4)),\n",
    "    'max_iters': 1000,\n",
    "    'gamma': [0.01, 0.1]\n",
    "}\n",
    "    metrics, params = ridge_regression_cv(ty_train, tX_train, param_grid=param_grid)\n",
    "    return metrics, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_zero, params_zero = train(X_train_zero, y_train_zero, X_test_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_one, params_one = train(X_train_one, y_train_one, X_test_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_many, params_many = train(X_train_many, y_train_many, X_test_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss': 0.24364324945993907,\n",
       "  'accuracy': 83.20020560267284,\n",
       "  'f1_score': 0.6209447368524985},\n",
       " {'lambda_': 0.0001, 'degree': 3, 'max_iters': 1000, 'gamma': 0.01})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_zero, params_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss': 0.31804954150970566,\n",
       "  'accuracy': 78.1462314225053,\n",
       "  'f1_score': 0.6755361956263567},\n",
       " {'lambda_': 0.0001, 'degree': 3, 'max_iters': 1000, 'gamma': 0.01})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_one, params_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss': 0.2968016621447154,\n",
       "  'accuracy': 80.30970648271979,\n",
       "  'f1_score': 0.7802551375347928},\n",
       " {'lambda_': 0.0001, 'degree': 3, 'max_iters': 1000, 'gamma': 0.01})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_many, params_many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy with jet_num training is 80.79384219673017\n"
     ]
    }
   ],
   "source": [
    "a = X_train_zero.shape[0]\n",
    "b =  X_train_one.shape[0] \n",
    "c = X_train_many.shape[0]\n",
    "avg_accuracy = ((metrics_zero['accuracy']*a) +  (metrics_one['accuracy']*b) + (metrics_many['accuracy']*c))/(a+b+c)\n",
    "\n",
    "print(f\"Average accuracy with jet_num training is {avg_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting by jet_num lets us arrive in the 80% accuracy territory."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e78a7ef29a5e3028f948eff69c34ba1d8ebd35a887497a02775c6aab840f6bc2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
