{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 1,
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y_train, X_train, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 3,
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 4,
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_zero, y_train_zero, X_train_one, y_train_one, X_train_many, y_train_many = split_by_jet_num(DATA_TRAIN_PATH, X_train, y_train)\n",
    "X_test_zero, ids_test_zero, X_test_one, ids_test_one, X_test_many, ids_test_many = split_by_jet_num(DATA_TRAIN_PATH, X_test, ids_test)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 5,
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(X_train, y_train, X_test, max_iters=3000, degree=2, lambda_=0.01, gamma=0.01, imputable_th=0, encodable_th=1, verbose=True):\n",
    "    tX_train, ty_train, tX_test, _, cont_features = preprocess(X_train, y_train, X_test, imputable_th=imputable_th, encodable_th=encodable_th, switch_encoding=True)\n",
    "    tX_train_poly = np.c_[build_poly(tX_train, degree=degree, cont_features=cont_features), cross_polynomial_feature_expansion(tX_train)]\n",
    "    tX_train_poly_log = np.c_[tX_train_poly,simple_logarithmic_feature_expansion(tX_train,1)]\n",
    "\n",
    "\n",
    "    weights, loss = reg_logistic_regression(ty_train, tX_train_poly_log, max_iters=max_iters, lambda_=lambda_, gamma=gamma, verbose=verbose)\n",
    "    tX_test_poly = np.c_[build_poly(tX_test, degree=degree, cont_features=cont_features), cross_polynomial_feature_expansion(tX_test)]\n",
    "    tX_test_poly_log = np.c_[tX_test_poly,simple_logarithmic_feature_expansion(tX_test,1)]\n",
    "\n",
    "    y_pred = predict_logistic(weights, tX_test_poly_log)\n",
    "    y_pred = replace_values(y_pred, from_val=0, to_val=-1)\n",
    "    return y_pred, weights, loss"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
=======
   "execution_count": 6,
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "lambda_ = 0.001\n",
    "gamma = 0.01\n",
    "max_iters = 3000"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
=======
   "execution_count": 7,
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Cross Polynomial expansion of degree 1 done : adding (99913, 120)\n",
      "Logarithmic expansion of degree 1 done : adding (99913, 16)\n",
      "Iteration = 0, accuracy = 25.514197351695977, loss = 14.264683276222623\n",
      "Iteration = 10, accuracy = 25.514197351695977, loss = 12.849620464324376\n",
      "Iteration = 20, accuracy = 25.50919299790818, loss = 10.618486518760792\n",
      "Iteration = 30, accuracy = 25.571246984876844, loss = 7.848748416914822\n",
      "Iteration = 40, accuracy = 31.47037922993004, loss = 5.234045821497157\n",
      "Iteration = 50, accuracy = 44.98313532773513, loss = 3.7699639024183824\n",
      "Iteration = 60, accuracy = 53.06116321199443, loss = 3.1485614988948445\n",
      "Iteration = 70, accuracy = 57.11769239238137, loss = 2.8481880527237387\n",
      "Iteration = 80, accuracy = 59.466736060372526, loss = 2.6704629464714187\n",
      "Iteration = 90, accuracy = 61.1842302803439, loss = 2.547000620573437\n",
      "Iteration = 100, accuracy = 62.387276930929914, loss = 2.450543581435801\n",
      "Iteration = 110, accuracy = 63.31408325242961, loss = 2.3693533546759418\n",
      "Iteration = 120, accuracy = 64.10176853862862, loss = 2.2976376011755764\n",
      "Iteration = 130, accuracy = 64.79337023210194, loss = 2.2325236760403833\n",
      "Iteration = 140, accuracy = 65.35385785633501, loss = 2.17211661324916\n",
      "Iteration = 150, accuracy = 65.80024621420635, loss = 2.115617410357368\n",
      "Iteration = 160, accuracy = 66.2396284767748, loss = 2.0624852431565133\n",
      "Iteration = 170, accuracy = 66.66299680722229, loss = 2.012164745946429\n",
      "Iteration = 180, accuracy = 67.05934162721567, loss = 1.9642280473943896\n",
      "Iteration = 190, accuracy = 67.41064726311892, loss = 1.9185488170825906\n",
      "Iteration = 200, accuracy = 67.78897640947625, loss = 1.8750723299689265\n",
      "Iteration = 210, accuracy = 68.11425940568294, loss = 1.8337246822363944\n",
      "Iteration = 220, accuracy = 68.43353717734429, loss = 1.7942949123223906\n",
      "Iteration = 230, accuracy = 68.71878534324863, loss = 1.7567001755663956\n",
      "Iteration = 240, accuracy = 68.96299780809304, loss = 1.7207877569308723\n",
      "Iteration = 250, accuracy = 69.21121375596769, loss = 1.68631244770597\n",
      "Iteration = 260, accuracy = 69.45642709156967, loss = 1.6531856116681294\n",
      "Iteration = 270, accuracy = 69.67862039974779, loss = 1.6213464350339972\n",
      "Iteration = 280, accuracy = 69.91482589853172, loss = 1.5907372455780993\n",
      "Iteration = 290, accuracy = 70.13101398216448, loss = 1.5613192660900066\n",
      "Iteration = 300, accuracy = 70.35921251488794, loss = 1.5330860333679508\n",
      "Iteration = 310, accuracy = 70.61543542882308, loss = 1.5060048845613319\n",
      "Iteration = 320, accuracy = 70.79959564821394, loss = 1.4799112981209148\n",
      "Iteration = 330, accuracy = 71.003773282756, loss = 1.4546470890852459\n",
      "Iteration = 340, accuracy = 71.18192827760151, loss = 1.4302561409291843\n",
      "Iteration = 350, accuracy = 71.40412158577962, loss = 1.4066784683961417\n",
      "Iteration = 360, accuracy = 71.60729834956412, loss = 1.383858177665546\n",
      "Iteration = 370, accuracy = 71.80847337183349, loss = 1.3617659103558624\n",
      "Iteration = 380, accuracy = 71.99563620349704, loss = 1.3403984092956251\n",
      "Iteration = 390, accuracy = 72.172790327585, loss = 1.3196780474034824\n",
      "Iteration = 400, accuracy = 72.31991832894619, loss = 1.2995332300763984\n",
      "Iteration = 410, accuracy = 72.4810585209132, loss = 1.2799627954129948\n",
      "Iteration = 420, accuracy = 72.64520132515288, loss = 1.261012108040727\n",
      "Iteration = 430, accuracy = 72.78632410196872, loss = 1.2426432069577713\n",
      "Iteration = 440, accuracy = 72.93945732787526, loss = 1.2248469684691634\n",
      "Iteration = 450, accuracy = 73.10760361514518, loss = 1.2075752825264794\n",
      "Iteration = 460, accuracy = 73.25573248726393, loss = 1.190808857050317\n",
      "Iteration = 470, accuracy = 73.42487964529141, loss = 1.174501576046096\n",
      "Iteration = 480, accuracy = 73.54298239468338, loss = 1.1586259021057832\n",
      "Iteration = 490, accuracy = 73.6670903686207, loss = 1.143180073466939\n",
      "Iteration = 500, accuracy = 73.81321749922432, loss = 1.1281762941523346\n",
      "Iteration = 510, accuracy = 73.94132895619188, loss = 1.1135909651382245\n",
      "Iteration = 520, accuracy = 74.09246044058331, loss = 1.0994040781807757\n",
      "Iteration = 530, accuracy = 74.2145666730055, loss = 1.0856057010254767\n",
      "Iteration = 540, accuracy = 74.32766506860969, loss = 1.0721488784415665\n",
      "Iteration = 550, accuracy = 74.45277391330457, loss = 1.0590484566674812\n",
      "Iteration = 560, accuracy = 74.5888923363326, loss = 1.0465055661523535\n",
      "Iteration = 570, accuracy = 74.7039924734519, loss = 1.0345188245301509\n",
      "Iteration = 580, accuracy = 74.82409696435899, loss = 1.0226112487762407\n",
      "Iteration = 590, accuracy = 74.93119013541781, loss = 1.0111016516968652\n",
      "Iteration = 600, accuracy = 75.03227808193128, loss = 0.9999115510081036\n",
      "Iteration = 610, accuracy = 75.1393712529901, loss = 0.9889763072262024\n",
      "Iteration = 620, accuracy = 75.25847487313963, loss = 0.978238556781861\n",
      "Iteration = 630, accuracy = 75.34855324131995, loss = 0.9679668127327015\n",
      "Iteration = 640, accuracy = 75.45664728313632, loss = 0.9577297033775485\n",
      "Iteration = 650, accuracy = 75.52470649465035, loss = 0.947961625145206\n",
      "Iteration = 660, accuracy = 75.59376657692192, loss = 0.9381960124232613\n",
      "Iteration = 670, accuracy = 75.67083362525398, loss = 0.9288256272929194\n",
      "Iteration = 680, accuracy = 75.77792679631278, loss = 0.9195407050846632\n",
      "Iteration = 690, accuracy = 75.87601213055358, loss = 0.9104308104663205\n",
      "Iteration = 700, accuracy = 75.96508962797634, loss = 0.9017435002319693\n",
      "Iteration = 710, accuracy = 76.04515928858106, loss = 0.8930366977487832\n",
      "Iteration = 720, accuracy = 76.1032097925195, loss = 0.8847349477780985\n",
      "Iteration = 730, accuracy = 76.17727422857887, loss = 0.8764088710407195\n",
      "Iteration = 740, accuracy = 76.26835346751673, loss = 0.8682966449702197\n",
      "Iteration = 750, accuracy = 76.35542922342438, loss = 0.8605042514274339\n",
      "Iteration = 760, accuracy = 76.46552500675587, loss = 0.8526989494517283\n",
      "Iteration = 770, accuracy = 76.53258334751234, loss = 0.8452197352388949\n",
      "Iteration = 780, accuracy = 76.60764865432927, loss = 0.837833939059779\n",
      "Iteration = 790, accuracy = 76.68271396114619, loss = 0.8305264827770932\n",
      "Iteration = 800, accuracy = 76.74777056038754, loss = 0.8233372572496431\n",
      "Iteration = 810, accuracy = 76.79481148599281, loss = 0.816460786772717\n",
      "Iteration = 820, accuracy = 76.83684805781029, loss = 0.8095500905773609\n",
      "Iteration = 830, accuracy = 76.92092120144525, loss = 0.8029295849099287\n",
      "Iteration = 840, accuracy = 77.0079969573529, loss = 0.7963360229473263\n",
      "Iteration = 850, accuracy = 77.0740544273518, loss = 0.789822981387556\n",
      "Iteration = 860, accuracy = 77.14511625113849, loss = 0.7835552523542701\n",
      "Iteration = 870, accuracy = 77.21517720416763, loss = 0.7772749465388888\n",
      "Iteration = 880, accuracy = 77.28223554492408, loss = 0.7711727547518337\n",
      "Iteration = 890, accuracy = 77.32227037522644, loss = 0.7651890807378477\n",
      "Iteration = 900, accuracy = 77.4063435188614, loss = 0.7592598118683792\n",
      "Iteration = 910, accuracy = 77.48140882567834, loss = 0.7536431900906648\n",
      "Iteration = 920, accuracy = 77.5484671664348, loss = 0.7479187013946371\n",
      "Iteration = 930, accuracy = 77.60551679961567, loss = 0.7422967025968749\n",
      "Iteration = 940, accuracy = 77.6535585959785, loss = 0.7367673650763741\n",
      "Iteration = 950, accuracy = 77.72061693673496, loss = 0.7315333228407267\n",
      "Iteration = 960, accuracy = 77.77966831143095, loss = 0.7261904835005729\n",
      "Iteration = 970, accuracy = 77.82971184930891, loss = 0.7209342076122123\n",
      "Iteration = 980, accuracy = 77.89176583627756, loss = 0.715762847024782\n",
      "Iteration = 990, accuracy = 77.93580414961016, loss = 0.7108136543456095\n",
      "Iteration = 1000, accuracy = 77.98784942900323, loss = 0.7058257265938649\n",
      "Iteration = 1010, accuracy = 78.04189644991142, loss = 0.7009214218388035\n",
      "Iteration = 1020, accuracy = 78.09894608309229, loss = 0.6961329802726766\n",
      "Iteration = 1030, accuracy = 78.15699658703072, loss = 0.6914905302843768\n",
      "Iteration = 1040, accuracy = 78.23206189384764, loss = 0.6868116624728889\n",
      "Iteration = 1050, accuracy = 78.26308888733197, loss = 0.6822282708857658\n",
      "Iteration = 1060, accuracy = 78.32214026202796, loss = 0.6777934472093461\n",
      "Iteration = 1070, accuracy = 78.37618728293614, loss = 0.6733555197251138\n",
      "Iteration = 1080, accuracy = 78.42623082081411, loss = 0.6689887406708274\n",
      "Iteration = 1090, accuracy = 78.49028654929789, loss = 0.6647936545279535\n",
      "Iteration = 1100, accuracy = 78.51630918899443, loss = 0.6606493487185564\n",
      "Iteration = 1110, accuracy = 78.57235795141774, loss = 0.6564809311133312\n",
      "Iteration = 1120, accuracy = 78.62740584308348, loss = 0.6523792215198935\n",
      "Iteration = 1130, accuracy = 78.66343719035561, loss = 0.6483407597779798\n",
      "Iteration = 1140, accuracy = 78.71147898671845, loss = 0.6443595089569962\n",
      "Iteration = 1150, accuracy = 78.76752774914175, loss = 0.6404482504596756\n",
      "Iteration = 1160, accuracy = 78.81056519171679, loss = 0.6367044960276752\n",
      "Iteration = 1170, accuracy = 78.85160089277672, loss = 0.632881266150064\n",
      "Iteration = 1180, accuracy = 78.90965139671515, loss = 0.6291545791626724\n",
      "Iteration = 1190, accuracy = 78.9376757779268, loss = 0.625444835509056\n",
      "Iteration = 1200, accuracy = 78.9767097374716, loss = 0.6218384580643431\n",
      "Iteration = 1210, accuracy = 78.99572628186522, loss = 0.618315543304474\n",
      "Iteration = 1220, accuracy = 79.03976459519782, loss = 0.6148234676652491\n",
      "Iteration = 1230, accuracy = 79.09281074534846, loss = 0.6113879001315473\n",
      "Iteration = 1240, accuracy = 79.1258394803479, loss = 0.6080062232389982\n",
      "Iteration = 1250, accuracy = 79.15886821534735, loss = 0.6046767212468094\n",
      "Iteration = 1260, accuracy = 79.2019056579224, loss = 0.6013991436787354\n",
      "Iteration = 1270, accuracy = 79.2579544203457, loss = 0.5981502045032423\n",
      "Iteration = 1280, accuracy = 79.30399447519342, loss = 0.5949759621404725\n",
      "Iteration = 1290, accuracy = 79.3510354007987, loss = 0.5918851299885295\n",
      "Iteration = 1300, accuracy = 79.38806761882839, loss = 0.5887499617119418\n",
      "Iteration = 1310, accuracy = 79.43711028594878, loss = 0.5857179394965174\n",
      "Iteration = 1320, accuracy = 79.46813727943311, loss = 0.5827356961031347\n",
      "Iteration = 1330, accuracy = 79.51317646352327, loss = 0.5798009551601634\n",
      "Iteration = 1340, accuracy = 79.55221042306808, loss = 0.5769103338301739\n",
      "Iteration = 1350, accuracy = 79.59124438261287, loss = 0.5740642169459584\n",
      "Iteration = 1360, accuracy = 79.63027834215768, loss = 0.5712544568232665\n",
      "Iteration = 1370, accuracy = 79.6713140432176, loss = 0.5684957498599696\n",
      "Iteration = 1380, accuracy = 79.70934713200485, loss = 0.5656366668209166\n",
      "Iteration = 1390, accuracy = 79.75138370382233, loss = 0.563008518702468\n",
      "Iteration = 1400, accuracy = 79.79942550018517, loss = 0.560375141406362\n",
      "Iteration = 1410, accuracy = 79.84046120124508, loss = 0.557779229933107\n",
      "Iteration = 1420, accuracy = 79.88850299760793, loss = 0.555079372638615\n",
      "Iteration = 1430, accuracy = 79.93053956942539, loss = 0.5525576851940854\n",
      "Iteration = 1440, accuracy = 79.96557004593997, loss = 0.5500735686212957\n",
      "Iteration = 1450, accuracy = 79.99759791018187, loss = 0.5476172866066514\n",
      "Iteration = 1460, accuracy = 80.01161010078769, loss = 0.5452101967109987\n",
      "Iteration = 1470, accuracy = 80.04263709427202, loss = 0.5428057914865367\n",
      "Iteration = 1480, accuracy = 80.067658863211, loss = 0.5404706488640548\n",
      "Iteration = 1490, accuracy = 80.10769369351335, loss = 0.538038301076596\n",
      "Iteration = 1500, accuracy = 80.14472591154305, loss = 0.5357713186804517\n",
      "Iteration = 1510, accuracy = 80.16874680972445, loss = 0.5335376073624587\n",
      "Iteration = 1520, accuracy = 80.19376857866344, loss = 0.5313300032182383\n",
      "Iteration = 1530, accuracy = 80.24981734108675, loss = 0.5290380985904262\n",
      "Iteration = 1540, accuracy = 80.29585739593446, loss = 0.526901432967466\n",
      "Iteration = 1550, accuracy = 80.33188874320659, loss = 0.5248177532529759\n",
      "Iteration = 1560, accuracy = 80.36992183199384, loss = 0.5226268263096008\n",
      "Iteration = 1570, accuracy = 80.40194969623573, loss = 0.5205913407823887\n",
      "Iteration = 1580, accuracy = 80.4369801727503, loss = 0.5185762999589258\n",
      "Iteration = 1590, accuracy = 80.4710097785073, loss = 0.5165013628549429\n",
      "Iteration = 1600, accuracy = 80.51104460880967, loss = 0.5145080438416676\n",
      "Iteration = 1610, accuracy = 80.53406463623352, loss = 0.5126134608851635\n",
      "Iteration = 1620, accuracy = 80.58010469108125, loss = 0.5106007858808589\n",
      "Iteration = 1630, accuracy = 80.60212384774755, loss = 0.5087183566194451\n",
      "Iteration = 1640, accuracy = 80.62714561668652, loss = 0.5068236783940122\n",
      "Iteration = 1650, accuracy = 80.66517870547376, loss = 0.5049524131764686\n",
      "Iteration = 1660, accuracy = 80.70221092350344, loss = 0.5031386536541914\n",
      "Iteration = 1670, accuracy = 80.72423008016975, loss = 0.5013628420194122\n",
      "Iteration = 1680, accuracy = 80.76226316895699, loss = 0.4995464593389352\n",
      "Iteration = 1690, accuracy = 80.78728493789596, loss = 0.4978286541278036\n",
      "Iteration = 1700, accuracy = 80.82131454365297, loss = 0.49608239285125605\n",
      "Iteration = 1710, accuracy = 80.84933892486464, loss = 0.4944173995333593\n",
      "Iteration = 1720, accuracy = 80.88537027213675, loss = 0.4927608545801433\n",
      "Iteration = 1730, accuracy = 80.91639726562109, loss = 0.49105024458503577\n",
      "Iteration = 1740, accuracy = 80.92740684395424, loss = 0.4894537006040218\n",
      "Iteration = 1750, accuracy = 80.9504268713781, loss = 0.4878212038596354\n",
      "Iteration = 1760, accuracy = 80.99046170168046, loss = 0.48629523649922496\n",
      "Iteration = 1770, accuracy = 81.02449130743747, loss = 0.4846840327825275\n",
      "Iteration = 1780, accuracy = 81.04150611031598, loss = 0.483183031871302\n",
      "Iteration = 1790, accuracy = 81.07653658683054, loss = 0.4816217666694847\n",
      "Iteration = 1800, accuracy = 81.10055748501196, loss = 0.4801624609441827\n",
      "Iteration = 1810, accuracy = 81.12858186622361, loss = 0.4787223268476489\n",
      "Iteration = 1820, accuracy = 81.15260276440503, loss = 0.4772278224602017\n",
      "Iteration = 1830, accuracy = 81.17462192107133, loss = 0.475849556119145\n",
      "Iteration = 1840, accuracy = 81.18863411167716, loss = 0.47439900200452134\n",
      "Iteration = 1850, accuracy = 81.23667590803998, loss = 0.4729981268890968\n",
      "Iteration = 1860, accuracy = 81.25168896940338, loss = 0.4716680781043864\n",
      "Iteration = 1870, accuracy = 81.27370812606968, loss = 0.47028514421506773\n",
      "Iteration = 1880, accuracy = 81.29072292894818, loss = 0.46898121375590857\n",
      "Iteration = 1890, accuracy = 81.32174992243252, loss = 0.4676658083374893\n",
      "Iteration = 1900, accuracy = 81.34476994985637, loss = 0.4663807638377177\n",
      "Iteration = 1910, accuracy = 81.3757969433407, loss = 0.4650993169871189\n",
      "Iteration = 1920, accuracy = 81.39981784152212, loss = 0.46384622849296453\n",
      "Iteration = 1930, accuracy = 81.43284657652158, loss = 0.4626155551190772\n",
      "Iteration = 1940, accuracy = 81.44685876712741, loss = 0.46141224441866346\n",
      "Iteration = 1950, accuracy = 81.46787705303615, loss = 0.4602265558273581\n",
      "Iteration = 1960, accuracy = 81.48188924364197, loss = 0.4590270862364308\n",
      "Iteration = 1970, accuracy = 81.50490927106583, loss = 0.4578684252892493\n",
      "Iteration = 1980, accuracy = 81.52692842773213, loss = 0.4567280663777681\n",
      "Iteration = 1990, accuracy = 81.54194148909552, loss = 0.45560227327366254\n",
      "Iteration = 2000, accuracy = 81.57196761182229, loss = 0.4544708080491313\n",
      "Iteration = 2010, accuracy = 81.60199373454905, loss = 0.45339542750782147\n",
      "Iteration = 2020, accuracy = 81.62501376197292, loss = 0.45229197925310577\n",
      "Iteration = 2030, accuracy = 81.64302943560898, loss = 0.45122272895983745\n",
      "Iteration = 2040, accuracy = 81.64603204788166, loss = 0.4501889585401832\n",
      "Iteration = 2050, accuracy = 81.67305555833575, loss = 0.4491330487890696\n",
      "Iteration = 2060, accuracy = 81.70308168106253, loss = 0.44810566611473057\n",
      "Iteration = 2070, accuracy = 81.71709387166835, loss = 0.44710395198753156\n",
      "Iteration = 2080, accuracy = 81.74411738212245, loss = 0.44610620356544445\n",
      "Iteration = 2090, accuracy = 81.75612783121315, loss = 0.44512707525517586\n",
      "Iteration = 2100, accuracy = 81.77414350484922, loss = 0.4441534430708129\n",
      "Iteration = 2110, accuracy = 81.79416092000041, loss = 0.4432051076201405\n",
      "Iteration = 2120, accuracy = 81.8211844304545, loss = 0.44226335555991453\n",
      "Iteration = 2130, accuracy = 81.83619749181788, loss = 0.4413374180158681\n",
      "Iteration = 2140, accuracy = 81.86322100227198, loss = 0.4404184041646506\n",
      "Iteration = 2150, accuracy = 81.87322970984758, loss = 0.4395272667910202\n",
      "Iteration = 2160, accuracy = 81.89324712499875, loss = 0.4386357943037847\n",
      "Iteration = 2170, accuracy = 81.91526628166504, loss = 0.4377562466552961\n",
      "Iteration = 2180, accuracy = 81.93428282605866, loss = 0.4368976012519641\n",
      "Iteration = 2190, accuracy = 81.95530111196742, loss = 0.4360454128089145\n",
      "Iteration = 2200, accuracy = 81.97331678560347, loss = 0.4352016242619228\n",
      "Iteration = 2210, accuracy = 81.99833855454244, loss = 0.4343778267023174\n",
      "Iteration = 2220, accuracy = 82.00334290833024, loss = 0.43355890640113576\n",
      "Iteration = 2230, accuracy = 82.01735509893608, loss = 0.4327579388397481\n",
      "Iteration = 2240, accuracy = 82.0363716433297, loss = 0.43196308609940076\n",
      "Iteration = 2250, accuracy = 82.05939167075356, loss = 0.4311784664169506\n",
      "Iteration = 2260, accuracy = 82.07840821514718, loss = 0.43040904191713175\n",
      "Iteration = 2270, accuracy = 82.10042737181348, loss = 0.42964631617618854\n",
      "Iteration = 2280, accuracy = 82.10943520863151, loss = 0.428893756573016\n",
      "Iteration = 2290, accuracy = 82.12144565772222, loss = 0.4281542072510729\n",
      "Iteration = 2300, accuracy = 82.14446568514607, loss = 0.42742241088434346\n",
      "Iteration = 2310, accuracy = 82.15947874650946, loss = 0.42670088863335115\n",
      "Iteration = 2320, accuracy = 82.17649354938797, loss = 0.42598969198921566\n",
      "Iteration = 2330, accuracy = 82.18450051544845, loss = 0.42528814003242144\n",
      "Iteration = 2340, accuracy = 82.20051444756939, loss = 0.42459626277512835\n",
      "Iteration = 2350, accuracy = 82.21552750893278, loss = 0.42391357292281256\n",
      "Iteration = 2360, accuracy = 82.21953099196301, loss = 0.42324002895876967\n",
      "Iteration = 2370, accuracy = 82.2365457948415, loss = 0.42257550152322987\n",
      "Iteration = 2380, accuracy = 82.24355189014443, loss = 0.421920037142438\n",
      "Iteration = 2390, accuracy = 82.26657191756829, loss = 0.42127323440357034\n",
      "Iteration = 2400, accuracy = 82.29259455726482, loss = 0.42063500229379747\n",
      "Iteration = 2410, accuracy = 82.31461371393112, loss = 0.4200053774937644\n",
      "Iteration = 2420, accuracy = 82.32762503377938, loss = 0.41938403368521404\n",
      "Iteration = 2430, accuracy = 82.346641578173, loss = 0.41877083873504295\n",
      "Iteration = 2440, accuracy = 82.36265551029396, loss = 0.41816579786359065\n",
      "Iteration = 2450, accuracy = 82.36966160559686, loss = 0.4175687118774528\n",
      "Iteration = 2460, accuracy = 82.39268163302073, loss = 0.4169794063032604\n",
      "Iteration = 2470, accuracy = 82.39968772832364, loss = 0.4163978440095201\n",
      "Iteration = 2480, accuracy = 82.42370862650506, loss = 0.4158238233715148\n",
      "Iteration = 2490, accuracy = 82.43171559256554, loss = 0.41525727879992486\n",
      "Iteration = 2500, accuracy = 82.43571907559577, loss = 0.4146980683590687\n",
      "Iteration = 2510, accuracy = 82.45273387847428, loss = 0.4141460759532726\n",
      "Iteration = 2520, accuracy = 82.46274258604987, loss = 0.41360081771383644\n",
      "Iteration = 2530, accuracy = 82.47775564741325, loss = 0.41306256283591075\n",
      "Iteration = 2540, accuracy = 82.49477045029175, loss = 0.4125312015437352\n",
      "Iteration = 2550, accuracy = 82.5107843824127, loss = 0.4120066259911654\n",
      "Iteration = 2560, accuracy = 82.52079308998829, loss = 0.41148873026466726\n",
      "Iteration = 2570, accuracy = 82.532803539079, loss = 0.4109774104235081\n",
      "Iteration = 2580, accuracy = 82.55582356650287, loss = 0.4104725644997968\n",
      "Iteration = 2590, accuracy = 82.56383053256333, loss = 0.409974092103434\n",
      "Iteration = 2600, accuracy = 82.56883488635113, loss = 0.40948189389816236\n",
      "Iteration = 2610, accuracy = 82.58785143074475, loss = 0.40899587225201955\n",
      "Iteration = 2620, accuracy = 82.60186362135057, loss = 0.40851593163612965\n",
      "Iteration = 2630, accuracy = 82.61987929498665, loss = 0.4080419782597588\n",
      "Iteration = 2640, accuracy = 82.63589322710759, loss = 0.40757391993861275\n",
      "Iteration = 2650, accuracy = 82.65891325453144, loss = 0.4071116660744227\n",
      "Iteration = 2660, accuracy = 82.66591934983435, loss = 0.40665512763104444\n",
      "Iteration = 2670, accuracy = 82.69094111877334, loss = 0.4062042171077572\n",
      "Iteration = 2680, accuracy = 82.69794721407625, loss = 0.40575884851259747\n",
      "Iteration = 2690, accuracy = 82.70495330937916, loss = 0.4053189373361015\n",
      "Iteration = 2700, accuracy = 82.72497072453035, loss = 0.4048844005255101\n",
      "Iteration = 2710, accuracy = 82.73998378589373, loss = 0.40445515645944585\n",
      "Iteration = 2720, accuracy = 82.7650055548327, loss = 0.40403112492304666\n",
      "Iteration = 2730, accuracy = 82.77201165013561, loss = 0.40361222708356287\n",
      "Iteration = 2740, accuracy = 82.7820203577112, loss = 0.4031983854663859\n",
      "Iteration = 2750, accuracy = 82.79503167755948, loss = 0.40278948043651297\n",
      "Iteration = 2760, accuracy = 82.8090438681653, loss = 0.40238498087743363\n",
      "Iteration = 2770, accuracy = 82.81604996346822, loss = 0.4019853224952765\n",
      "Iteration = 2780, accuracy = 82.82305605877113, loss = 0.4015904328308839\n",
      "Iteration = 2790, accuracy = 82.83806912013452, loss = 0.4012002406726642\n",
      "Iteration = 2800, accuracy = 82.84507521543743, loss = 0.4008146760349131\n",
      "Iteration = 2810, accuracy = 82.85408305225546, loss = 0.4004336701364608\n",
      "Iteration = 2820, accuracy = 82.85908740604326, loss = 0.40005715537965286\n",
      "Iteration = 2830, accuracy = 82.86709437210374, loss = 0.3996850653296392\n",
      "Iteration = 2840, accuracy = 82.8851100457398, loss = 0.3993173346939792\n",
      "Iteration = 2850, accuracy = 82.89611962407294, loss = 0.39895389930254166\n",
      "Iteration = 2860, accuracy = 82.91013181467876, loss = 0.39859469608769826\n",
      "Iteration = 2870, accuracy = 82.91713790998169, loss = 0.39823966306480535\n",
      "Iteration = 2880, accuracy = 82.94115880816311, loss = 0.39788873931296903\n",
      "Iteration = 2890, accuracy = 82.95016664498114, loss = 0.3975418649560871\n",
      "Iteration = 2900, accuracy = 82.95917448179917, loss = 0.39719898114416385\n",
      "Iteration = 2910, accuracy = 82.95717274028405, loss = 0.39686003003489656\n",
      "Iteration = 2920, accuracy = 82.97118493088988, loss = 0.39652495477553623\n",
      "Iteration = 2930, accuracy = 82.97719015543524, loss = 0.39619369948501054\n",
      "Iteration = 2940, accuracy = 82.9801927677079, loss = 0.3958662092363068\n",
      "Iteration = 2950, accuracy = 82.9902014752835, loss = 0.39554243003912853\n",
      "Iteration = 2960, accuracy = 83.00321279513176, loss = 0.39522230882280807\n",
      "Iteration = 2970, accuracy = 83.00921801967712, loss = 0.39490579341947424\n",
      "Iteration = 2980, accuracy = 83.01021889043467, loss = 0.3945928325474859\n",
      "Iteration = 2990, accuracy = 83.01622411498003, loss = 0.39428337579511746\n",
      "Iteration = 3000, accuracy = 83.02022759801027, loss = 0.39397737360449814\n",
      "Iteration = 3010, accuracy = 83.01822585649515, loss = 0.39367477725580435\n",
      "Iteration = 3020, accuracy = 83.02623282255563, loss = 0.3933754910037374\n",
      "Iteration = 3030, accuracy = 83.03724240088877, loss = 0.39307924086499124\n",
      "Iteration = 3040, accuracy = 83.04825197922192, loss = 0.392786257981172\n",
      "Iteration = 3050, accuracy = 83.04725110846437, loss = 0.39249649679587006\n",
      "Iteration = 3060, accuracy = 83.05725981603996, loss = 0.3922099125158713\n",
      "Iteration = 3070, accuracy = 83.0682693943731, loss = 0.3919264610967355\n",
      "Iteration = 3080, accuracy = 83.0732737481609, loss = 0.3916460992285774\n",
      "Iteration = 3090, accuracy = 83.07427461891847, loss = 0.39136878432201305\n",
      "Iteration = 3100, accuracy = 83.08528419725161, loss = 0.39109447449425677\n",
      "Iteration = 3110, accuracy = 83.08628506800918, loss = 0.39082312855531237\n",
      "Iteration = 3120, accuracy = 83.09429203406964, loss = 0.39055470599419534\n",
      "Iteration = 3130, accuracy = 83.1053016124028, loss = 0.3902891669651009\n",
      "Iteration = 3140, accuracy = 83.1123077077057, loss = 0.39002647227336573\n",
      "Iteration = 3150, accuracy = 83.12531902755397, loss = 0.3897665833610291\n",
      "Iteration = 3160, accuracy = 83.12531902755397, loss = 0.3895094622916908\n",
      "Iteration = 3170, accuracy = 83.12531902755397, loss = 0.3892550717342705\n",
      "Iteration = 3180, accuracy = 83.13833034740225, loss = 0.38900337494519277\n",
      "Iteration = 3190, accuracy = 83.14733818422027, loss = 0.38875433574863494\n",
      "Iteration = 3200, accuracy = 83.15534515028075, loss = 0.3885079185150604\n",
      "Iteration = 3210, accuracy = 83.17235995315924, loss = 0.3882640881398142\n",
      "Iteration = 3220, accuracy = 83.19037562679532, loss = 0.3880228100261191\n",
      "Iteration = 3230, accuracy = 83.20138520512846, loss = 0.38778405007907796\n",
      "Iteration = 3240, accuracy = 83.21539739573429, loss = 0.3875477747156942\n",
      "Iteration = 3250, accuracy = 83.22540610330988, loss = 0.3873139508881464\n",
      "Iteration = 3260, accuracy = 83.23741655240059, loss = 0.3870825461082895\n",
      "Iteration = 3270, accuracy = 83.2444226477035, loss = 0.38685352846023213\n",
      "Iteration = 3280, accuracy = 83.25743396755178, loss = 0.3866268665966201\n",
      "Iteration = 3290, accuracy = 83.26844354588492, loss = 0.3864025297236296\n",
      "Iteration = 3300, accuracy = 83.28545834876341, loss = 0.3861804875821118\n",
      "Iteration = 3310, accuracy = 83.295467056339, loss = 0.38596071042952496\n",
      "Iteration = 3320, accuracy = 83.3004714101268, loss = 0.3857431690240494\n",
      "Iteration = 3330, accuracy = 83.31348272997508, loss = 0.3855278346105821\n",
      "Iteration = 3340, accuracy = 83.33149840361114, loss = 0.3853146789082246\n",
      "Iteration = 3350, accuracy = 83.33650275739893, loss = 0.38510367409859964\n",
      "Iteration = 3360, accuracy = 83.33850449891406, loss = 0.3848947928148689\n",
      "Iteration = 3370, accuracy = 83.33850449891406, loss = 0.38468800813131687\n",
      "Iteration = 3380, accuracy = 83.33650275739893, loss = 0.38448329355339794\n",
      "Iteration = 3390, accuracy = 83.33650275739893, loss = 0.3842806230082007\n",
      "Iteration = 3400, accuracy = 83.34551059421696, loss = 0.384079970835287\n",
      "Iteration = 3410, accuracy = 83.34450972345941, loss = 0.38388131177787915\n",
      "Iteration = 3420, accuracy = 83.34951407724719, loss = 0.3836846209743724\n",
      "Iteration = 3430, accuracy = 83.35852191406524, loss = 0.3834898739501473\n",
      "Iteration = 3440, accuracy = 83.36352626785303, loss = 0.38329704660965547\n",
      "Iteration = 3450, accuracy = 83.37553671694374, loss = 0.38310611522877547\n",
      "Iteration = 3460, accuracy = 83.37954019997397, loss = 0.3829170564474068\n",
      "Iteration = 3470, accuracy = 83.38154194148909, loss = 0.3827298472622982\n",
      "Iteration = 3480, accuracy = 83.38954890754957, loss = 0.38254446502009254\n",
      "Iteration = 3490, accuracy = 83.39455326133736, loss = 0.38236088741057644\n",
      "Iteration = 3500, accuracy = 83.40456196891294, loss = 0.38217909246012366\n",
      "Iteration = 3510, accuracy = 83.41757328876123, loss = 0.38199905852532245\n",
      "Iteration = 3520, accuracy = 83.42858286709436, loss = 0.38182076428677825\n",
      "Iteration = 3530, accuracy = 83.4305846086095, loss = 0.38164418874308037\n",
      "Iteration = 3540, accuracy = 83.43258635012461, loss = 0.38146931120492544\n",
      "Iteration = 3550, accuracy = 83.4375907039124, loss = 0.3812961112893961\n",
      "Iteration = 3560, accuracy = 83.44960115300312, loss = 0.381124568914378\n",
      "Iteration = 3570, accuracy = 83.45860898982114, loss = 0.3809546642931153\n",
      "Iteration = 3580, accuracy = 83.4596098605787, loss = 0.380786377928901\n",
      "Iteration = 3590, accuracy = 83.46861769739672, loss = 0.3806196906098912\n",
      "Iteration = 3600, accuracy = 83.47362205118452, loss = 0.3804545834040402\n",
      "Iteration = 3610, accuracy = 83.47462292194209, loss = 0.38029103765415356\n",
      "Iteration = 3620, accuracy = 83.47662466345722, loss = 0.38012903497305056\n",
      "Iteration = 3630, accuracy = 83.48062814648745, loss = 0.3799685572388389\n",
      "Iteration = 3640, accuracy = 83.48563250027524, loss = 0.37980958659029196\n",
      "Iteration = 3650, accuracy = 83.48963598330548, loss = 0.3796521054223247\n",
      "Iteration = 3660, accuracy = 83.49564120785084, loss = 0.37949609638157045\n",
      "Iteration = 3670, accuracy = 83.49764294936594, loss = 0.3793415423620484\n",
      "Iteration = 3680, accuracy = 83.49564120785084, loss = 0.3791884265009264\n",
      "Iteration = 3690, accuracy = 83.50965339845666, loss = 0.3790367321743667\n",
      "Iteration = 3700, accuracy = 83.52066297678981, loss = 0.3788864429934638\n",
      "Iteration = 3710, accuracy = 83.51665949375958, loss = 0.37873754280026123\n",
      "Iteration = 3720, accuracy = 83.51966210603224, loss = 0.3785900156638508\n",
      "Iteration = 3730, accuracy = 83.5256673305776, loss = 0.37844384587654817\n",
      "Iteration = 3740, accuracy = 83.53167255512297, loss = 0.378299017950149\n",
      "Iteration = 3750, accuracy = 83.53467516739563, loss = 0.3781555166122552\n",
      "Iteration = 3760, accuracy = 83.5376777796683, loss = 0.3780133268026766\n",
      "Iteration = 3770, accuracy = 83.5426821334561, loss = 0.3778724336699013\n",
      "Iteration = 3780, accuracy = 83.55168997027414, loss = 0.3777328225676365\n",
      "Iteration = 3790, accuracy = 83.56069780709217, loss = 0.3775944790514136\n",
      "Iteration = 3800, accuracy = 83.56570216087997, loss = 0.37745738887525976\n",
      "Iteration = 3810, accuracy = 83.58071522224336, loss = 0.3773215379884351\n",
      "Iteration = 3820, accuracy = 83.5867204467887, loss = 0.3771869125322308\n",
      "Iteration = 3830, accuracy = 83.58872218830382, loss = 0.3770534988368271\n",
      "Iteration = 3840, accuracy = 83.58872218830382, loss = 0.3769212834182118\n",
      "Iteration = 3850, accuracy = 83.59873089587941, loss = 0.3767902529751565\n",
      "Iteration = 3860, accuracy = 83.59472741284918, loss = 0.37666039438624815\n",
      "Iteration = 3870, accuracy = 83.60273437890964, loss = 0.37653169470697867\n",
      "Iteration = 3880, accuracy = 83.61074134497012, loss = 0.37640414116688614\n",
      "Iteration = 3890, accuracy = 83.61774744027304, loss = 0.37627772116675084\n",
      "Iteration = 3900, accuracy = 83.62675527709106, loss = 0.376152422275843\n",
      "Iteration = 3910, accuracy = 83.62575440633351, loss = 0.37602823222921916\n",
      "Iteration = 3920, accuracy = 83.6237526648184, loss = 0.37590513892507105\n",
      "Iteration = 3930, accuracy = 83.63075876012131, loss = 0.3757831304221226\n",
      "Iteration = 3940, accuracy = 83.63476224315154, loss = 0.37566219493707026\n",
      "Iteration = 3950, accuracy = 83.64577182148469, loss = 0.37554232084207534\n",
      "Iteration = 3960, accuracy = 83.64877443375737, loss = 0.3754234966623013\n",
      "Iteration = 3970, accuracy = 83.6527779167876, loss = 0.37530571107349286\n",
      "Iteration = 3980, accuracy = 83.65978401209053, loss = 0.3751889528996002\n",
      "Iteration = 3990, accuracy = 83.66578923663587, loss = 0.37507321111044745\n",
      "Iteration = 4000, accuracy = 83.66278662436319, loss = 0.37495847481944056\n",
      "Iteration = 4010, accuracy = 83.66779097815099, loss = 0.3748447332813196\n",
      "Iteration = 4020, accuracy = 83.67379620269635, loss = 0.3747319758899472\n",
      "Iteration = 4030, accuracy = 83.6798014272417, loss = 0.3746201921761405\n",
      "Iteration = 4040, accuracy = 83.68780839330218, loss = 0.37450937180554034\n",
      "Iteration = 4050, accuracy = 83.69281274708997, loss = 0.3743995045765181\n",
      "Iteration = 4060, accuracy = 83.69781710087776, loss = 0.37429058041812024\n",
      "Iteration = 4070, accuracy = 83.70482319618068, loss = 0.37418258938804994\n",
      "Iteration = 4080, accuracy = 83.70482319618068, loss = 0.37407552167068336\n",
      "Iteration = 4090, accuracy = 83.70982754996848, loss = 0.37396936757512167\n",
      "Iteration = 4100, accuracy = 83.71583277451383, loss = 0.37386411753327614\n",
      "Iteration = 4110, accuracy = 83.72183799905918, loss = 0.37375976209799033\n",
      "Iteration = 4120, accuracy = 83.72083712830161, loss = 0.37365629194119293\n",
      "Iteration = 4130, accuracy = 83.71983625754407, loss = 0.3735536978520824\n",
      "Iteration = 4140, accuracy = 83.72083712830161, loss = 0.37345197073534664\n",
      "Iteration = 4150, accuracy = 83.7288440943621, loss = 0.3733511016094103\n",
      "Iteration = 4160, accuracy = 83.7338484481499, loss = 0.3732510816047147\n",
      "Iteration = 4170, accuracy = 83.73685106042257, loss = 0.3731519019620282\n",
      "Iteration = 4180, accuracy = 83.735850189665, loss = 0.37305355403078494\n",
      "Iteration = 4190, accuracy = 83.735850189665, loss = 0.3729560292674529\n",
      "Iteration = 4200, accuracy = 83.74685976799816, loss = 0.3728593192339306\n",
      "Iteration = 4210, accuracy = 83.75186412178596, loss = 0.37276341559597215\n",
      "Iteration = 4220, accuracy = 83.75486673405862, loss = 0.37266831012163787\n",
      "Iteration = 4230, accuracy = 83.7578693463313, loss = 0.37257399467977403\n",
      "Iteration = 4240, accuracy = 83.7578693463313, loss = 0.3724804612385181\n",
      "Iteration = 4250, accuracy = 83.76487544163422, loss = 0.37238770186382747\n",
      "Iteration = 4260, accuracy = 83.76987979542201, loss = 0.3722957087180374\n",
      "Iteration = 4270, accuracy = 83.77488414920981, loss = 0.3722044740584424\n",
      "Iteration = 4280, accuracy = 83.77888763224006, loss = 0.3721139902358999\n",
      "Iteration = 4290, accuracy = 83.78389198602785, loss = 0.37202424969346126\n",
      "Iteration = 4300, accuracy = 83.79790417663368, loss = 0.3719352449650237\n",
      "Iteration = 4310, accuracy = 83.79990591814878, loss = 0.3718469686740072\n",
      "Iteration = 4320, accuracy = 83.81091549648194, loss = 0.3717594135320523\n",
      "Iteration = 4330, accuracy = 83.8119163672395, loss = 0.3716725723377412\n",
      "Iteration = 4340, accuracy = 83.81491897951217, loss = 0.37158643797534\n",
      "Iteration = 4350, accuracy = 83.81992333329997, loss = 0.37150100341356346\n",
      "Iteration = 4360, accuracy = 83.81491897951217, loss = 0.37141623879342045\n",
      "Iteration = 4370, accuracy = 83.81892246254242, loss = 0.37133199555429247\n",
      "Iteration = 4380, accuracy = 83.81992333329997, loss = 0.3712484326857718\n",
      "Iteration = 4390, accuracy = 83.82492768708777, loss = 0.3711655434729273\n",
      "Iteration = 4400, accuracy = 83.83493639466336, loss = 0.37108332127949967\n",
      "Iteration = 4410, accuracy = 83.83293465314824, loss = 0.3710017595467849\n",
      "Iteration = 4420, accuracy = 83.83693813617847, loss = 0.3709208517925369\n",
      "Iteration = 4430, accuracy = 83.84194248996627, loss = 0.37084059160988714\n",
      "Iteration = 4440, accuracy = 83.84994945602674, loss = 0.3707609726662836\n",
      "Iteration = 4450, accuracy = 83.85195119754187, loss = 0.37068198870244556\n",
      "Iteration = 4460, accuracy = 83.85695555132966, loss = 0.37060363353133846\n",
      "Iteration = 4470, accuracy = 83.86195990511746, loss = 0.37052590103716465\n",
      "Iteration = 4480, accuracy = 83.86996687117792, loss = 0.37044878517436974\n",
      "Iteration = 4490, accuracy = 83.87997557875352, loss = 0.37037227996666805\n",
      "Iteration = 4500, accuracy = 83.8779738372384, loss = 0.37029637950608146\n",
      "Iteration = 4510, accuracy = 83.87397035420815, loss = 0.37022107795199627\n",
      "Iteration = 4520, accuracy = 83.88397906178375, loss = 0.3701463695302339\n",
      "Iteration = 4530, accuracy = 83.88998428632911, loss = 0.37007224853213905\n",
      "Iteration = 4540, accuracy = 83.89198602784423, loss = 0.3699987093136815\n",
      "Iteration = 4550, accuracy = 83.89598951087446, loss = 0.369925746294573\n",
      "Iteration = 4560, accuracy = 83.89498864011689, loss = 0.36985335395739866\n",
      "Iteration = 4570, accuracy = 83.90299560617737, loss = 0.3697815268467611\n",
      "Iteration = 4580, accuracy = 83.90499734769249, loss = 0.36971025956844145\n",
      "Iteration = 4590, accuracy = 83.90399647693494, loss = 0.3696395467885718\n",
      "Iteration = 4600, accuracy = 83.90799995996517, loss = 0.36956938323282235\n",
      "Iteration = 4610, accuracy = 83.90499734769249, loss = 0.3694997636856005\n",
      "Iteration = 4620, accuracy = 83.90599821845005, loss = 0.3694306829892643\n",
      "Iteration = 4630, accuracy = 83.91000170148028, loss = 0.36936213604334733\n",
      "Iteration = 4640, accuracy = 83.91100257223785, loss = 0.3692941178037969\n",
      "Iteration = 4650, accuracy = 83.90900083072272, loss = 0.3692266232822251\n",
      "Iteration = 4660, accuracy = 83.90699908920762, loss = 0.3691596475451702\n",
      "Iteration = 4670, accuracy = 83.90900083072272, loss = 0.36909318571337163\n",
      "Iteration = 4680, accuracy = 83.91800866754076, loss = 0.36902723296105466\n",
      "Iteration = 4690, accuracy = 83.922012150571, loss = 0.3689617845152286\n",
      "Iteration = 4700, accuracy = 83.92601563360124, loss = 0.3688968356549939\n",
      "Iteration = 4710, accuracy = 83.92601563360124, loss = 0.36883238171086247\n",
      "Iteration = 4720, accuracy = 83.92501476284367, loss = 0.36876841806408794\n",
      "Iteration = 4730, accuracy = 83.93202085814659, loss = 0.3687049401460059\n",
      "Iteration = 4740, accuracy = 83.93802608269195, loss = 0.36864194343738554\n",
      "Iteration = 4750, accuracy = 83.93802608269195, loss = 0.3685794234677906\n",
      "Iteration = 4760, accuracy = 83.94903566102509, loss = 0.3685173758149508\n",
      "Iteration = 4770, accuracy = 83.95003653178264, loss = 0.3684557961041428\n",
      "Iteration = 4780, accuracy = 83.94603304875241, loss = 0.3683946800075816\n",
      "Iteration = 4790, accuracy = 83.95103740254021, loss = 0.36833402324382036\n",
      "Iteration = 4800, accuracy = 83.95404001481289, loss = 0.3682738215771603\n",
      "Iteration = 4810, accuracy = 83.95904436860067, loss = 0.36821407081707\n",
      "Iteration = 4820, accuracy = 83.9610461101158, loss = 0.3681547668176132\n",
      "Iteration = 4830, accuracy = 83.96504959314603, loss = 0.36809590547688453\n",
      "Iteration = 4840, accuracy = 83.96304785163092, loss = 0.3680374827364555\n",
      "Iteration = 4850, accuracy = 83.96304785163092, loss = 0.3679794945808287\n",
      "Iteration = 4860, accuracy = 83.96404872238847, loss = 0.3679219370369002\n",
      "Iteration = 4870, accuracy = 83.96705133466116, loss = 0.3678648061734288\n",
      "Iteration = 4880, accuracy = 83.9710548176914, loss = 0.3678080981005156\n",
      "Iteration = 4890, accuracy = 83.97405742996406, loss = 0.3677518089690904\n",
      "Iteration = 4900, accuracy = 83.98106352526699, loss = 0.36769593497040504\n",
      "Iteration = 4910, accuracy = 83.98706874981234, loss = 0.3676404723355361\n",
      "Iteration = 4920, accuracy = 83.98806962056989, loss = 0.36758541733489436\n",
      "Iteration = 4930, accuracy = 83.99307397435769, loss = 0.3675307662777405\n",
      "Iteration = 4940, accuracy = 83.99407484511525, loss = 0.3674765155117108\n",
      "Iteration = 4950, accuracy = 83.99607658663037, loss = 0.3674226614223466\n",
      "Iteration = 4960, accuracy = 83.99807832814548, loss = 0.3673692004326335\n",
      "Iteration = 4970, accuracy = 84.00208181117573, loss = 0.3673161290025455\n",
      "Iteration = 4980, accuracy = 84.00608529420596, loss = 0.3672634436285975\n",
      "Iteration = 4990, accuracy = 84.00808703572108, loss = 0.3672111408434035\n",
      "Cross Polynomial expansion of degree 1 done : adding (227458, 120)\n",
      "Logarithmic expansion of degree 1 done : adding (227458, 16)\n"
=======
      "Iteration = 0, accuracy = 27.355799545604675, loss = 4.873196092496017\n",
      "Iteration = 10, accuracy = 29.419595047691494, loss = 4.179022439433003\n",
      "Iteration = 20, accuracy = 32.112938256282966, loss = 3.5307493515776085\n",
      "Iteration = 30, accuracy = 35.33374035410807, loss = 2.9554653891757177\n",
      "Iteration = 40, accuracy = 39.20510844434658, loss = 2.471933032252379\n",
      "Iteration = 50, accuracy = 43.218600182158475, loss = 2.0869651973936225\n",
      "Iteration = 60, accuracy = 46.84975929058281, loss = 1.792437810426043\n",
      "Iteration = 70, accuracy = 49.800326283866966, loss = 1.5678536064320647\n",
      "Iteration = 80, accuracy = 52.171389108524416, loss = 1.3918634433277213\n",
      "Iteration = 90, accuracy = 54.28622901924675, loss = 1.2524443198047621\n",
      "Iteration = 100, accuracy = 56.14184340376127, loss = 1.1415184683864295\n",
      "Iteration = 110, accuracy = 57.933402059792016, loss = 1.0528169781656658\n",
      "Iteration = 120, accuracy = 59.49776305385686, loss = 0.9817662595270782\n",
      "Iteration = 130, accuracy = 60.91199343428782, loss = 0.924545034758663\n",
      "Iteration = 140, accuracy = 62.105031377298246, loss = 0.8778040266465021\n",
      "Iteration = 150, accuracy = 63.312081510914496, loss = 0.8392441779976371\n",
      "Iteration = 160, accuracy = 64.36299580635153, loss = 0.8067874181247235\n",
      "Iteration = 170, accuracy = 65.33384044118384, loss = 0.7792586817843337\n",
      "Iteration = 180, accuracy = 66.14754836707935, loss = 0.7555344714463337\n",
      "Iteration = 190, accuracy = 66.99128241570166, loss = 0.7346243665777735\n",
      "Iteration = 200, accuracy = 67.72792329326514, loss = 0.7160481347696029\n",
      "Iteration = 210, accuracy = 68.41251889143555, loss = 0.6993721107469252\n",
      "Iteration = 220, accuracy = 69.02104831203147, loss = 0.6842591986861406\n",
      "Iteration = 230, accuracy = 69.64158818171809, loss = 0.6704379672831328\n",
      "Iteration = 240, accuracy = 70.19206709837559, loss = 0.6576517029084076\n",
      "Iteration = 250, accuracy = 70.72352947063946, loss = 0.6457694614708015\n",
      "Iteration = 260, accuracy = 71.22296397866144, loss = 0.6347663638748344\n",
      "Iteration = 270, accuracy = 71.64933492138161, loss = 0.6244458366568703\n",
      "Iteration = 280, accuracy = 72.07670673485933, loss = 0.614793414889828\n",
      "Iteration = 290, accuracy = 72.48906548697367, loss = 0.605690918676423\n",
      "Iteration = 300, accuracy = 72.90542772211825, loss = 0.5971399350670156\n",
      "Iteration = 310, accuracy = 73.2587350995366, loss = 0.5891064921006048\n",
      "Iteration = 320, accuracy = 73.64407034119684, loss = 0.5814489180509589\n",
      "Iteration = 330, accuracy = 73.99337423558497, loss = 0.5742637762036752\n",
      "Iteration = 340, accuracy = 74.28763023830733, loss = 0.5674210583753382\n",
      "Iteration = 350, accuracy = 74.60090278542332, loss = 0.5609331616677514\n",
      "Iteration = 360, accuracy = 74.87414050223694, loss = 0.554767798632203\n",
      "Iteration = 370, accuracy = 75.19441914465585, loss = 0.5489148772226019\n",
      "Iteration = 380, accuracy = 75.47866643980264, loss = 0.5433478783633393\n",
      "Iteration = 390, accuracy = 75.73989370752555, loss = 0.5380536184876982\n",
      "Iteration = 400, accuracy = 76.02814448570257, loss = 0.5330093302504914\n",
      "Iteration = 410, accuracy = 76.29337523645572, loss = 0.5282035621390011\n",
      "Iteration = 420, accuracy = 76.51156506160359, loss = 0.5236120450750756\n",
      "Iteration = 430, accuracy = 76.73876272356951, loss = 0.519242034073431\n",
      "Iteration = 440, accuracy = 76.95795341947495, loss = 0.5150480815078662\n",
      "Iteration = 450, accuracy = 77.18515108144085, loss = 0.5110462892442186\n",
      "Iteration = 460, accuracy = 77.40834526037654, loss = 0.5072228676598021\n",
      "Iteration = 470, accuracy = 77.63254031006977, loss = 0.5035582038395581\n",
      "Iteration = 480, accuracy = 77.81870227097575, loss = 0.5000507412036534\n",
      "Iteration = 490, accuracy = 78.01086945642709, loss = 0.49669396195939935\n",
      "Iteration = 500, accuracy = 78.20704012490867, loss = 0.4934689859103535\n",
      "Iteration = 510, accuracy = 78.3871968612693, loss = 0.4903806009400736\n",
      "Iteration = 520, accuracy = 78.51630918899443, loss = 0.48741684272494024\n",
      "Iteration = 530, accuracy = 78.66643980262829, loss = 0.4845723681979898\n",
      "Iteration = 540, accuracy = 78.81356780398947, loss = 0.4818408860881468\n",
      "Iteration = 550, accuracy = 78.94768448550239, loss = 0.4792169232264559\n",
      "Iteration = 560, accuracy = 79.07179245943972, loss = 0.47669331044176383\n",
      "Iteration = 570, accuracy = 79.2019056579224, loss = 0.47426834866972956\n",
      "Iteration = 580, accuracy = 79.34202756398066, loss = 0.47193629832905093\n",
      "Iteration = 590, accuracy = 79.48515208231161, loss = 0.4696923943294748\n",
      "Iteration = 600, accuracy = 79.60325483170358, loss = 0.4675320167797844\n",
      "Iteration = 610, accuracy = 79.720356710338, loss = 0.46545184137901013\n",
      "Iteration = 620, accuracy = 79.84446468427532, loss = 0.46344938234737243\n",
      "Iteration = 630, accuracy = 79.93154044018296, loss = 0.4615194026959633\n",
      "Iteration = 640, accuracy = 80.05464754336272, loss = 0.45965993403726546\n",
      "Iteration = 650, accuracy = 80.14372504078548, loss = 0.4578676577255851\n",
      "Iteration = 660, accuracy = 80.2338034089658, loss = 0.45613916832568463\n",
      "Iteration = 670, accuracy = 80.3118713280554, loss = 0.45447247214314024\n",
      "Iteration = 680, accuracy = 80.39794621320549, loss = 0.4528643136652048\n",
      "Iteration = 690, accuracy = 80.47801587381022, loss = 0.4513126619493677\n",
      "Iteration = 700, accuracy = 80.56709337123297, loss = 0.44981495059995585\n",
      "Iteration = 710, accuracy = 80.6401569365348, loss = 0.4483690191822646\n",
      "Iteration = 720, accuracy = 80.68219350835226, loss = 0.44697272959358564\n",
      "Iteration = 730, accuracy = 80.71422137259415, loss = 0.44562405703530505\n",
      "Iteration = 740, accuracy = 80.79629277471399, loss = 0.4443210702032916\n",
      "Iteration = 750, accuracy = 80.85334240789487, loss = 0.4430619265342545\n",
      "Iteration = 760, accuracy = 80.90738942880306, loss = 0.4418448671101895\n",
      "Iteration = 770, accuracy = 80.96944341577172, loss = 0.4406682120160917\n",
      "Iteration = 780, accuracy = 81.00347302152872, loss = 0.43953013026213766\n",
      "Iteration = 790, accuracy = 81.06752875001251, loss = 0.43842914220315626\n",
      "Iteration = 800, accuracy = 81.12357751243582, loss = 0.4373639516867359\n",
      "Iteration = 810, accuracy = 81.15860798895038, loss = 0.43633315263262507\n",
      "Iteration = 820, accuracy = 81.19564020698007, loss = 0.4353353995052291\n",
      "Iteration = 830, accuracy = 81.25669332319117, loss = 0.4343694050429444\n",
      "Iteration = 840, accuracy = 81.2927246704633, loss = 0.43343393694784743\n",
      "Iteration = 850, accuracy = 81.32875601773543, loss = 0.43252781505904236\n",
      "Iteration = 860, accuracy = 81.36078388197731, loss = 0.431649908930286\n",
      "Iteration = 870, accuracy = 81.38980913394653, loss = 0.4307991355818373\n",
      "Iteration = 880, accuracy = 81.41583177364306, loss = 0.42997445738340234\n",
      "Iteration = 890, accuracy = 81.45286399167276, loss = 0.42917488005412985\n",
      "Iteration = 900, accuracy = 81.48188924364197, loss = 0.4283994507685921\n",
      "Iteration = 910, accuracy = 81.49790317576291, loss = 0.427647256359227\n",
      "Iteration = 920, accuracy = 81.5349353937926, loss = 0.4269174216069705\n",
      "Iteration = 930, accuracy = 81.56596238727694, loss = 0.4262091076129629\n",
      "Iteration = 940, accuracy = 81.60199373454905, loss = 0.42552151024514284\n",
      "Iteration = 950, accuracy = 81.64302943560898, loss = 0.4248538586544266\n",
      "Iteration = 960, accuracy = 81.6670503337904, loss = 0.4242054138558811\n",
      "Iteration = 970, accuracy = 81.68506600742647, loss = 0.42357546737090557\n",
      "Iteration = 980, accuracy = 81.71409125939567, loss = 0.42296333992696944\n",
      "Iteration = 990, accuracy = 81.74611912363756, loss = 0.4223683802118793\n",
      "Iteration = 1000, accuracy = 81.76713740954631, loss = 0.4217899636799032\n",
      "Iteration = 1010, accuracy = 81.79015743697016, loss = 0.42122749140738125\n",
      "Iteration = 1020, accuracy = 81.80316875681844, loss = 0.4206803889956831\n",
      "Iteration = 1030, accuracy = 81.82718965499986, loss = 0.4201481055195767\n",
      "Iteration = 1040, accuracy = 81.84920881166616, loss = 0.4196301125192164\n",
      "Iteration = 1050, accuracy = 81.87823406363536, loss = 0.419125903034092\n",
      "Iteration = 1060, accuracy = 81.89524886651387, loss = 0.4186349906773748\n",
      "Iteration = 1070, accuracy = 81.90926105711969, loss = 0.4181569087491789\n",
      "Iteration = 1080, accuracy = 81.92027063545284, loss = 0.41769120938732424\n",
      "Iteration = 1090, accuracy = 81.9432906628767, loss = 0.41723746275423\n",
      "Iteration = 1100, accuracy = 81.96430894878544, loss = 0.41679525625862196\n",
      "Iteration = 1110, accuracy = 81.98232462242152, loss = 0.4163641938107611\n",
      "Iteration = 1120, accuracy = 81.99933942530001, loss = 0.4159438951099533\n",
      "Iteration = 1130, accuracy = 82.00634552060292, loss = 0.41553399496311166\n",
      "Iteration = 1140, accuracy = 82.0193568404512, loss = 0.4151341426331677\n",
      "Iteration = 1150, accuracy = 82.04037512635993, loss = 0.4147439109871019\n",
      "Iteration = 1160, accuracy = 82.06339515378379, loss = 0.4143628083378656\n",
      "Iteration = 1170, accuracy = 82.09041866423789, loss = 0.41399078632032893\n",
      "Iteration = 1180, accuracy = 82.10743346711638, loss = 0.4136275459995771\n",
      "Iteration = 1190, accuracy = 82.1294526237827, loss = 0.4132728000477174\n",
      "Iteration = 1200, accuracy = 82.15447439272167, loss = 0.41292627224791384\n",
      "Iteration = 1210, accuracy = 82.17349093711529, loss = 0.4125876970206863\n",
      "Iteration = 1220, accuracy = 82.19651096453914, loss = 0.4122568189715086\n",
      "Iteration = 1230, accuracy = 82.20651967211474, loss = 0.4119333924587721\n",
      "Iteration = 1240, accuracy = 82.22453534575081, loss = 0.4116171811812032\n",
      "Iteration = 1250, accuracy = 82.2415501486293, loss = 0.4113079577838712\n",
      "Iteration = 1260, accuracy = 82.24855624393223, loss = 0.411005503481933\n",
      "Iteration = 1270, accuracy = 82.25956582226537, loss = 0.41070960770131615\n",
      "Iteration = 1280, accuracy = 82.27958323741656, loss = 0.41042006773555295\n",
      "Iteration = 1290, accuracy = 82.28658933271946, loss = 0.4101366884180205\n",
      "Iteration = 1300, accuracy = 82.29759891105262, loss = 0.40985928180886794\n",
      "Iteration = 1310, accuracy = 82.31961806771892, loss = 0.4095876668959419\n",
      "Iteration = 1320, accuracy = 82.3346311290823, loss = 0.4093216693090496\n",
      "Iteration = 1330, accuracy = 82.35264680271837, loss = 0.40906112104692555\n",
      "Iteration = 1340, accuracy = 82.3836737962027, loss = 0.40880586021629955\n",
      "Iteration = 1350, accuracy = 82.40168946983876, loss = 0.40855573078247454\n",
      "Iteration = 1360, accuracy = 82.42871298029286, loss = 0.4083105823308589\n",
      "Iteration = 1370, accuracy = 82.43571907559577, loss = 0.4080702698389153\n",
      "Iteration = 1380, accuracy = 82.44272517089868, loss = 0.40783465345799447\n",
      "Iteration = 1390, accuracy = 82.4617417152923, loss = 0.4076035983045461\n",
      "Iteration = 1400, accuracy = 82.47275129362546, loss = 0.4073769742602031\n",
      "Iteration = 1410, accuracy = 82.48476174271616, loss = 0.40715465578022897\n",
      "Iteration = 1420, accuracy = 82.49376957953419, loss = 0.40693652170981176\n",
      "Iteration = 1430, accuracy = 82.50377828710978, loss = 0.40672245510765265\n",
      "Iteration = 1440, accuracy = 82.51578873620049, loss = 0.4065123430762462\n",
      "Iteration = 1450, accuracy = 82.532803539079, loss = 0.4063060765981542\n",
      "Iteration = 1460, accuracy = 82.55682443726042, loss = 0.4061035503774388\n",
      "Iteration = 1470, accuracy = 82.56883488635113, loss = 0.4059046626852006\n",
      "Iteration = 1480, accuracy = 82.57183749862381, loss = 0.40570931520789555\n",
      "Iteration = 1490, accuracy = 82.58785143074475, loss = 0.40551741289678966\n",
      "Iteration = 1500, accuracy = 82.59986187983546, loss = 0.40532886381676647\n",
      "Iteration = 1510, accuracy = 82.61287319968372, loss = 0.4051435789932333\n",
      "Iteration = 1520, accuracy = 82.61887842422908, loss = 0.4049614722581491\n",
      "Iteration = 1530, accuracy = 82.62688539028954, loss = 0.40478246010159513\n",
      "Iteration = 1540, accuracy = 82.63689409786514, loss = 0.4046064615437306\n",
      "Iteration = 1550, accuracy = 82.65290802998608, loss = 0.4044333980478549\n",
      "Iteration = 1560, accuracy = 82.65390890074364, loss = 0.404263193487082\n",
      "Iteration = 1570, accuracy = 82.66291673756167, loss = 0.4040957741503883\n",
      "Iteration = 1580, accuracy = 82.6699228328646, loss = 0.4039310687487586\n",
      "Iteration = 1590, accuracy = 82.67792979892506, loss = 0.4037690083868728\n",
      "Iteration = 1600, accuracy = 82.68293415271286, loss = 0.40360952649524096\n",
      "Iteration = 1610, accuracy = 82.68793850650066, loss = 0.40345255873998886\n",
      "Iteration = 1620, accuracy = 82.68793850650066, loss = 0.4032980429283667\n",
      "Iteration = 1630, accuracy = 82.69794721407625, loss = 0.4031459189187738\n",
      "Iteration = 1640, accuracy = 82.7039524386216, loss = 0.40299612853697864\n",
      "Iteration = 1650, accuracy = 82.70695505089428, loss = 0.4028486154976642\n",
      "Iteration = 1660, accuracy = 82.70695505089428, loss = 0.4027033253304666\n",
      "Iteration = 1670, accuracy = 82.71596288771231, loss = 0.40256020530955394\n",
      "Iteration = 1680, accuracy = 82.72497072453035, loss = 0.40241920438663065\n",
      "Iteration = 1690, accuracy = 82.72797333680302, loss = 0.4022802731270051\n",
      "Iteration = 1700, accuracy = 82.73197681983325, loss = 0.40214336364852\n",
      "Iteration = 1710, accuracy = 82.73998378589373, loss = 0.40200842956317945\n",
      "Iteration = 1720, accuracy = 82.74598901043908, loss = 0.4018754259213131\n",
      "Iteration = 1730, accuracy = 82.7579994595298, loss = 0.4017443091581395\n",
      "Iteration = 1740, accuracy = 82.76200294256003, loss = 0.4016150370426006\n",
      "Iteration = 1750, accuracy = 82.76800816710538, loss = 0.4014875686283504\n",
      "Iteration = 1760, accuracy = 82.77401339165074, loss = 0.4013618642067937\n",
      "Iteration = 1770, accuracy = 82.77301252089318, loss = 0.40123788526206733\n",
      "Iteration = 1780, accuracy = 82.77501426240829, loss = 0.4011155944278796\n",
      "Iteration = 1790, accuracy = 82.78302122846877, loss = 0.4009949554461123\n",
      "Iteration = 1800, accuracy = 82.78001861619609, loss = 0.40087593312711145\n",
      "Iteration = 1810, accuracy = 82.7820203577112, loss = 0.40075849331158087\n",
      "Iteration = 1820, accuracy = 82.78502296998388, loss = 0.4006426028340146\n",
      "Iteration = 1830, accuracy = 82.78902645301413, loss = 0.4005282294875922\n",
      "Iteration = 1840, accuracy = 82.7920290652868, loss = 0.4004153419904776\n",
      "Iteration = 1850, accuracy = 82.79403080680193, loss = 0.4003039099534538\n",
      "Iteration = 1860, accuracy = 82.80504038513506, loss = 0.40019390384884\n",
      "Iteration = 1870, accuracy = 82.80203777286239, loss = 0.4000852949806337\n",
      "Iteration = 1880, accuracy = 82.80804299740775, loss = 0.39997805545582255\n",
      "Iteration = 1890, accuracy = 82.81104560968042, loss = 0.39987215815681915\n",
      "Iteration = 1900, accuracy = 82.81604996346822, loss = 0.3997675767149679\n",
      "Iteration = 1910, accuracy = 82.82305605877113, loss = 0.3996642854850797\n",
      "Iteration = 1920, accuracy = 82.83106302483161, loss = 0.3995622595209507\n",
      "Iteration = 1930, accuracy = 82.83606737861939, loss = 0.39946147455182235\n",
      "Iteration = 1940, accuracy = 82.84007086164964, loss = 0.3993619069597447\n",
      "Iteration = 1950, accuracy = 82.84207260316475, loss = 0.3992635337578048\n",
      "Iteration = 1960, accuracy = 82.85007956922522, loss = 0.3991663325691818\n",
      "Iteration = 1970, accuracy = 82.84707695695255, loss = 0.3990702816069983\n",
      "Iteration = 1980, accuracy = 82.84707695695255, loss = 0.39897535965493075\n",
      "Iteration = 1990, accuracy = 82.85007956922522, loss = 0.39888154604855114\n",
      "Iteration = 2000, accuracy = 82.85408305225546, loss = 0.39878882065736687\n",
      "Iteration = 2010, accuracy = 82.85708566452814, loss = 0.3986971638675319\n",
      "Iteration = 2020, accuracy = 82.85608479377058, loss = 0.3986065565652019\n",
      "Iteration = 2030, accuracy = 82.85108043998278, loss = 0.3985169801205048\n",
      "Iteration = 2040, accuracy = 82.85508392301301, loss = 0.398428416372105\n",
      "Iteration = 2050, accuracy = 82.8650926305886, loss = 0.39834084761233446\n",
      "Iteration = 2060, accuracy = 82.86409175983106, loss = 0.398254256572869\n",
      "Iteration = 2070, accuracy = 82.8700969843764, loss = 0.3981686264109281\n",
      "Iteration = 2080, accuracy = 82.87109785513397, loss = 0.39808394069597625\n",
      "Iteration = 2090, accuracy = 82.87710307967933, loss = 0.39800018339690646\n",
      "Iteration = 2100, accuracy = 82.88210743346711, loss = 0.39791733886968667\n",
      "Iteration = 2110, accuracy = 82.88310830422468, loss = 0.39783539184544975\n",
      "Iteration = 2120, accuracy = 82.88811265801247, loss = 0.3977543274190107\n",
      "Iteration = 2130, accuracy = 82.89311701180027, loss = 0.39767408144618516\n",
      "Iteration = 2140, accuracy = 82.89311701180027, loss = 0.3975945756589952\n",
      "Iteration = 2150, accuracy = 82.90012310710318, loss = 0.39751591201683123\n",
      "Iteration = 2160, accuracy = 82.90312571937586, loss = 0.3974380769366478\n",
      "Iteration = 2170, accuracy = 82.90412659013342, loss = 0.39736105714763637\n",
      "Iteration = 2180, accuracy = 82.90813007316366, loss = 0.39728483968197825\n",
      "Iteration = 2190, accuracy = 82.9121335561939, loss = 0.39720941186591746\n",
      "Iteration = 2200, accuracy = 82.9121335561939, loss = 0.397134761311146\n",
      "Iteration = 2210, accuracy = 82.92014052225436, loss = 0.39706087590648476\n",
      "Iteration = 2220, accuracy = 82.92214226376949, loss = 0.3969877438098523\n",
      "Iteration = 2230, accuracy = 82.92714661755728, loss = 0.3969153534405087\n",
      "Iteration = 2240, accuracy = 82.93115010058752, loss = 0.39684369347156456\n",
      "Iteration = 2250, accuracy = 82.93315184210263, loss = 0.39677275282274443\n",
      "Iteration = 2260, accuracy = 82.93415271286018, loss = 0.3967025206533954\n",
      "Iteration = 2270, accuracy = 82.93315184210263, loss = 0.39663298635573224\n",
      "Iteration = 2280, accuracy = 82.92914835907239, loss = 0.3965641395483094\n",
      "Iteration = 2290, accuracy = 82.92914835907239, loss = 0.3964959700697107\n",
      "Iteration = 2300, accuracy = 82.92914835907239, loss = 0.39642846797244924\n",
      "Iteration = 2310, accuracy = 82.93415271286018, loss = 0.39636162351707166\n",
      "Iteration = 2320, accuracy = 82.93115010058752, loss = 0.39629542716645444\n",
      "Iteration = 2330, accuracy = 82.93515358361775, loss = 0.3962298695802905\n",
      "Iteration = 2340, accuracy = 82.93915706664798, loss = 0.3961649416097538\n",
      "Iteration = 2350, accuracy = 82.94316054967823, loss = 0.3961006342923411\n",
      "Iteration = 2360, accuracy = 82.94716403270846, loss = 0.3960369388468776\n",
      "Iteration = 2370, accuracy = 82.95216838649625, loss = 0.39597384666868735\n",
      "Iteration = 2380, accuracy = 82.9581736110416, loss = 0.39591134932491734\n",
      "Iteration = 2390, accuracy = 82.96618057710208, loss = 0.39584943855001226\n",
      "Iteration = 2400, accuracy = 82.97418754316254, loss = 0.395788106241333\n",
      "Iteration = 2410, accuracy = 82.97819102619279, loss = 0.3957273444549155\n",
      "Iteration = 2420, accuracy = 82.97919189695034, loss = 0.3956671454013624\n",
      "Iteration = 2430, accuracy = 82.98219450922304, loss = 0.39560750144186546\n",
      "Iteration = 2440, accuracy = 82.9801927677079, loss = 0.39554840508435124\n",
      "Iteration = 2450, accuracy = 82.98319537998057, loss = 0.39548984897974876\n",
      "Iteration = 2460, accuracy = 82.98819973376837, loss = 0.3954318259183715\n",
      "Iteration = 2470, accuracy = 82.99120234604106, loss = 0.39537432882641227\n",
      "Iteration = 2480, accuracy = 82.9902014752835, loss = 0.3953173507625461\n",
      "Iteration = 2490, accuracy = 82.9952058290713, loss = 0.3952608849146363\n",
      "Iteration = 2500, accuracy = 83.00021018285909, loss = 0.39520492459654233\n",
      "Iteration = 2510, accuracy = 83.00021018285909, loss = 0.39514946324502326\n",
      "Iteration = 2520, accuracy = 83.00521453664689, loss = 0.3950944944167361\n",
      "Iteration = 2530, accuracy = 83.00421366588932, loss = 0.3950400117853234\n",
      "Iteration = 2540, accuracy = 83.01021889043467, loss = 0.3949860091385886\n",
      "Iteration = 2550, accuracy = 83.01522324422247, loss = 0.3949324803757557\n",
      "Iteration = 2560, accuracy = 83.01922672725271, loss = 0.3948794195048092\n",
      "Iteration = 2570, accuracy = 83.01922672725271, loss = 0.3948268206399144\n",
      "Iteration = 2580, accuracy = 83.02122846876783, loss = 0.3947746779989119\n",
      "Iteration = 2590, accuracy = 83.02222933952538, loss = 0.39472298590088567\n",
      "Iteration = 2600, accuracy = 83.02623282255563, loss = 0.39467173876380274\n",
      "Iteration = 2610, accuracy = 83.02723369331318, loss = 0.3946209311022201\n",
      "Iteration = 2620, accuracy = 83.02723369331318, loss = 0.3945705575250586\n",
      "Iteration = 2630, accuracy = 83.0242310810405, loss = 0.3945206127334407\n",
      "Iteration = 2640, accuracy = 83.02323021028295, loss = 0.39447109151858845\n",
      "Iteration = 2650, accuracy = 83.02323021028295, loss = 0.3944219887597838\n",
      "Iteration = 2660, accuracy = 83.0172249857376, loss = 0.39437329942238386\n",
      "Iteration = 2670, accuracy = 83.01822585649515, loss = 0.39432501855589436\n",
      "Iteration = 2680, accuracy = 83.0242310810405, loss = 0.39427714129209496\n",
      "Iteration = 2690, accuracy = 83.03023630558586, loss = 0.39422966284321864\n",
      "Iteration = 2700, accuracy = 83.02823456407074, loss = 0.3941825785001799\n",
      "Iteration = 2710, accuracy = 83.0292354348283, loss = 0.39413588363085317\n",
      "Iteration = 2720, accuracy = 83.03223804710098, loss = 0.39408957367839775\n",
      "Iteration = 2730, accuracy = 83.03323891785853, loss = 0.3940436441596285\n",
      "Iteration = 2740, accuracy = 83.03724240088877, loss = 0.3939980906634309\n",
      "Iteration = 2750, accuracy = 83.03524065937366, loss = 0.39395290884921963\n",
      "Iteration = 2760, accuracy = 83.03824327164632, loss = 0.3939080944454371\n",
      "Iteration = 2770, accuracy = 83.04124588391902, loss = 0.393863643248094\n",
      "Iteration = 2780, accuracy = 83.04124588391902, loss = 0.39381955111934724\n",
      "Iteration = 2790, accuracy = 83.04024501316145, loss = 0.3937758139861162\n",
      "Iteration = 2800, accuracy = 83.04625023770681, loss = 0.3937324278387361\n",
      "Iteration = 2810, accuracy = 83.04324762543412, loss = 0.3936893887296446\n",
      "Iteration = 2820, accuracy = 83.04424849619168, loss = 0.3936466927721059\n",
      "Iteration = 2830, accuracy = 83.04725110846437, loss = 0.3936043361389645\n",
      "Iteration = 2840, accuracy = 83.04725110846437, loss = 0.39356231506143413\n",
      "Iteration = 2850, accuracy = 83.05125459149461, loss = 0.39352062582791614\n",
      "Iteration = 2860, accuracy = 83.05225546225216, loss = 0.393479264782848\n",
      "Iteration = 2870, accuracy = 83.05425720376728, loss = 0.39343822832558234\n",
      "Iteration = 2880, accuracy = 83.05325633300971, loss = 0.39339751290929326\n",
      "Iteration = 2890, accuracy = 83.05525807452484, loss = 0.3933571150399111\n",
      "Iteration = 2900, accuracy = 83.06026242831264, loss = 0.39331703127508283\n",
      "Iteration = 2910, accuracy = 83.06226416982774, loss = 0.39327725822315923\n",
      "Iteration = 2920, accuracy = 83.06526678210042, loss = 0.39323779254220675\n",
      "Iteration = 2930, accuracy = 83.06526678210042, loss = 0.39319863093904467\n",
      "Iteration = 2940, accuracy = 83.06426591134287, loss = 0.39315977016830406\n",
      "Iteration = 2950, accuracy = 83.0632650405853, loss = 0.39312120703151154\n",
      "Iteration = 2960, accuracy = 83.06626765285799, loss = 0.3930829383761951\n",
      "Iteration = 2970, accuracy = 83.0682693943731, loss = 0.39304496109500997\n",
      "Iteration = 2980, accuracy = 83.07027113588822, loss = 0.3930072721248873\n",
      "Iteration = 2990, accuracy = 83.07527548967602, loss = 0.39296986844620285\n"
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
     ]
    }
   ],
   "source": [
    "y_pred_zero, weights_zero, loss_zero = train_predict(X_train_zero, y_train_zero, X_test_zero, max_iters=5000, degree=degree, lambda_=0.001, imputable_th=0.5)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
=======
   "execution_count": 8,
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Cross Polynomial expansion of degree 1 done : adding (77544, 253)\n",
      "Logarithmic expansion of degree 1 done : adding (77544, 23)\n",
      "Iteration = 0, accuracy = 35.73455070669556, loss = 12.787676070269784\n",
      "Iteration = 10, accuracy = 35.73455070669556, loss = 12.400580498308672\n",
      "Iteration = 20, accuracy = 35.73455070669556, loss = 11.407840699254594\n",
      "Iteration = 30, accuracy = 35.801609408851746, loss = 9.78340327238678\n",
      "Iteration = 40, accuracy = 37.16728567007119, loss = 7.735851458728391\n",
      "Iteration = 50, accuracy = 41.41261735272877, loss = 5.8981451856726395\n",
      "Iteration = 60, accuracy = 46.3826988548437, loss = 4.785964778533652\n",
      "Iteration = 70, accuracy = 50.28113071288559, loss = 4.240606504663877\n",
      "Iteration = 80, accuracy = 52.51341174043124, loss = 3.959615164658513\n",
      "Iteration = 90, accuracy = 54.03512844320644, loss = 3.788879006955929\n",
      "Iteration = 100, accuracy = 55.08098627875787, loss = 3.6682103099275087\n",
      "Iteration = 110, accuracy = 55.82250077375426, loss = 3.5714285187754555\n",
      "Iteration = 120, accuracy = 56.47890230062932, loss = 3.485590147864598\n",
      "Iteration = 130, accuracy = 57.10177447642629, loss = 3.4056987836639996\n",
      "Iteration = 140, accuracy = 57.65887754049314, loss = 3.3296402981054385\n",
      "Iteration = 150, accuracy = 58.088311152378004, loss = 3.255673049991966\n",
      "Iteration = 160, accuracy = 58.47776746105436, loss = 3.1824971437102363\n",
      "Iteration = 170, accuracy = 58.90462189208707, loss = 3.110987551590086\n",
      "Iteration = 180, accuracy = 59.277313525224386, loss = 3.0415146929862535\n",
      "Iteration = 190, accuracy = 59.619054988135765, loss = 2.974096487873375\n",
      "Iteration = 200, accuracy = 59.98916744042092, loss = 2.909032898192761\n",
      "Iteration = 210, accuracy = 60.306406685236766, loss = 2.8463276843159413\n",
      "Iteration = 220, accuracy = 60.6442793768699, loss = 2.785600221432582\n",
      "Iteration = 230, accuracy = 60.97312493552048, loss = 2.727133126440215\n",
      "Iteration = 240, accuracy = 61.24651810584958, loss = 2.671021833023294\n",
      "Iteration = 250, accuracy = 61.589549159187044, loss = 2.6169142527014477\n",
      "Iteration = 260, accuracy = 61.86810069122048, loss = 2.5645658969814695\n",
      "Iteration = 270, accuracy = 62.129887547714844, loss = 2.5139065241824103\n",
      "Iteration = 280, accuracy = 62.43552047869597, loss = 2.4649231599992305\n",
      "Iteration = 290, accuracy = 62.677963478799136, loss = 2.4178961871392914\n",
      "Iteration = 300, accuracy = 62.95909419168472, loss = 2.3725870651895864\n",
      "Iteration = 310, accuracy = 63.19766842050964, loss = 2.3291454520298926\n",
      "Iteration = 320, accuracy = 63.40271329825647, loss = 2.287513138771396\n",
      "Iteration = 330, accuracy = 63.679975239863815, loss = 2.2475770546788194\n",
      "Iteration = 340, accuracy = 63.93660373465387, loss = 2.2091759550365295\n",
      "Iteration = 350, accuracy = 64.15970287836583, loss = 2.1722244727865565\n",
      "Iteration = 360, accuracy = 64.35958939440833, loss = 2.1366260167210784\n",
      "Iteration = 370, accuracy = 64.59171567110286, loss = 2.1022776843144344\n",
      "Iteration = 380, accuracy = 64.84318580418858, loss = 2.069197014074292\n",
      "Iteration = 390, accuracy = 65.09078716599608, loss = 2.0372605090103835\n",
      "Iteration = 400, accuracy = 65.33709893737749, loss = 2.0065345949886746\n",
      "Iteration = 410, accuracy = 65.55246053853297, loss = 1.976822395340618\n",
      "Iteration = 420, accuracy = 65.70076343753225, loss = 1.9480770276273842\n",
      "Iteration = 430, accuracy = 65.89420200144434, loss = 1.9202986934134063\n",
      "Iteration = 440, accuracy = 66.09537810791292, loss = 1.8933924556485717\n",
      "Iteration = 450, accuracy = 66.28365831012071, loss = 1.8673219788133268\n",
      "Iteration = 460, accuracy = 66.48096564531104, loss = 1.842048322893842\n",
      "Iteration = 470, accuracy = 66.6705354379449, loss = 1.817507967564583\n",
      "Iteration = 480, accuracy = 66.85881564015268, loss = 1.7936835213671933\n",
      "Iteration = 490, accuracy = 67.03806870937791, loss = 1.7705654984851085\n",
      "Iteration = 500, accuracy = 67.22763850201177, loss = 1.7481098890296602\n",
      "Iteration = 510, accuracy = 67.42623542762819, loss = 1.7262537287453386\n",
      "Iteration = 520, accuracy = 67.58614464046218, loss = 1.7049967088727325\n",
      "Iteration = 530, accuracy = 67.73057876818322, loss = 1.6843149633734893\n",
      "Iteration = 540, accuracy = 67.92659651294748, loss = 1.6641171205672298\n",
      "Iteration = 550, accuracy = 68.08779531620758, loss = 1.6444283370030335\n",
      "Iteration = 560, accuracy = 68.24770452904157, loss = 1.6252528016329024\n",
      "Iteration = 570, accuracy = 68.38955947591046, loss = 1.6065621897646738\n",
      "Iteration = 580, accuracy = 68.51207056638812, loss = 1.5883350841659\n",
      "Iteration = 590, accuracy = 68.65779428453523, loss = 1.5705295291388026\n",
      "Iteration = 600, accuracy = 68.76998865160425, loss = 1.5531134364981063\n",
      "Iteration = 610, accuracy = 68.92860827401218, loss = 1.5361073520280406\n",
      "Iteration = 620, accuracy = 69.05498813576808, loss = 1.5194865791491394\n",
      "Iteration = 630, accuracy = 69.18523676880223, loss = 1.5032208563270761\n",
      "Iteration = 640, accuracy = 69.32451253481894, loss = 1.4873496178047414\n",
      "Iteration = 650, accuracy = 69.45476116785308, loss = 1.4718523311912235\n",
      "Iteration = 660, accuracy = 69.58500980088725, loss = 1.4567259131825396\n",
      "Iteration = 670, accuracy = 69.70752089136491, loss = 1.4419492184853147\n",
      "Iteration = 680, accuracy = 69.81197771587743, loss = 1.4275201261872066\n",
      "Iteration = 690, accuracy = 69.92675126379862, loss = 1.4134278641988711\n",
      "Iteration = 700, accuracy = 70.02347054575468, loss = 1.3996614521745585\n",
      "Iteration = 710, accuracy = 70.13824409367585, loss = 1.3862073785107638\n",
      "Iteration = 720, accuracy = 70.2014340245538, loss = 1.3730420378678094\n",
      "Iteration = 730, accuracy = 70.29428453523161, loss = 1.3601559557090115\n",
      "Iteration = 740, accuracy = 70.39358299803983, loss = 1.3475206530068278\n",
      "Iteration = 750, accuracy = 70.52512122150006, loss = 1.3351472172159429\n",
      "Iteration = 760, accuracy = 70.65408026410812, loss = 1.3230025499814784\n",
      "Iteration = 770, accuracy = 70.74306200350769, loss = 1.3110748218618267\n",
      "Iteration = 780, accuracy = 70.84107087588983, loss = 1.2993757571626352\n",
      "Iteration = 790, accuracy = 70.92489425358505, loss = 1.2878827121054952\n",
      "Iteration = 800, accuracy = 71.02677189724544, loss = 1.2765841889217509\n",
      "Iteration = 810, accuracy = 71.1028577323842, loss = 1.2654875432782897\n",
      "Iteration = 820, accuracy = 71.21118332817498, loss = 1.2545644983001025\n",
      "Iteration = 830, accuracy = 71.29113793459197, loss = 1.2438034137949923\n",
      "Iteration = 840, accuracy = 71.403332301661, loss = 1.2332057490891017\n",
      "Iteration = 850, accuracy = 71.50520994532137, loss = 1.2227996998548536\n",
      "Iteration = 860, accuracy = 71.59419168472093, loss = 1.2125822804949002\n",
      "Iteration = 870, accuracy = 71.65222325389456, loss = 1.202531251475256\n",
      "Iteration = 880, accuracy = 71.72057154647685, loss = 1.1926484903091832\n",
      "Iteration = 890, accuracy = 71.80568451459817, loss = 1.1829337527500863\n",
      "Iteration = 900, accuracy = 71.88434953058909, loss = 1.1733697859191399\n",
      "Iteration = 910, accuracy = 71.96301454658001, loss = 1.1639517935320307\n",
      "Iteration = 920, accuracy = 72.00686062106675, loss = 1.1546762067361611\n",
      "Iteration = 930, accuracy = 72.08939440833592, loss = 1.145543412231788\n",
      "Iteration = 940, accuracy = 72.16934901475291, loss = 1.13656086569569\n",
      "Iteration = 950, accuracy = 72.23511812648303, loss = 1.1277269744626923\n",
      "Iteration = 960, accuracy = 72.30346641906532, loss = 1.1190432999273516\n",
      "Iteration = 970, accuracy = 72.3834210254823, loss = 1.110499154198839\n",
      "Iteration = 980, accuracy = 72.47240276488188, loss = 1.102091078489635\n",
      "Iteration = 990, accuracy = 72.54075105746415, loss = 1.0938108116135623\n",
      "Iteration = 1000, accuracy = 72.60265139791602, loss = 1.0856568600668475\n",
      "Iteration = 1010, accuracy = 72.6774476426287, loss = 1.077633174967907\n",
      "Iteration = 1020, accuracy = 72.7380583926545, loss = 1.069730476097963\n",
      "Iteration = 1030, accuracy = 72.78706282884556, loss = 1.0619406550304926\n",
      "Iteration = 1040, accuracy = 72.86314866398432, loss = 1.0542622488410207\n",
      "Iteration = 1050, accuracy = 72.92375941401012, loss = 1.04669468447042\n",
      "Iteration = 1060, accuracy = 72.99597647787063, loss = 1.0392356591381222\n",
      "Iteration = 1070, accuracy = 73.05400804704426, loss = 1.0318876517457338\n",
      "Iteration = 1080, accuracy = 73.1378314247395, loss = 1.0246488937062113\n",
      "Iteration = 1090, accuracy = 73.17522954709584, loss = 1.0175139942387768\n",
      "Iteration = 1100, accuracy = 73.23455070669556, loss = 1.0104845533491393\n",
      "Iteration = 1110, accuracy = 73.30031981842566, loss = 1.0035545589241879\n",
      "Iteration = 1120, accuracy = 73.35706179717322, loss = 0.9967261371143742\n",
      "Iteration = 1130, accuracy = 73.42412049932942, loss = 0.9899931124838713\n",
      "Iteration = 1140, accuracy = 73.46151862168576, loss = 0.9833589858437559\n",
      "Iteration = 1150, accuracy = 73.51568141958114, loss = 0.9768139481089166\n",
      "Iteration = 1160, accuracy = 73.56210667492005, loss = 0.9703587601891926\n",
      "Iteration = 1170, accuracy = 73.61626947281543, loss = 0.9639974974001304\n",
      "Iteration = 1180, accuracy = 73.69751366965852, loss = 0.9577198327241175\n",
      "Iteration = 1190, accuracy = 73.7452285154235, loss = 0.9515248650669099\n",
      "Iteration = 1200, accuracy = 73.80326008459713, loss = 0.9454141421428781\n",
      "Iteration = 1210, accuracy = 73.88579387186628, loss = 0.9393917931749737\n",
      "Iteration = 1220, accuracy = 73.92190240379655, loss = 0.933451333109755\n",
      "Iteration = 1230, accuracy = 73.99540905808315, loss = 0.9275959126204371\n",
      "Iteration = 1240, accuracy = 74.05473021768286, loss = 0.9218224334867563\n",
      "Iteration = 1250, accuracy = 74.10502424430001, loss = 0.9161308080741689\n",
      "Iteration = 1260, accuracy = 74.14113277623026, loss = 0.910518115440629\n",
      "Iteration = 1270, accuracy = 74.18626844114308, loss = 0.9049802697673212\n",
      "Iteration = 1280, accuracy = 74.24301041989064, loss = 0.8995096047299334\n",
      "Iteration = 1290, accuracy = 74.27396059011659, loss = 0.8941050753389614\n",
      "Iteration = 1300, accuracy = 74.31264830289899, loss = 0.8887702415079151\n",
      "Iteration = 1310, accuracy = 74.37067987207263, loss = 0.8835006607647816\n",
      "Iteration = 1320, accuracy = 74.41710512741153, loss = 0.8782952220786718\n",
      "Iteration = 1330, accuracy = 74.43000103167235, loss = 0.8731551649886077\n",
      "Iteration = 1340, accuracy = 74.46095120189827, loss = 0.8680786291892734\n",
      "Iteration = 1350, accuracy = 74.50350768595894, loss = 0.8630618619786403\n",
      "Iteration = 1360, accuracy = 74.54864335087177, loss = 0.8581102075745599\n",
      "Iteration = 1370, accuracy = 74.61828123388013, loss = 0.8532232035741344\n",
      "Iteration = 1380, accuracy = 74.66728567007118, loss = 0.8484009965980062\n",
      "Iteration = 1390, accuracy = 74.68405034561023, loss = 0.8436424525909514\n",
      "Iteration = 1400, accuracy = 74.71500051583617, loss = 0.8389469012218365\n",
      "Iteration = 1410, accuracy = 74.73692355307954, loss = 0.8343153292578005\n",
      "Iteration = 1420, accuracy = 74.7859279892706, loss = 0.8297466970625813\n",
      "Iteration = 1430, accuracy = 74.82074693077479, loss = 0.8252372564048043\n",
      "Iteration = 1440, accuracy = 74.86717218611369, loss = 0.8207876915733634\n",
      "Iteration = 1450, accuracy = 74.89683276591354, loss = 0.8163945847005865\n",
      "Iteration = 1460, accuracy = 74.93938924997421, loss = 0.8120577481221005\n",
      "Iteration = 1470, accuracy = 74.98323532446095, loss = 0.8077763505175253\n",
      "Iteration = 1480, accuracy = 75.0128959042608, loss = 0.8035500106028662\n",
      "Iteration = 1490, accuracy = 75.0670587021562, loss = 0.7993786695719777\n",
      "Iteration = 1500, accuracy = 75.11090477664294, loss = 0.7952614660598031\n",
      "Iteration = 1510, accuracy = 75.13798617559063, loss = 0.7911953676861452\n",
      "Iteration = 1520, accuracy = 75.1740947075209, loss = 0.7871810718207588\n",
      "Iteration = 1530, accuracy = 75.22309914371196, loss = 0.7832195797261424\n",
      "Iteration = 1540, accuracy = 75.2604972660683, loss = 0.7793096408079309\n",
      "Iteration = 1550, accuracy = 75.28370989373775, loss = 0.7754492635731092\n",
      "Iteration = 1560, accuracy = 75.31208088311152, loss = 0.7716374406530166\n",
      "Iteration = 1570, accuracy = 75.3340039203549, loss = 0.767874779412005\n",
      "Iteration = 1580, accuracy = 75.37784999484164, loss = 0.7641601953320357\n",
      "Iteration = 1590, accuracy = 75.40751057464149, loss = 0.7604932082590831\n",
      "Iteration = 1600, accuracy = 75.4668317342412, loss = 0.756872956541577\n",
      "Iteration = 1610, accuracy = 75.51325698958011, loss = 0.7532989333697677\n",
      "Iteration = 1620, accuracy = 75.54549675023212, loss = 0.749768647861339\n",
      "Iteration = 1630, accuracy = 75.57128855875374, loss = 0.746282112430187\n",
      "Iteration = 1640, accuracy = 75.59579077684927, loss = 0.7428384833025624\n",
      "Iteration = 1650, accuracy = 75.6409264417621, loss = 0.7394385132078793\n",
      "Iteration = 1660, accuracy = 75.67445579284019, loss = 0.7360801696752977\n",
      "Iteration = 1670, accuracy = 75.71056432477046, loss = 0.7327593911911804\n",
      "Iteration = 1680, accuracy = 75.7337769524399, loss = 0.729480100176447\n",
      "Iteration = 1690, accuracy = 75.75312080883111, loss = 0.7262437047013148\n",
      "Iteration = 1700, accuracy = 75.79954606417002, loss = 0.7230496602785369\n",
      "Iteration = 1710, accuracy = 75.83178582482203, loss = 0.7198955092929877\n",
      "Iteration = 1720, accuracy = 75.86273599504797, loss = 0.7167765594603095\n",
      "Iteration = 1730, accuracy = 75.89626534612607, loss = 0.713696102996763\n",
      "Iteration = 1740, accuracy = 75.94398019189106, loss = 0.7106554385631795\n",
      "Iteration = 1750, accuracy = 75.96332404828226, loss = 0.7076490816709187\n",
      "Iteration = 1760, accuracy = 76.00201176106468, loss = 0.7046805037759284\n",
      "Iteration = 1770, accuracy = 76.02393479830806, loss = 0.7017451231907375\n",
      "Iteration = 1780, accuracy = 76.04198906427318, loss = 0.6988444697577457\n",
      "Iteration = 1790, accuracy = 76.08583513875993, loss = 0.695981459053185\n",
      "Iteration = 1800, accuracy = 76.11936448983802, loss = 0.6931528395545957\n",
      "Iteration = 1810, accuracy = 76.15289384091612, loss = 0.6903597866044358\n",
      "Iteration = 1820, accuracy = 76.1735272877334, loss = 0.6876017533632128\n",
      "Iteration = 1830, accuracy = 76.18255442071597, loss = 0.6848773992663368\n",
      "Iteration = 1840, accuracy = 76.20318786753327, loss = 0.6821859569370026\n",
      "Iteration = 1850, accuracy = 76.21737336222016, loss = 0.6795299203632791\n",
      "Iteration = 1860, accuracy = 76.23929639946353, loss = 0.6769088568823728\n",
      "Iteration = 1870, accuracy = 76.2676673888373, loss = 0.6743223366447533\n",
      "Iteration = 1880, accuracy = 76.28959042608068, loss = 0.6717696063742866\n",
      "Iteration = 1890, accuracy = 76.33601568141958, loss = 0.669250569747904\n",
      "Iteration = 1900, accuracy = 76.3553595378108, loss = 0.6667641117560855\n",
      "Iteration = 1910, accuracy = 76.39920561229754, loss = 0.664309639881274\n",
      "Iteration = 1920, accuracy = 76.44434127721036, loss = 0.6618833863386474\n",
      "Iteration = 1930, accuracy = 76.45981636232332, loss = 0.6594876391512919\n",
      "Iteration = 1940, accuracy = 76.49463530382751, loss = 0.6571235655080058\n",
      "Iteration = 1950, accuracy = 76.53074383575776, loss = 0.6547907595723705\n",
      "Iteration = 1960, accuracy = 76.56685236768801, loss = 0.6524870846394493\n",
      "Iteration = 1970, accuracy = 76.5887754049314, loss = 0.6502117213562792\n",
      "Iteration = 1980, accuracy = 76.62359434643558, loss = 0.6479562256492717\n",
      "Iteration = 1990, accuracy = 76.64551738367894, loss = 0.6457283113698443\n",
      "Iteration = 2000, accuracy = 76.66486124007015, loss = 0.6435297880634531\n",
      "Iteration = 2010, accuracy = 76.67646755390489, loss = 0.641360267984394\n",
      "Iteration = 2020, accuracy = 76.70483854327865, loss = 0.6392193667074936\n",
      "Iteration = 2030, accuracy = 76.71386567626122, loss = 0.6371067032306433\n",
      "Iteration = 2040, accuracy = 76.73063035180027, loss = 0.6350219000436065\n",
      "Iteration = 2050, accuracy = 76.75255338904364, loss = 0.6329645831652531\n",
      "Iteration = 2060, accuracy = 76.78608274012174, loss = 0.6309329555938358\n",
      "Iteration = 2070, accuracy = 76.80929536779118, loss = 0.6289272658848161\n",
      "Iteration = 2080, accuracy = 76.85958939440833, loss = 0.6269477911315209\n",
      "Iteration = 2090, accuracy = 76.8879603837821, loss = 0.6249943564371216\n",
      "Iteration = 2100, accuracy = 76.89827710719076, loss = 0.6230666050916372\n",
      "Iteration = 2110, accuracy = 76.91504178272982, loss = 0.6211641836273928\n",
      "Iteration = 2120, accuracy = 76.92922727741669, loss = 0.6192865009861789\n",
      "Iteration = 2130, accuracy = 76.95372949551222, loss = 0.6174334391760116\n",
      "Iteration = 2140, accuracy = 76.96920458062519, loss = 0.6156046632750255\n",
      "Iteration = 2150, accuracy = 77.00273393170329, loss = 0.6137998324030404\n",
      "Iteration = 2160, accuracy = 77.01047147425977, loss = 0.6120186087790087\n",
      "Iteration = 2170, accuracy = 77.04013205405963, loss = 0.6102606577280526\n",
      "Iteration = 2180, accuracy = 77.04915918704219, loss = 0.608525330418501\n",
      "Iteration = 2190, accuracy = 77.06334468172909, loss = 0.6068114877231273\n",
      "Iteration = 2200, accuracy = 77.07366140513773, loss = 0.6051194728430037\n",
      "Iteration = 2210, accuracy = 77.0891364902507, loss = 0.6034480617373305\n",
      "Iteration = 2220, accuracy = 77.11234911792016, loss = 0.6017982378190084\n",
      "Iteration = 2230, accuracy = 77.12137625090271, loss = 0.6001687350997955\n",
      "Iteration = 2240, accuracy = 77.13169297431136, loss = 0.5985598528169914\n",
      "Iteration = 2250, accuracy = 77.13685133601568, loss = 0.5969711615858341\n",
      "Iteration = 2260, accuracy = 77.14974724027648, loss = 0.5954017206152604\n",
      "Iteration = 2270, accuracy = 77.15748478283298, loss = 0.5938517263686881\n",
      "Iteration = 2280, accuracy = 77.16780150624162, loss = 0.5923196684235615\n",
      "Iteration = 2290, accuracy = 77.18714536263282, loss = 0.5908031308713827\n",
      "Iteration = 2300, accuracy = 77.20777880945012, loss = 0.5893059305563393\n",
      "Iteration = 2310, accuracy = 77.23099143711957, loss = 0.5878277817069723\n",
      "Iteration = 2320, accuracy = 77.2554936552151, loss = 0.5863684029856924\n",
      "Iteration = 2330, accuracy = 77.29031259671928, loss = 0.5849275092627056\n",
      "Iteration = 2340, accuracy = 77.31352522438874, loss = 0.58350430654796\n",
      "Iteration = 2350, accuracy = 77.33157949035386, loss = 0.5820990588023992\n",
      "Iteration = 2360, accuracy = 77.34705457546683, loss = 0.5807115039122162\n",
      "Iteration = 2370, accuracy = 77.38832146910141, loss = 0.5793413847181141\n",
      "Iteration = 2380, accuracy = 77.41024450634478, loss = 0.5779884489678737\n",
      "Iteration = 2390, accuracy = 77.43345713401423, loss = 0.5766521085235887\n",
      "Iteration = 2400, accuracy = 77.46053853296193, loss = 0.5753318691189022\n",
      "Iteration = 2410, accuracy = 77.46440730424017, loss = 0.5740280911327341\n",
      "Iteration = 2420, accuracy = 77.46698648509233, loss = 0.5727405407387522\n",
      "Iteration = 2430, accuracy = 77.4953574744661, loss = 0.5714689883065052\n",
      "Iteration = 2440, accuracy = 77.51341174043124, loss = 0.5702132082661007\n",
      "Iteration = 2450, accuracy = 77.53146600639637, loss = 0.5689729789789019\n",
      "Iteration = 2460, accuracy = 77.53017641597029, loss = 0.5677480825884802\n",
      "Iteration = 2470, accuracy = 77.54436191065717, loss = 0.5665383048066985\n",
      "Iteration = 2480, accuracy = 77.56757453832662, loss = 0.5653434345812827\n",
      "Iteration = 2490, accuracy = 77.56499535747446, loss = 0.5641632636018385\n",
      "Iteration = 2500, accuracy = 77.56757453832662, loss = 0.5629975856395587\n",
      "Iteration = 2510, accuracy = 77.57918085216136, loss = 0.5618461957643404\n",
      "Iteration = 2520, accuracy = 77.58820798514392, loss = 0.5607084347021476\n",
      "Iteration = 2530, accuracy = 77.59207675642216, loss = 0.559583911780779\n",
      "Iteration = 2540, accuracy = 77.61013102238728, loss = 0.5584702336145047\n",
      "Iteration = 2550, accuracy = 77.61528938409161, loss = 0.5573699374741422\n",
      "Iteration = 2560, accuracy = 77.637212421335, loss = 0.5562829715848565\n",
      "Iteration = 2570, accuracy = 77.65268750644795, loss = 0.5552091309633825\n",
      "Iteration = 2580, accuracy = 77.65526668730011, loss = 0.5541482097917623\n",
      "Iteration = 2590, accuracy = 77.67074177241308, loss = 0.5531000044091486\n",
      "Iteration = 2600, accuracy = 77.69524399050862, loss = 0.5520643142298065\n",
      "Iteration = 2610, accuracy = 77.70685030434335, loss = 0.5510409424577003\n",
      "Iteration = 2620, accuracy = 77.71329825647373, loss = 0.5500290961112259\n",
      "Iteration = 2630, accuracy = 77.7223253894563, loss = 0.5490277172981342\n",
      "Iteration = 2640, accuracy = 77.72103579903022, loss = 0.5480366789239617\n",
      "Iteration = 2650, accuracy = 77.73006293201279, loss = 0.5470572620307385\n",
      "Iteration = 2660, accuracy = 77.74166924584752, loss = 0.5460892903897793\n",
      "Iteration = 2670, accuracy = 77.74037965542145, loss = 0.5451325919024367\n",
      "Iteration = 2680, accuracy = 77.75198596925617, loss = 0.5441869982845181\n",
      "Iteration = 2690, accuracy = 77.76746105436914, loss = 0.5432523447480364\n",
      "Iteration = 2700, accuracy = 77.79970081502114, loss = 0.5423284697007994\n",
      "Iteration = 2710, accuracy = 77.81130712885587, loss = 0.5414152144809123\n",
      "Iteration = 2720, accuracy = 77.82549262354276, loss = 0.5405124231354342\n",
      "Iteration = 2730, accuracy = 77.84096770865573, loss = 0.5396195533872027\n",
      "Iteration = 2740, accuracy = 77.84999484163829, loss = 0.5387357357890101\n",
      "Iteration = 2750, accuracy = 77.86933869802951, loss = 0.5378619387157938\n",
      "Iteration = 2760, accuracy = 77.87578665015991, loss = 0.5369980157554228\n",
      "Iteration = 2770, accuracy = 77.88610337356855, loss = 0.536143822744606\n",
      "Iteration = 2780, accuracy = 77.8835241927164, loss = 0.5352992177676261\n",
      "Iteration = 2790, accuracy = 77.91447436294233, loss = 0.5344640611572921\n",
      "Iteration = 2800, accuracy = 77.92221190549881, loss = 0.5336382155084461\n",
      "Iteration = 2810, accuracy = 77.93381821933355, loss = 0.532821545681523\n",
      "Iteration = 2820, accuracy = 77.94155576189003, loss = 0.532013918813067\n",
      "Iteration = 2830, accuracy = 77.94413494274218, loss = 0.5312152043078315\n",
      "Iteration = 2840, accuracy = 77.96347879913338, loss = 0.5304252738344905\n",
      "Iteration = 2850, accuracy = 77.97766429382028, loss = 0.5296440013054408\n",
      "Iteration = 2860, accuracy = 77.99829774063758, loss = 0.5288712628559239\n",
      "Iteration = 2870, accuracy = 78.00732487362015, loss = 0.528106386053108\n",
      "Iteration = 2880, accuracy = 78.02022077788095, loss = 0.5273491902026874\n",
      "Iteration = 2890, accuracy = 78.02666873001135, loss = 0.526600111677474\n",
      "Iteration = 2900, accuracy = 78.03440627256784, loss = 0.5258581396843363\n",
      "Iteration = 2910, accuracy = 78.04730217682864, loss = 0.5251238172787642\n",
      "Iteration = 2920, accuracy = 78.05761890023729, loss = 0.5243972764313818\n",
      "Iteration = 2930, accuracy = 78.0769627566285, loss = 0.5236784731680499\n",
      "Iteration = 2940, accuracy = 78.10404415557619, loss = 0.5229671501735338\n",
      "Iteration = 2950, accuracy = 78.11565046941091, loss = 0.5222627375330414\n",
      "Iteration = 2960, accuracy = 78.12854637367172, loss = 0.5215657470324602\n",
      "Iteration = 2970, accuracy = 78.14402145878468, loss = 0.5208760766097953\n",
      "Iteration = 2980, accuracy = 78.16594449602806, loss = 0.5201936259820397\n",
      "Iteration = 2990, accuracy = 78.17626121943671, loss = 0.5195182966096616\n",
      "Iteration = 3000, accuracy = 78.17884040028888, loss = 0.518849734332334\n",
      "Iteration = 3010, accuracy = 78.1839987619932, loss = 0.518187727664858\n",
      "Iteration = 3020, accuracy = 78.19818425668008, loss = 0.517532559962871\n",
      "Iteration = 3030, accuracy = 78.2033426183844, loss = 0.5168841392443241\n",
      "Iteration = 3040, accuracy = 78.2046322088105, loss = 0.5162423751153584\n",
      "Iteration = 3050, accuracy = 78.20592179923656, loss = 0.5156071787402724\n",
      "Iteration = 3060, accuracy = 78.21881770349736, loss = 0.5149784628118612\n",
      "Iteration = 3070, accuracy = 78.2368719694625, loss = 0.5143561415219975\n",
      "Iteration = 3080, accuracy = 78.24718869287113, loss = 0.5137401305327224\n",
      "Iteration = 3090, accuracy = 78.24589910244507, loss = 0.5131303469471067\n",
      "Iteration = 3100, accuracy = 78.2575054162798, loss = 0.5125267092809255\n",
      "Iteration = 3110, accuracy = 78.26782213968843, loss = 0.511929137433884\n",
      "Iteration = 3120, accuracy = 78.27942845352315, loss = 0.5113375526615381\n",
      "Iteration = 3130, accuracy = 78.2897451769318, loss = 0.5107518775473384\n",
      "Iteration = 3140, accuracy = 78.31295780460125, loss = 0.5101720359746221\n",
      "Iteration = 3150, accuracy = 78.32456411843597, loss = 0.5095979530992641\n",
      "Iteration = 3160, accuracy = 78.33746002269679, loss = 0.5090295553225447\n",
      "Iteration = 3170, accuracy = 78.34777674610544, loss = 0.5084667702642491\n",
      "Iteration = 3180, accuracy = 78.35293510780976, loss = 0.507909526736337\n",
      "Iteration = 3190, accuracy = 78.34777674610544, loss = 0.5073577547170348\n",
      "Iteration = 3200, accuracy = 78.35938305994016, loss = 0.5068113853252757\n",
      "Iteration = 3210, accuracy = 78.37356855462704, loss = 0.5062703507959586\n",
      "Iteration = 3220, accuracy = 78.38001650675746, loss = 0.505734584455453\n",
      "Iteration = 3230, accuracy = 78.39420200144434, loss = 0.5052040206979515\n",
      "Iteration = 3240, accuracy = 78.39291241101826, loss = 0.5046778885389426\n",
      "Iteration = 3250, accuracy = 78.4174146291138, loss = 0.5041566763355613\n",
      "Iteration = 3260, accuracy = 78.42773135252243, loss = 0.5036397373529327\n",
      "Iteration = 3270, accuracy = 78.44191684720933, loss = 0.5031277612002254\n",
      "Iteration = 3280, accuracy = 78.45610234189621, loss = 0.5026206871615375\n",
      "Iteration = 3290, accuracy = 78.47157742700918, loss = 0.5021184554631656\n",
      "Iteration = 3300, accuracy = 78.4767357887135, loss = 0.5016209885691111\n",
      "Iteration = 3310, accuracy = 78.49994841638296, loss = 0.5011274358551363\n",
      "Iteration = 3320, accuracy = 78.50897554936552, loss = 0.500637746701949\n",
      "Iteration = 3330, accuracy = 78.5244506344785, loss = 0.5001526823287485\n",
      "Iteration = 3340, accuracy = 78.53218817703498, loss = 0.4996721873104915\n",
      "Iteration = 3350, accuracy = 78.5437944908697, loss = 0.49919593953344815\n",
      "Iteration = 3360, accuracy = 78.54121531001755, loss = 0.49872360933329263\n",
      "Iteration = 3370, accuracy = 78.55540080470442, loss = 0.49825568926849206\n",
      "Iteration = 3380, accuracy = 78.5631383472609, loss = 0.4977921271867739\n",
      "Iteration = 3390, accuracy = 78.56571752811307, loss = 0.49733287171102014\n",
      "Iteration = 3400, accuracy = 78.58506138450429, loss = 0.49687787222656626\n",
      "Iteration = 3410, accuracy = 78.58893015578252, loss = 0.49642707886804516\n",
      "Iteration = 3420, accuracy = 78.60311565046942, loss = 0.4959804425044422\n",
      "Iteration = 3430, accuracy = 78.60182606004334, loss = 0.4955379147290063\n",
      "Iteration = 3440, accuracy = 78.60569483132157, loss = 0.49509944784625004\n",
      "Iteration = 3450, accuracy = 78.60182606004334, loss = 0.49466455359842854\n",
      "Iteration = 3460, accuracy = 78.60827401217372, loss = 0.49423350713212977\n",
      "Iteration = 3470, accuracy = 78.61730114515629, loss = 0.49380638541755134\n",
      "Iteration = 3480, accuracy = 78.62632827813887, loss = 0.49338252978998787\n",
      "Iteration = 3490, accuracy = 78.63793459197359, loss = 0.4929624395160236\n",
      "Iteration = 3500, accuracy = 78.64309295367791, loss = 0.4925461446404577\n",
      "Iteration = 3510, accuracy = 78.65083049623439, loss = 0.4921336021000229\n",
      "Iteration = 3520, accuracy = 78.66372640049521, loss = 0.49172476945339116\n",
      "Iteration = 3530, accuracy = 78.67920148560817, loss = 0.4913196048716661\n",
      "Iteration = 3540, accuracy = 78.67920148560817, loss = 0.49091806712692443\n",
      "Iteration = 3550, accuracy = 78.68564943773858, loss = 0.49051973985080144\n",
      "Iteration = 3560, accuracy = 78.70112452285154, loss = 0.49012487112597447\n",
      "Iteration = 3570, accuracy = 78.70370370370371, loss = 0.48973351050396735\n",
      "Iteration = 3580, accuracy = 78.71659960796451, loss = 0.48934561906347473\n",
      "Iteration = 3590, accuracy = 78.72562674094708, loss = 0.4889611584352829\n",
      "Iteration = 3600, accuracy = 78.7372330547818, loss = 0.4885800907945667\n",
      "Iteration = 3610, accuracy = 78.74239141648613, loss = 0.4882023788506931\n",
      "Iteration = 3620, accuracy = 78.76173527287733, loss = 0.4878279858378089\n",
      "Iteration = 3630, accuracy = 78.76947281543382, loss = 0.48745687550697814\n",
      "Iteration = 3640, accuracy = 78.78236871969463, loss = 0.4870890121160573\n",
      "Iteration = 3650, accuracy = 78.78752708139895, loss = 0.48672436042160816\n",
      "Iteration = 3660, accuracy = 78.77978953884246, loss = 0.4863621980870783\n",
      "Iteration = 3670, accuracy = 78.78752708139895, loss = 0.4860031785320937\n",
      "Iteration = 3680, accuracy = 78.77849994841638, loss = 0.4856472728485762\n",
      "Iteration = 3690, accuracy = 78.78881667182502, loss = 0.48529444766468055\n",
      "Iteration = 3700, accuracy = 78.79913339523368, loss = 0.48494467006556546\n",
      "Iteration = 3710, accuracy = 78.80945011864232, loss = 0.48459790758465676\n",
      "Iteration = 3720, accuracy = 78.81331888992055, loss = 0.484254128196147\n",
      "Iteration = 3730, accuracy = 78.81460848034664, loss = 0.4839133003070622\n",
      "Iteration = 3740, accuracy = 78.82621479418137, loss = 0.4835753927494026\n",
      "Iteration = 3750, accuracy = 78.84555865057258, loss = 0.48324021173216286\n",
      "Iteration = 3760, accuracy = 78.85587537398122, loss = 0.48290760164330393\n",
      "Iteration = 3770, accuracy = 78.85974414525947, loss = 0.48257782296965374\n",
      "Iteration = 3780, accuracy = 78.87521923037242, loss = 0.4822508461610311\n",
      "Iteration = 3790, accuracy = 78.87908800165067, loss = 0.4819266420560031\n",
      "Iteration = 3800, accuracy = 78.8906943154854, loss = 0.48160518187477486\n",
      "Iteration = 3810, accuracy = 78.89456308676365, loss = 0.48128629918323046\n",
      "Iteration = 3820, accuracy = 78.90230062932014, loss = 0.48096967083287967\n",
      "Iteration = 3830, accuracy = 78.90616940059837, loss = 0.4806557052907104\n",
      "Iteration = 3840, accuracy = 78.93196120911998, loss = 0.48034437521943235\n",
      "Iteration = 3850, accuracy = 78.94356752295471, loss = 0.4800356536311015\n",
      "Iteration = 3860, accuracy = 78.95259465593728, loss = 0.47972951388112356\n",
      "Iteration = 3870, accuracy = 78.94872588465903, loss = 0.47942592966216874\n",
      "Iteration = 3880, accuracy = 78.95388424636336, loss = 0.47912487499850087\n",
      "Iteration = 3890, accuracy = 78.96549056019808, loss = 0.4788263242400252\n",
      "Iteration = 3900, accuracy = 78.96420096977201, loss = 0.47853025205696864\n",
      "Iteration = 3910, accuracy = 78.97451769318064, loss = 0.4782366334341943\n",
      "Iteration = 3920, accuracy = 78.96806974105024, loss = 0.47794544366603253\n",
      "Iteration = 3930, accuracy = 78.97193851232849, loss = 0.4776566583510028\n",
      "Iteration = 3940, accuracy = 78.97580728360673, loss = 0.4773702533867725\n",
      "Iteration = 3950, accuracy = 78.97580728360673, loss = 0.4770862049651817\n",
      "Iteration = 3960, accuracy = 78.97967605488498, loss = 0.4768044895673694\n",
      "Iteration = 3970, accuracy = 78.98741359744145, loss = 0.4765250839591141\n",
      "Iteration = 3980, accuracy = 78.9912823687197, loss = 0.47624796518620016\n",
      "Iteration = 3990, accuracy = 78.99386154957185, loss = 0.4759731105699354\n",
      "Iteration = 4000, accuracy = 78.9977303208501, loss = 0.4757004977027125\n",
      "Iteration = 4010, accuracy = 79.00933663468483, loss = 0.4754301044437876\n",
      "Iteration = 4020, accuracy = 79.01578458681523, loss = 0.47516190891509896\n",
      "Iteration = 4030, accuracy = 79.01707417724131, loss = 0.47489588949717165\n",
      "Iteration = 4040, accuracy = 79.0248117197978, loss = 0.47463202482514627\n",
      "Iteration = 4050, accuracy = 79.0248117197978, loss = 0.47437029378492523\n",
      "Iteration = 4060, accuracy = 79.04028680491076, loss = 0.4741106755093618\n",
      "Iteration = 4070, accuracy = 79.04931393789333, loss = 0.4738531493745778\n",
      "Iteration = 4080, accuracy = 79.04931393789333, loss = 0.47359752660325855\n",
      "Iteration = 4090, accuracy = 79.05963066130197, loss = 0.4733438017758799\n",
      "Iteration = 4100, accuracy = 79.06736820385845, loss = 0.4730921097187656\n",
      "Iteration = 4110, accuracy = 79.0776849272671, loss = 0.4728424307429572\n",
      "Iteration = 4120, accuracy = 79.08155369854533, loss = 0.4725947453855582\n",
      "Iteration = 4130, accuracy = 79.07639533684102, loss = 0.4723490344064778\n",
      "Iteration = 4140, accuracy = 79.08542246982358, loss = 0.47210498657016253\n",
      "Iteration = 4150, accuracy = 79.0970287836583, loss = 0.47186275052175897\n",
      "Iteration = 4160, accuracy = 79.10218714536263, loss = 0.471622434405259\n",
      "Iteration = 4170, accuracy = 79.1112142783452, loss = 0.471384019835967\n",
      "Iteration = 4180, accuracy = 79.11379345919735, loss = 0.4711471749763907\n",
      "Iteration = 4190, accuracy = 79.12282059217992, loss = 0.4709121134606451\n",
      "Iteration = 4200, accuracy = 79.1305581347364, loss = 0.47067890157647096\n",
      "Iteration = 4210, accuracy = 79.13958526771897, loss = 0.47044752174384274\n",
      "Iteration = 4220, accuracy = 79.14861240070154, loss = 0.47021795657943183\n",
      "Iteration = 4230, accuracy = 79.1576395336841, loss = 0.46999018889404315\n",
      "Iteration = 4240, accuracy = 79.16279789538842, loss = 0.4697639310555229\n",
      "Iteration = 4250, accuracy = 79.16924584751884, loss = 0.4695393861905689\n",
      "Iteration = 4260, accuracy = 79.17440420922316, loss = 0.4693165897652242\n",
      "Iteration = 4270, accuracy = 79.17956257092747, loss = 0.4690955253358278\n",
      "Iteration = 4280, accuracy = 79.19116888476219, loss = 0.4688761766404741\n",
      "Iteration = 4290, accuracy = 79.19245847518827, loss = 0.4686585275966811\n",
      "Iteration = 4300, accuracy = 79.19632724646652, loss = 0.46844256229907677\n",
      "Iteration = 4310, accuracy = 79.20148560817084, loss = 0.46822826501713505\n",
      "Iteration = 4320, accuracy = 79.20019601774476, loss = 0.4680156201929517\n",
      "Iteration = 4330, accuracy = 79.20922315072733, loss = 0.4678046124390443\n",
      "Iteration = 4340, accuracy = 79.21696069328381, loss = 0.467595226536195\n",
      "Iteration = 4350, accuracy = 79.2246982358403, loss = 0.46738744743132743\n",
      "Iteration = 4360, accuracy = 79.21567110285773, loss = 0.4671812602354026\n",
      "Iteration = 4370, accuracy = 79.22211905498814, loss = 0.4669766502213606\n",
      "Iteration = 4380, accuracy = 79.22727741669246, loss = 0.46677360282208025\n",
      "Iteration = 4390, accuracy = 79.22727741669246, loss = 0.4665721036283683\n",
      "Iteration = 4400, accuracy = 79.23501495924894, loss = 0.46637213838700214\n",
      "Iteration = 4410, accuracy = 79.24017332095326, loss = 0.4661736929987534\n",
      "Iteration = 4420, accuracy = 79.23759414010111, loss = 0.4659767535164885\n",
      "Iteration = 4430, accuracy = 79.24920045393583, loss = 0.46578130614324714\n",
      "Iteration = 4440, accuracy = 79.26338594862273, loss = 0.4655873372303881\n",
      "Iteration = 4450, accuracy = 79.25951717734448, loss = 0.46539483327574155\n",
      "Iteration = 4460, accuracy = 79.2646755390488, loss = 0.4652037809217806\n",
      "Iteration = 4470, accuracy = 79.26338594862273, loss = 0.46501416695382863\n",
      "Iteration = 4480, accuracy = 79.2646755390488, loss = 0.4648259782982757\n",
      "Iteration = 4490, accuracy = 79.26596512947488, loss = 0.4646392020208415\n",
      "Iteration = 4500, accuracy = 79.26983390075311, loss = 0.464453825324836\n",
      "Iteration = 4510, accuracy = 79.2775714433096, loss = 0.46426983554945883\n",
      "Iteration = 4520, accuracy = 79.27886103373568, loss = 0.46408722016811504\n",
      "Iteration = 4530, accuracy = 79.28272980501393, loss = 0.4639059667867586\n",
      "Iteration = 4540, accuracy = 79.28659857629216, loss = 0.46372606314223874\n",
      "Iteration = 4550, accuracy = 79.29304652842258, loss = 0.46354749710069515\n",
      "Iteration = 4560, accuracy = 79.29562570927473, loss = 0.4633702566559616\n",
      "Iteration = 4570, accuracy = 79.2982048901269, loss = 0.4631943299279742\n",
      "Iteration = 4580, accuracy = 79.29562570927473, loss = 0.4630197051612327\n",
      "Iteration = 4590, accuracy = 79.30336325183121, loss = 0.462846370723254\n",
      "Iteration = 4600, accuracy = 79.30981120396162, loss = 0.4626741948370665\n",
      "Iteration = 4610, accuracy = 79.31754874651811, loss = 0.46250307558421794\n",
      "Iteration = 4620, accuracy = 79.32657587950067, loss = 0.46233321402457367\n",
      "Iteration = 4630, accuracy = 79.32915506035283, loss = 0.4621645989959793\n",
      "Iteration = 4640, accuracy = 79.32657587950067, loss = 0.4619972194519389\n",
      "Iteration = 4650, accuracy = 79.32915506035283, loss = 0.4618307957773929\n",
      "Iteration = 4660, accuracy = 79.33947178376147, loss = 0.4616655365704587\n",
      "Iteration = 4670, accuracy = 79.3446301454658, loss = 0.4615014817343529\n",
      "Iteration = 4680, accuracy = 79.34205096461365, loss = 0.46133862066713777\n",
      "Iteration = 4690, accuracy = 79.34720932631795, loss = 0.46117694287561173\n",
      "Iteration = 4700, accuracy = 79.34720932631795, loss = 0.46101643797398867\n",
      "Iteration = 4710, accuracy = 79.35365727844837, loss = 0.460857095682601\n",
      "Iteration = 4720, accuracy = 79.35752604972662, loss = 0.4606989058266158\n",
      "Iteration = 4730, accuracy = 79.36784277313525, loss = 0.46054185833476946\n",
      "Iteration = 4740, accuracy = 79.37558031569174, loss = 0.4603859432381233\n",
      "Iteration = 4750, accuracy = 79.37944908696998, loss = 0.46023115066882925\n",
      "Iteration = 4760, accuracy = 79.38718662952647, loss = 0.4600774708589239\n",
      "Iteration = 4770, accuracy = 79.39492417208295, loss = 0.4599248941391341\n",
      "Iteration = 4780, accuracy = 79.40266171463944, loss = 0.45977341093769464\n",
      "Iteration = 4790, accuracy = 79.40782007634375, loss = 0.4596230117791951\n",
      "Iteration = 4800, accuracy = 79.41297843804807, loss = 0.4594736872834337\n",
      "Iteration = 4810, accuracy = 79.42200557103064, loss = 0.4593254281642903\n",
      "Iteration = 4820, accuracy = 79.42716393273497, loss = 0.45917822522861257\n",
      "Iteration = 4830, accuracy = 79.44134942742184, loss = 0.4590320693751284\n",
      "Iteration = 4840, accuracy = 79.44650778912617, loss = 0.45888695159336335\n",
      "Iteration = 4850, accuracy = 79.45295574125659, loss = 0.4587428629625765\n",
      "Iteration = 4860, accuracy = 79.45424533168266, loss = 0.45859953553637617\n",
      "Iteration = 4870, accuracy = 79.45811410296089, loss = 0.4584571894000394\n",
      "Iteration = 4880, accuracy = 79.46069328381307, loss = 0.45831584766795447\n",
      "Iteration = 4890, accuracy = 79.46972041679562, loss = 0.45817550176034183\n",
      "Iteration = 4900, accuracy = 79.46843082636954, loss = 0.45803614318209057\n",
      "Iteration = 4910, accuracy = 79.46972041679562, loss = 0.45789776352177963\n",
      "Iteration = 4920, accuracy = 79.48261632105643, loss = 0.4577603544507249\n",
      "Iteration = 4930, accuracy = 79.49035386361291, loss = 0.457623907722035\n",
      "Iteration = 4940, accuracy = 79.48906427318684, loss = 0.4574884151696736\n",
      "Iteration = 4950, accuracy = 79.49938099659548, loss = 0.45735386870754446\n",
      "Iteration = 4960, accuracy = 79.51098731043021, loss = 0.45722026032858215\n",
      "Iteration = 4970, accuracy = 79.50969772000413, loss = 0.4570875821038607\n",
      "Iteration = 4980, accuracy = 79.52130403383886, loss = 0.45695582618171315\n",
      "Iteration = 4990, accuracy = 79.52001444341276, loss = 0.45682498478686195\n",
      "Cross Polynomial expansion of degree 1 done : adding (175338, 253)\n",
      "Logarithmic expansion of degree 1 done : adding (175338, 23)\n"
=======
      "Iteration = 0, accuracy = 35.73841947797379, loss = 7.117980078706898\n",
      "Iteration = 10, accuracy = 35.77710719075621, loss = 6.381175450079116\n",
      "Iteration = 20, accuracy = 36.0053646961725, loss = 5.613945176883278\n",
      "Iteration = 30, accuracy = 36.668214175177965, loss = 4.834021723739728\n",
      "Iteration = 40, accuracy = 38.21701227690086, loss = 4.067727630503282\n",
      "Iteration = 50, accuracy = 40.4763747033942, loss = 3.353485689659376\n",
      "Iteration = 60, accuracy = 43.497885071701226, loss = 2.7298565810200697\n",
      "Iteration = 70, accuracy = 46.92561642422366, loss = 2.219677830034287\n",
      "Iteration = 80, accuracy = 50.17925306922522, loss = 1.8283380728348302\n",
      "Iteration = 90, accuracy = 53.363251831218406, loss = 1.5460220453499545\n",
      "Iteration = 100, accuracy = 55.79412978438049, loss = 1.3470902878271502\n",
      "Iteration = 110, accuracy = 57.66403590219746, loss = 1.2050737017061655\n",
      "Iteration = 120, accuracy = 59.243784174146285, loss = 1.1038578342219219\n",
      "Iteration = 130, accuracy = 60.48952852574022, loss = 1.032455481960418\n",
      "Iteration = 140, accuracy = 61.49282987723099, loss = 0.9821066623636382\n",
      "Iteration = 150, accuracy = 62.33751160631383, loss = 0.9454979516031984\n",
      "Iteration = 160, accuracy = 62.947487877849994, loss = 0.9174613572526007\n",
      "Iteration = 170, accuracy = 63.53554111214278, loss = 0.8946271669552018\n",
      "Iteration = 180, accuracy = 64.06298359640978, loss = 0.8749652266565582\n",
      "Iteration = 190, accuracy = 64.4756525327556, loss = 0.857376224565091\n",
      "Iteration = 200, accuracy = 64.83673785205819, loss = 0.841255081506296\n",
      "Iteration = 210, accuracy = 65.19911276178686, loss = 0.8262532032583684\n",
      "Iteration = 220, accuracy = 65.49442896935933, loss = 0.8121780078364961\n",
      "Iteration = 230, accuracy = 65.79490353863613, loss = 0.7989016970488224\n",
      "Iteration = 240, accuracy = 66.08248220365212, loss = 0.7863365849330959\n",
      "Iteration = 250, accuracy = 66.32750438460745, loss = 0.7744148086141494\n",
      "Iteration = 260, accuracy = 66.60218714536263, loss = 0.7630757788715739\n",
      "Iteration = 270, accuracy = 66.85881564015268, loss = 0.7522605985395976\n",
      "Iteration = 280, accuracy = 67.11544413494273, loss = 0.7419503984853519\n",
      "Iteration = 290, accuracy = 67.38625812441968, loss = 0.732088463433122\n",
      "Iteration = 300, accuracy = 67.61580522026205, loss = 0.7226454212414629\n",
      "Iteration = 310, accuracy = 67.80795419374806, loss = 0.713605511058275\n",
      "Iteration = 320, accuracy = 68.01041989064274, loss = 0.704975450387736\n",
      "Iteration = 330, accuracy = 68.25673166202414, loss = 0.6967320464432596\n",
      "Iteration = 340, accuracy = 68.43211595997111, loss = 0.6888532194820945\n",
      "Iteration = 350, accuracy = 68.60492107706592, loss = 0.6813175824834158\n",
      "Iteration = 360, accuracy = 68.80738677396059, loss = 0.6741043639981611\n",
      "Iteration = 370, accuracy = 69.01114206128133, loss = 0.6671936905771899\n",
      "Iteration = 380, accuracy = 69.1942639017848, loss = 0.6605647448183168\n",
      "Iteration = 390, accuracy = 69.42896935933148, loss = 0.6542017989827901\n",
      "Iteration = 400, accuracy = 69.61724956153925, loss = 0.6480915084529222\n",
      "Iteration = 410, accuracy = 69.80810894459918, loss = 0.6422206235789326\n",
      "Iteration = 420, accuracy = 69.99252037552873, loss = 0.6365766066240461\n",
      "Iteration = 430, accuracy = 70.16403590219747, loss = 0.6311469008206152\n",
      "Iteration = 440, accuracy = 70.3690807799443, loss = 0.6259191147271002\n",
      "Iteration = 450, accuracy = 70.588311152378, loss = 0.6208817825066909\n",
      "Iteration = 460, accuracy = 70.74306200350769, loss = 0.6160247856749055\n",
      "Iteration = 470, accuracy = 70.91328793975033, loss = 0.6113392724942872\n",
      "Iteration = 480, accuracy = 71.06545961002786, loss = 0.6068173890874206\n",
      "Iteration = 490, accuracy = 71.23052718456618, loss = 0.6024508123866792\n",
      "Iteration = 500, accuracy = 71.39043639740018, loss = 0.5982331208288709\n",
      "Iteration = 510, accuracy = 71.53100175384299, loss = 0.59415717239087\n",
      "Iteration = 520, accuracy = 71.67285670071185, loss = 0.5902183531634065\n",
      "Iteration = 530, accuracy = 71.81987000928504, loss = 0.5864097415856931\n",
      "Iteration = 540, accuracy = 71.94753946146703, loss = 0.5827287856466787\n",
      "Iteration = 550, accuracy = 72.09197358918807, loss = 0.5791706441769641\n",
      "Iteration = 560, accuracy = 72.21577427009181, loss = 0.575729716006215\n",
      "Iteration = 570, accuracy = 72.3434437222738, loss = 0.5724013606663764\n",
      "Iteration = 580, accuracy = 72.48400907871661, loss = 0.5691823544820712\n",
      "Iteration = 590, accuracy = 72.61554730217684, loss = 0.5660690238685543\n",
      "Iteration = 600, accuracy = 72.75611265861961, loss = 0.5630575003367568\n",
      "Iteration = 610, accuracy = 72.86314866398432, loss = 0.5601440768260705\n",
      "Iteration = 620, accuracy = 72.98694934488806, loss = 0.5573252003479903\n",
      "Iteration = 630, accuracy = 73.1107500257918, loss = 0.554597465410731\n",
      "Iteration = 640, accuracy = 73.22294439286082, loss = 0.5519576078368322\n",
      "Iteration = 650, accuracy = 73.35319302589498, loss = 0.549402498698579\n",
      "Iteration = 660, accuracy = 73.4640978025379, loss = 0.5469285421536701\n",
      "Iteration = 670, accuracy = 73.56210667492005, loss = 0.5445306310687106\n",
      "Iteration = 680, accuracy = 73.67688022284122, loss = 0.5422088273388073\n",
      "Iteration = 690, accuracy = 73.77617868564944, loss = 0.539960481934641\n",
      "Iteration = 700, accuracy = 73.88966264314453, loss = 0.5377830483181596\n",
      "Iteration = 710, accuracy = 74.01346332404827, loss = 0.5356740753723249\n",
      "Iteration = 720, accuracy = 74.1127617868565, loss = 0.5336312004892224\n",
      "Iteration = 730, accuracy = 74.21077065923862, loss = 0.531652143175891\n",
      "Iteration = 740, accuracy = 74.31651707417724, loss = 0.5297346995679941\n",
      "Iteration = 750, accuracy = 74.392602909316, loss = 0.527876738150345\n",
      "Iteration = 760, accuracy = 74.46739915402868, loss = 0.5260761967824784\n",
      "Iteration = 770, accuracy = 74.58991024450634, loss = 0.5243310808874246\n",
      "Iteration = 780, accuracy = 74.68791911688848, loss = 0.522639462472961\n",
      "Iteration = 790, accuracy = 74.77819044671412, loss = 0.5209994795759482\n",
      "Iteration = 800, accuracy = 74.89812235633963, loss = 0.5194093357548207\n",
      "Iteration = 810, accuracy = 74.97162901062623, loss = 0.5178628811773984\n",
      "Iteration = 820, accuracy = 75.06061075002579, loss = 0.5163621681428302\n",
      "Iteration = 830, accuracy = 75.14443412772104, loss = 0.514903607792853\n",
      "Iteration = 840, accuracy = 75.23986381925101, loss = 0.5134872822384204\n",
      "Iteration = 850, accuracy = 75.33529351078097, loss = 0.51211059969868\n",
      "Iteration = 860, accuracy = 75.40622098421541, loss = 0.5107743694769517\n",
      "Iteration = 870, accuracy = 75.48617559063243, loss = 0.5094772174085829\n",
      "Iteration = 880, accuracy = 75.54420715980605, loss = 0.5082178206219806\n",
      "Iteration = 890, accuracy = 75.60997627153615, loss = 0.5069949052976828\n",
      "Iteration = 900, accuracy = 75.68993087795316, loss = 0.505807244522869\n",
      "Iteration = 910, accuracy = 75.76214794181368, loss = 0.5046536562620277\n",
      "Iteration = 920, accuracy = 75.82662746311772, loss = 0.5035330014531141\n",
      "Iteration = 930, accuracy = 75.89239657484784, loss = 0.5024441822302214\n",
      "Iteration = 940, accuracy = 75.94140101103889, loss = 0.5013861402680632\n",
      "Iteration = 950, accuracy = 75.98653667595173, loss = 0.5003578552399047\n",
      "Iteration = 960, accuracy = 76.05746414938615, loss = 0.49935834337845014\n",
      "Iteration = 970, accuracy = 76.10646858557722, loss = 0.49838665612827104\n",
      "Iteration = 980, accuracy = 76.15934179304654, loss = 0.49744187887828917\n",
      "Iteration = 990, accuracy = 76.22124213349841, loss = 0.4965223475843269\n",
      "Iteration = 1000, accuracy = 76.28443206437635, loss = 0.49562729113583304\n",
      "Iteration = 1010, accuracy = 76.31409264417621, loss = 0.4947566040458985\n",
      "Iteration = 1020, accuracy = 76.36567626121943, loss = 0.49390949631504216\n",
      "Iteration = 1030, accuracy = 76.41210151655834, loss = 0.49308520638734105\n",
      "Iteration = 1040, accuracy = 76.46239554317549, loss = 0.4922826648587897\n",
      "Iteration = 1050, accuracy = 76.50366243681007, loss = 0.49150142156727855\n",
      "Iteration = 1060, accuracy = 76.56040441555761, loss = 0.4907408678591924\n",
      "Iteration = 1070, accuracy = 76.62875270813988, loss = 0.49000034684562643\n",
      "Iteration = 1080, accuracy = 76.65712369751367, loss = 0.48927922545034247\n",
      "Iteration = 1090, accuracy = 76.71644485711339, loss = 0.4885768935571964\n",
      "Iteration = 1100, accuracy = 76.76287011245229, loss = 0.48789276317932057\n",
      "Iteration = 1110, accuracy = 76.79639946353038, loss = 0.48722626765015387\n",
      "Iteration = 1120, accuracy = 76.83895594759105, loss = 0.4865768608365205\n",
      "Iteration = 1130, accuracy = 76.86603734653875, loss = 0.48594401637405166\n",
      "Iteration = 1140, accuracy = 76.92406891571237, loss = 0.4853272269252677\n",
      "Iteration = 1150, accuracy = 76.95759826679047, loss = 0.4847260034606303\n",
      "Iteration = 1160, accuracy = 76.99886516042504, loss = 0.4841398745628294\n",
      "Iteration = 1170, accuracy = 77.02078819766842, loss = 0.4835683857545195\n",
      "Iteration = 1180, accuracy = 77.03884246363356, loss = 0.48301109884964954\n",
      "Iteration = 1190, accuracy = 77.07624058598988, loss = 0.482467591328467\n",
      "Iteration = 1200, accuracy = 77.10590116578975, loss = 0.48193745573620683\n",
      "Iteration = 1210, accuracy = 77.14458887857216, loss = 0.48142029910540507\n",
      "Iteration = 1220, accuracy = 77.18714536263282, loss = 0.4809157424017151\n",
      "Iteration = 1230, accuracy = 77.2026204477458, loss = 0.4804234199930324\n",
      "Iteration = 1240, accuracy = 77.23486020839782, loss = 0.47994297914166617\n",
      "Iteration = 1250, accuracy = 77.26709996904984, loss = 0.47947407951921023\n",
      "Iteration = 1260, accuracy = 77.28644382544104, loss = 0.4790163927436818\n",
      "Iteration = 1270, accuracy = 77.29031259671928, loss = 0.4785696019383718\n",
      "Iteration = 1280, accuracy = 77.30965645311049, loss = 0.47813340131171367\n",
      "Iteration = 1290, accuracy = 77.33157949035386, loss = 0.47770749575729404\n",
      "Iteration = 1300, accuracy = 77.37155679356236, loss = 0.4772916004728919\n",
      "Iteration = 1310, accuracy = 77.38574228824925, loss = 0.4768854405971441\n",
      "Iteration = 1320, accuracy = 77.41927163932735, loss = 0.47648875086207887\n",
      "Iteration = 1330, accuracy = 77.46182812338802, loss = 0.4761012752593225\n",
      "Iteration = 1340, accuracy = 77.49148870318787, loss = 0.4757227667172984\n",
      "Iteration = 1350, accuracy = 77.50696378830084, loss = 0.47535298678619364\n",
      "Iteration = 1360, accuracy = 77.53017641597029, loss = 0.474991705326931\n",
      "Iteration = 1370, accuracy = 77.55725781491797, loss = 0.47463870019989757\n",
      "Iteration = 1380, accuracy = 77.58433921386568, loss = 0.47429375694886244\n",
      "Iteration = 1390, accuracy = 77.60884143196121, loss = 0.4739566684754753\n",
      "Iteration = 1400, accuracy = 77.62818528835243, loss = 0.4736272347001414\n",
      "Iteration = 1410, accuracy = 77.67461054369133, loss = 0.47330526220605934\n",
      "Iteration = 1420, accuracy = 77.69911276178686, loss = 0.47299056386490457\n",
      "Iteration = 1430, accuracy = 77.70813989476942, loss = 0.47268295844508884\n",
      "Iteration = 1440, accuracy = 77.72490457030847, loss = 0.4723822702066088\n",
      "Iteration = 1450, accuracy = 77.74811719797792, loss = 0.4720883284899372\n",
      "Iteration = 1460, accuracy = 77.75972351181265, loss = 0.4718009673097361\n",
      "Iteration = 1470, accuracy = 77.77906736820385, loss = 0.4715200249667656\n",
      "Iteration = 1480, accuracy = 77.79970081502114, loss = 0.4712453436925945\n",
      "Iteration = 1490, accuracy = 77.829361394821, loss = 0.47097676934110455\n",
      "Iteration = 1500, accuracy = 77.86289074589911, loss = 0.4707141511381281\n",
      "Iteration = 1510, accuracy = 77.86933869802951, loss = 0.4704573414961319\n",
      "Iteration = 1520, accuracy = 77.87965542143816, loss = 0.4702061958952879\n",
      "Iteration = 1530, accuracy = 77.90157845868151, loss = 0.46996057282649345\n",
      "Iteration = 1540, accuracy = 77.92865985762921, loss = 0.4697203337868706\n",
      "Iteration = 1550, accuracy = 77.94671412359435, loss = 0.4694853433147529\n",
      "Iteration = 1560, accuracy = 77.96863716083772, loss = 0.4692554690495434\n",
      "Iteration = 1570, accuracy = 78.0034561023419, loss = 0.46903058180209983\n",
      "Iteration = 1580, accuracy = 78.0292479108635, loss = 0.4688105556230817\n",
      "Iteration = 1590, accuracy = 78.02795832043743, loss = 0.4685952678594585\n",
      "Iteration = 1600, accuracy = 78.05890849066337, loss = 0.46838459919251246\n",
      "Iteration = 1610, accuracy = 78.06922521407202, loss = 0.46817843365371\n",
      "Iteration = 1620, accuracy = 78.09372743216755, loss = 0.467976658617392\n",
      "Iteration = 1630, accuracy = 78.10920251728051, loss = 0.4677791647712205\n",
      "Iteration = 1640, accuracy = 78.12209842154132, loss = 0.46758584606663567\n",
      "Iteration = 1650, accuracy = 78.15433818219334, loss = 0.4673965996523537\n",
      "Iteration = 1660, accuracy = 78.17497162901063, loss = 0.46721132579421576\n",
      "Iteration = 1670, accuracy = 78.18141958114103, loss = 0.46702992778468194\n",
      "Iteration = 1680, accuracy = 78.19818425668008, loss = 0.4668523118450091\n",
      "Iteration = 1690, accuracy = 78.21881770349736, loss = 0.46667838702279435\n",
      "Iteration = 1700, accuracy = 78.22139688434953, loss = 0.4665080650871639\n",
      "Iteration = 1710, accuracy = 78.2368719694625, loss = 0.46634126042347707\n",
      "Iteration = 1720, accuracy = 78.25621582585372, loss = 0.46617788992904996\n",
      "Iteration = 1730, accuracy = 78.27427009181885, loss = 0.4660178729110667\n",
      "Iteration = 1740, accuracy = 78.2897451769318, loss = 0.46586113098758003\n",
      "Iteration = 1750, accuracy = 78.3103786237491, loss = 0.4657075879922512\n",
      "Iteration = 1760, accuracy = 78.31166821417519, loss = 0.4655571698832971\n",
      "Iteration = 1770, accuracy = 78.32456411843597, loss = 0.4654098046569476\n",
      "Iteration = 1780, accuracy = 78.33101207056639, loss = 0.4652654222655847\n",
      "Iteration = 1790, accuracy = 78.35293510780976, loss = 0.4651239545406273\n",
      "Iteration = 1800, accuracy = 78.36583101207056, loss = 0.46498533512012685\n",
      "Iteration = 1810, accuracy = 78.38130609718354, loss = 0.46484949938096615\n",
      "Iteration = 1820, accuracy = 78.38904363974002, loss = 0.46471638437549506\n",
      "Iteration = 1830, accuracy = 78.40451872485299, loss = 0.4645859287723745\n",
      "Iteration = 1840, accuracy = 78.42515217167028, loss = 0.46445807280137114\n",
      "Iteration = 1850, accuracy = 78.42902094294853, loss = 0.4643327582018075\n",
      "Iteration = 1860, accuracy = 78.44836479933973, loss = 0.4642099281743654\n",
      "Iteration = 1870, accuracy = 78.45481275147014, loss = 0.4640895273359212\n",
      "Iteration = 1880, accuracy = 78.44965438976581, loss = 0.46397150167710605\n",
      "Iteration = 1890, accuracy = 78.46899824615701, loss = 0.4638557985222861\n",
      "Iteration = 1900, accuracy = 78.48447333126998, loss = 0.46374236649167905\n",
      "Iteration = 1910, accuracy = 78.48705251212215, loss = 0.46363115546534617\n",
      "Iteration = 1920, accuracy = 78.49865882595688, loss = 0.463522116548824\n",
      "Iteration = 1930, accuracy = 78.50510677808728, loss = 0.46341520204019115\n",
      "Iteration = 1940, accuracy = 78.51155473021768, loss = 0.4633103653983887\n",
      "Iteration = 1950, accuracy = 78.52702981533065, loss = 0.46320756121264983\n",
      "Iteration = 1960, accuracy = 78.53218817703498, loss = 0.4631067451729093\n",
      "Iteration = 1970, accuracy = 78.53863612916537, loss = 0.463007874041098\n",
      "Iteration = 1980, accuracy = 78.55282162385227, loss = 0.46291090562324294\n",
      "Iteration = 1990, accuracy = 78.56829670896524, loss = 0.4628157987423103\n",
      "Iteration = 2000, accuracy = 78.5773238419478, loss = 0.4627225132117471\n",
      "Iteration = 2010, accuracy = 78.59537810791292, loss = 0.46263100980968774\n",
      "Iteration = 2020, accuracy = 78.60956360259982, loss = 0.4625412502538015\n",
      "Iteration = 2030, accuracy = 78.62245950686062, loss = 0.4624531971767599\n",
      "Iteration = 2040, accuracy = 78.6108531930259, loss = 0.4623668141023151\n",
      "Iteration = 2050, accuracy = 78.6301970494171, loss = 0.4622820654219762\n",
      "Iteration = 2060, accuracy = 78.63406582069534, loss = 0.46219891637227845\n",
      "Iteration = 2070, accuracy = 78.6366450015475, loss = 0.46211733301263536\n",
      "Iteration = 2080, accuracy = 78.64051377282576, loss = 0.4620372822037704\n",
      "Iteration = 2090, accuracy = 78.64954090580831, loss = 0.46195873158671946\n",
      "Iteration = 2100, accuracy = 78.65469926751264, loss = 0.46188164956239697\n",
      "Iteration = 2110, accuracy = 78.66759517177344, loss = 0.461806005271718\n",
      "Iteration = 2120, accuracy = 78.67146394305169, loss = 0.46173176857626724\n",
      "Iteration = 2130, accuracy = 78.67662230475601, loss = 0.4616589100395046\n",
      "Iteration = 2140, accuracy = 78.67920148560817, loss = 0.46158740090849676\n",
      "Iteration = 2150, accuracy = 78.67404312390384, loss = 0.4615172130961648\n",
      "Iteration = 2160, accuracy = 78.68178066646033, loss = 0.4614483191640343\n",
      "Iteration = 2170, accuracy = 78.68564943773858, loss = 0.46138069230547757\n",
      "Iteration = 2180, accuracy = 78.68822861859074, loss = 0.4613143063294331\n",
      "Iteration = 2190, accuracy = 78.68693902816466, loss = 0.4612491356445921\n",
      "Iteration = 2200, accuracy = 78.68693902816466, loss = 0.46118515524403775\n",
      "Iteration = 2210, accuracy = 78.68693902816466, loss = 0.4611223406903247\n",
      "Iteration = 2220, accuracy = 78.68693902816466, loss = 0.4610606681009849\n",
      "Iteration = 2230, accuracy = 78.6908077994429, loss = 0.46100011413445\n",
      "Iteration = 2240, accuracy = 78.70370370370371, loss = 0.46094065597637457\n",
      "Iteration = 2250, accuracy = 78.71144124626018, loss = 0.4608822713263493\n",
      "Iteration = 2260, accuracy = 78.71531001753843, loss = 0.4608249383849939\n",
      "Iteration = 2270, accuracy = 78.71402042711236, loss = 0.460768635841413\n",
      "Iteration = 2280, accuracy = 78.724337150521, loss = 0.46071334286101057\n",
      "Iteration = 2290, accuracy = 78.73336428350356, loss = 0.46065903907364475\n",
      "Iteration = 2300, accuracy = 78.74239141648613, loss = 0.46060570456211863\n",
      "Iteration = 2310, accuracy = 78.7578665015991, loss = 0.4605533198509906\n",
      "Iteration = 2320, accuracy = 78.76689363458166, loss = 0.46050186589569947\n",
      "Iteration = 2330, accuracy = 78.76173527287733, loss = 0.46045132407199085\n",
      "Iteration = 2340, accuracy = 78.76173527287733, loss = 0.4604016761656369\n",
      "Iteration = 2350, accuracy = 78.7578665015991, loss = 0.46035290436243953\n",
      "Iteration = 2360, accuracy = 78.76560404415558, loss = 0.4603049912385102\n",
      "Iteration = 2370, accuracy = 78.77334158671206, loss = 0.4602579197508142\n",
      "Iteration = 2380, accuracy = 78.77978953884246, loss = 0.460211673227975\n",
      "Iteration = 2390, accuracy = 78.78752708139895, loss = 0.460166235361327\n",
      "Iteration = 2400, accuracy = 78.79526462395543, loss = 0.4601215901962122\n",
      "Iteration = 2410, accuracy = 78.79913339523368, loss = 0.46007772212351167\n",
      "Iteration = 2420, accuracy = 78.79655421438152, loss = 0.46003461587140454\n",
      "Iteration = 2430, accuracy = 78.8107397090684, loss = 0.45999225649734765\n",
      "Iteration = 2440, accuracy = 78.80687093779017, loss = 0.45995062938026987\n",
      "Iteration = 2450, accuracy = 78.81202929949448, loss = 0.45990972021297377\n",
      "Iteration = 2460, accuracy = 78.81202929949448, loss = 0.4598695149947396\n",
      "Iteration = 2470, accuracy = 78.82105643247705, loss = 0.459830000024123\n",
      "Iteration = 2480, accuracy = 78.82234602290312, loss = 0.45979116189194363\n",
      "Iteration = 2490, accuracy = 78.8300835654596, loss = 0.4597529874744567\n",
      "Iteration = 2500, accuracy = 78.83653151759002, loss = 0.45971546392670337\n",
      "Iteration = 2510, accuracy = 78.83911069844217, loss = 0.45967857867603407\n",
      "Iteration = 2520, accuracy = 78.83266274631178, loss = 0.4596423194157994\n",
      "Iteration = 2530, accuracy = 78.83266274631178, loss = 0.45960667409920497\n",
      "Iteration = 2540, accuracy = 78.83653151759002, loss = 0.4595716309333229\n",
      "Iteration = 2550, accuracy = 78.84040028886825, loss = 0.45953717837325836\n",
      "Iteration = 2560, accuracy = 78.83653151759002, loss = 0.45950330511646453\n",
      "Iteration = 2570, accuracy = 78.83782110801609, loss = 0.45947000009720307\n",
      "Iteration = 2580, accuracy = 78.8442690601465, loss = 0.4594372524811438\n",
      "Iteration = 2590, accuracy = 78.83524192716393, loss = 0.4594050516601021\n",
      "Iteration = 2600, accuracy = 78.84297946972042, loss = 0.4593733872469087\n",
      "Iteration = 2610, accuracy = 78.84684824099865, loss = 0.4593422490704082\n",
      "Iteration = 2620, accuracy = 78.85329619312907, loss = 0.45931162717058305\n",
      "Iteration = 2630, accuracy = 78.85587537398122, loss = 0.45928151179379795\n",
      "Iteration = 2640, accuracy = 78.8571649644073, loss = 0.45925189338816474\n",
      "Iteration = 2650, accuracy = 78.86619209738987, loss = 0.4592227625990205\n",
      "Iteration = 2660, accuracy = 78.85845455483339, loss = 0.4591941102645164\n",
      "Iteration = 2670, accuracy = 78.8571649644073, loss = 0.45916592741131834\n",
      "Iteration = 2680, accuracy = 78.86232332611162, loss = 0.45913820525040955\n",
      "Iteration = 2690, accuracy = 78.8636129165377, loss = 0.4591109351729979\n",
      "Iteration = 2700, accuracy = 78.86619209738987, loss = 0.4590841087465226\n",
      "Iteration = 2710, accuracy = 78.86232332611162, loss = 0.4590577177107578\n",
      "Iteration = 2720, accuracy = 78.8636129165377, loss = 0.4590317539740098\n",
      "Iteration = 2730, accuracy = 78.86619209738987, loss = 0.4590062096094073\n",
      "Iteration = 2740, accuracy = 78.8636129165377, loss = 0.4589810768512807\n",
      "Iteration = 2750, accuracy = 78.86232332611162, loss = 0.45895634809162705\n",
      "Iteration = 2760, accuracy = 78.86232332611162, loss = 0.4589320158766618\n",
      "Iteration = 2770, accuracy = 78.85845455483339, loss = 0.45890807290345076\n",
      "Iteration = 2780, accuracy = 78.87006086866812, loss = 0.45888451201662234\n",
      "Iteration = 2790, accuracy = 78.87521923037242, loss = 0.45886132620515974\n",
      "Iteration = 2800, accuracy = 78.88037759207675, loss = 0.45883850859926617\n",
      "Iteration = 2810, accuracy = 78.88166718250284, loss = 0.45881605246730656\n",
      "Iteration = 2820, accuracy = 78.87521923037242, loss = 0.45879395121281896\n",
      "Iteration = 2830, accuracy = 78.87392963994635, loss = 0.45877219837159794\n",
      "Iteration = 2840, accuracy = 78.8777984112246, loss = 0.45875078760884447\n",
      "Iteration = 2850, accuracy = 78.87908800165067, loss = 0.4587297127163833\n",
      "Iteration = 2860, accuracy = 78.88553595378109, loss = 0.45870896760994506\n",
      "Iteration = 2870, accuracy = 78.88682554420716, loss = 0.45868854632651124\n",
      "Iteration = 2880, accuracy = 78.88166718250284, loss = 0.4586684430217198\n",
      "Iteration = 2890, accuracy = 78.88166718250284, loss = 0.45864865196733223\n",
      "Iteration = 2900, accuracy = 78.88037759207675, loss = 0.45862916754875743\n",
      "Iteration = 2910, accuracy = 78.88682554420716, loss = 0.4586099842626327\n",
      "Iteration = 2920, accuracy = 78.89456308676365, loss = 0.4585910967144618\n",
      "Iteration = 2930, accuracy = 78.89585267718972, loss = 0.45857249961630364\n",
      "Iteration = 2940, accuracy = 78.90745899102444, loss = 0.4585541877845178\n",
      "Iteration = 2950, accuracy = 78.91132776230269, loss = 0.4585361561375575\n",
      "Iteration = 2960, accuracy = 78.91777571443309, loss = 0.4585183996938153\n",
      "Iteration = 2970, accuracy = 78.92680284741566, loss = 0.45850091356951583\n",
      "Iteration = 2980, accuracy = 78.91777571443309, loss = 0.4584836929766575\n",
      "Iteration = 2990, accuracy = 78.92422366656349, loss = 0.4584667332209992\n"
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
     ]
    }
   ],
   "source": [
    "y_pred_one, weights_one, loss_one = train_predict(X_train_one, y_train_one, X_test_one, max_iters=5000, degree=degree, lambda_=0.1, imputable_th=0.5)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 41,
=======
   "execution_count": 9,
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Cross Polynomial expansion : 20.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11791/1009431935.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_many\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_many\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_many\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_many\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_many\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_many\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimputable_th\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_11791/1689696226.py\u001b[0m in \u001b[0;36mtrain_predict\u001b[0;34m(X_train, y_train, X_test, max_iters, degree, lambda_, gamma, imputable_th, encodable_th, verbose)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimputable_th\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencodable_th\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimputable_th\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimputable_th\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencodable_th\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencodable_th\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswitch_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtX_train_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcont_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_polynomial_feature_expansion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtX_train_poly_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtX_train_poly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msimple_logarithmic_feature_expansion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FLEP/MA1/ML/cs-433/ml-project-1-gradient_surfers/notebooks/../helpers.py\u001b[0m in \u001b[0;36mcross_polynomial_feature_expansion\u001b[0;34m(tx, degree)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                 \u001b[0mtx_dlc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtx_dlc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cross Polynomial expansion of degree {} done : adding {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx_dlc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FLEP/MA1/ML/cs-433/ml-project-1-gradient_surfers/env/lib/python3.9/site-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
=======
      "Iteration = 0, accuracy = 44.75276732420771, loss = 7.463962918553235\n",
      "Iteration = 10, accuracy = 44.75276732420771, loss = 6.76497306533056\n",
      "Iteration = 20, accuracy = 44.763795266255876, loss = 5.999796782206942\n",
      "Iteration = 30, accuracy = 44.863046744689356, loss = 5.181476801591986\n",
      "Iteration = 40, accuracy = 45.31932784693217, loss = 4.3375341144432955\n",
      "Iteration = 50, accuracy = 46.67300773334436, loss = 3.5238261422359143\n",
      "Iteration = 60, accuracy = 48.61806101208938, loss = 2.8127262234623656\n",
      "Iteration = 70, accuracy = 50.83192037825841, loss = 2.2469706893787076\n",
      "Iteration = 80, accuracy = 52.936878816701814, loss = 1.8293049709843736\n",
      "Iteration = 90, accuracy = 54.93982879119971, loss = 1.5384764890333331\n",
      "Iteration = 100, accuracy = 56.778738127731145, loss = 1.341011821729397\n",
      "Iteration = 110, accuracy = 58.30610810140193, loss = 1.205505013260816\n",
      "Iteration = 120, accuracy = 59.67495140813035, loss = 1.1095992005247624\n",
      "Iteration = 130, accuracy = 60.76809616365466, loss = 1.0392104453731303\n",
      "Iteration = 140, accuracy = 61.753718484209365, loss = 0.9852179729829903\n",
      "Iteration = 150, accuracy = 62.580814137821704, loss = 0.941940359430649\n",
      "Iteration = 160, accuracy = 63.308658313000564, loss = 0.9056217795500174\n",
      "Iteration = 170, accuracy = 63.996526198254834, loss = 0.8741831888636844\n",
      "Iteration = 180, accuracy = 64.56584370649132, loss = 0.8464917952496698\n",
      "Iteration = 190, accuracy = 65.12826875094771, loss = 0.8219371656747655\n",
      "Iteration = 200, accuracy = 65.66450243303971, loss = 0.8000092387798066\n",
      "Iteration = 210, accuracy = 66.11251257874639, loss = 0.7803882634008614\n",
      "Iteration = 220, accuracy = 66.5219249272845, loss = 0.7626110402814655\n",
      "Iteration = 230, accuracy = 66.97544904401528, loss = 0.7463697996952681\n",
      "Iteration = 240, accuracy = 67.36004852294502, loss = 0.7314027076887989\n",
      "Iteration = 250, accuracy = 67.75705443667893, loss = 0.7175375317108856\n",
      "Iteration = 260, accuracy = 68.14578939387673, loss = 0.7046442051314178\n",
      "Iteration = 270, accuracy = 68.44905780020126, loss = 0.6926125831114418\n",
      "Iteration = 280, accuracy = 68.7867885254263, loss = 0.6813499989628299\n",
      "Iteration = 290, accuracy = 69.08729994623877, loss = 0.6707773303182801\n",
      "Iteration = 300, accuracy = 69.44432957004811, loss = 0.660828379240589\n",
      "Iteration = 310, accuracy = 69.74621948361661, loss = 0.6514470178290174\n",
      "Iteration = 320, accuracy = 70.00399762899247, loss = 0.642584226981309\n",
      "Iteration = 330, accuracy = 70.27280371641646, loss = 0.6341966103045641\n",
      "Iteration = 340, accuracy = 70.51403994872007, loss = 0.6262459302852748\n",
      "Iteration = 350, accuracy = 70.81317287677653, loss = 0.6186986140748497\n",
      "Iteration = 360, accuracy = 71.06681554388432, loss = 0.6115251156519939\n",
      "Iteration = 370, accuracy = 71.31218725445598, loss = 0.6046993233057161\n",
      "Iteration = 380, accuracy = 71.57961484912396, loss = 0.598198046923367\n",
      "Iteration = 390, accuracy = 71.84704244379195, loss = 0.5920005681492682\n",
      "Iteration = 400, accuracy = 72.10068511089975, loss = 0.586088253590605\n",
      "Iteration = 410, accuracy = 72.32675792288713, loss = 0.5804442341727757\n",
      "Iteration = 420, accuracy = 72.50182650290172, loss = 0.5750531456424457\n",
      "Iteration = 430, accuracy = 72.69068001047656, loss = 0.569900919803475\n",
      "Iteration = 440, accuracy = 72.89745392387962, loss = 0.5649746154100113\n",
      "Iteration = 450, accuracy = 73.08079346043036, loss = 0.5602612868431708\n",
      "Iteration = 460, accuracy = 73.26137601146905, loss = 0.5557479544417709\n",
      "Iteration = 470, accuracy = 73.4378230842397, loss = 0.5514274944297388\n",
      "Iteration = 480, accuracy = 73.59083578015797, loss = 0.5472902788048571\n",
      "Iteration = 490, accuracy = 73.77693230222076, loss = 0.5433273026836779\n",
      "Iteration = 500, accuracy = 73.98232772286781, loss = 0.5395301233084554\n",
      "Iteration = 510, accuracy = 74.128447955006, loss = 0.5358891148312418\n",
      "Iteration = 520, accuracy = 74.29111010021643, loss = 0.5323984976186446\n",
      "Iteration = 530, accuracy = 74.4441227961347, loss = 0.5290512944997996\n",
      "Iteration = 540, accuracy = 74.60678494134513, loss = 0.5258408456333528\n",
      "Iteration = 550, accuracy = 74.80115241994403, loss = 0.5227608500743389\n",
      "Iteration = 560, accuracy = 74.96795004342253, loss = 0.5198053388887702\n",
      "Iteration = 570, accuracy = 75.11544876831671, loss = 0.5169686513510483\n",
      "Iteration = 580, accuracy = 75.25329804391878, loss = 0.5142454137698542\n",
      "Iteration = 590, accuracy = 75.38839033400879, loss = 0.5116305205609788\n",
      "Iteration = 600, accuracy = 75.53037508787891, loss = 0.5091191172469645\n",
      "Iteration = 610, accuracy = 75.61584163875219, loss = 0.5067065851149504\n",
      "Iteration = 620, accuracy = 75.76885433467048, loss = 0.5043885273070753\n",
      "Iteration = 630, accuracy = 75.91359607405263, loss = 0.5021607561535781\n",
      "Iteration = 640, accuracy = 76.00181961043795, loss = 0.5000192815883372\n",
      "Iteration = 650, accuracy = 76.12450546572379, loss = 0.49795959377065696\n",
      "Iteration = 660, accuracy = 76.22789242242533, loss = 0.4959787860245703\n",
      "Iteration = 670, accuracy = 76.32852239361482, loss = 0.49407341665600024\n",
      "Iteration = 680, accuracy = 76.43466633582841, loss = 0.4922401863944908\n",
      "Iteration = 690, accuracy = 76.55045972733413, loss = 0.49047594781838066\n",
      "Iteration = 700, accuracy = 76.66625311883986, loss = 0.48877769801360527\n",
      "Iteration = 710, accuracy = 76.7820465103456, loss = 0.48714257168910596\n",
      "Iteration = 720, accuracy = 76.87854100326703, loss = 0.48556783470101605\n",
      "Iteration = 730, accuracy = 76.97779248170052, loss = 0.48405087794409946\n",
      "Iteration = 740, accuracy = 77.08255793115806, loss = 0.48258921157449297\n",
      "Iteration = 750, accuracy = 77.17078146754339, loss = 0.4811804595327577\n",
      "Iteration = 760, accuracy = 77.23694911983237, loss = 0.4798223543406435\n",
      "Iteration = 770, accuracy = 77.34171456928995, loss = 0.4785127321488596\n",
      "Iteration = 780, accuracy = 77.4409660477234, loss = 0.4772495280165314\n",
      "Iteration = 790, accuracy = 77.50851219276844, loss = 0.47603077140594857\n",
      "Iteration = 800, accuracy = 77.59949271466579, loss = 0.474854581878671\n",
      "Iteration = 810, accuracy = 77.70563665687938, loss = 0.4737191649811224\n",
      "Iteration = 820, accuracy = 77.76491184538826, loss = 0.47262280830947523\n",
      "Iteration = 830, accuracy = 77.8476214107495, loss = 0.4715638777449792\n",
      "Iteration = 840, accuracy = 77.89311167169816, loss = 0.47054081385194263\n",
      "Iteration = 850, accuracy = 77.94411590367093, loss = 0.4695521284314027\n",
      "Iteration = 860, accuracy = 78.02268999076409, loss = 0.46859640122415325\n",
      "Iteration = 870, accuracy = 78.07920819376093, loss = 0.4676722767572754\n",
      "Iteration = 880, accuracy = 78.15088981707402, loss = 0.46677846132868295\n",
      "Iteration = 890, accuracy = 78.217057469363, loss = 0.46591372012446947\n",
      "Iteration = 900, accuracy = 78.28598210716403, loss = 0.4650768744640645\n",
      "Iteration = 910, accuracy = 78.35352825220903, loss = 0.4642667991683725\n",
      "Iteration = 920, accuracy = 78.39626152764568, loss = 0.46348242004622003\n",
      "Iteration = 930, accuracy = 78.44588726686241, loss = 0.4627227114945517\n",
      "Iteration = 940, accuracy = 78.5051624553713, loss = 0.46198669420793925\n",
      "Iteration = 950, accuracy = 78.58787202073253, loss = 0.46127343299307944\n",
      "Iteration = 960, accuracy = 78.63611926719325, loss = 0.4605820346840538\n",
      "Iteration = 970, accuracy = 78.67333857160581, loss = 0.4599116461542461\n",
      "Iteration = 980, accuracy = 78.72158581806653, loss = 0.45926145242090466\n",
      "Iteration = 990, accuracy = 78.74915567318693, loss = 0.45863067483848086\n",
      "Iteration = 1000, accuracy = 78.80980935445184, loss = 0.4580185693769463\n",
      "Iteration = 1010, accuracy = 78.86632755744868, loss = 0.457424424981443\n",
      "Iteration = 1020, accuracy = 78.90492535461726, loss = 0.4568475620097167\n",
      "Iteration = 1030, accuracy = 78.9476586300539, loss = 0.45628733074391103\n",
      "Iteration = 1040, accuracy = 79.00004135478268, loss = 0.4557431099734117\n",
      "Iteration = 1050, accuracy = 79.03588216643922, loss = 0.45521430564555165\n",
      "Iteration = 1060, accuracy = 79.07172297809575, loss = 0.45470034958109906\n",
      "Iteration = 1070, accuracy = 79.10480680424024, loss = 0.4542006982515729\n",
      "Iteration = 1080, accuracy = 79.15305405070097, loss = 0.4537148316155369\n",
      "Iteration = 1090, accuracy = 79.1820023985774, loss = 0.45324182590871653\n",
      "Iteration = 1100, accuracy = 79.20267978991771, loss = 0.4527806510180545\n",
      "Iteration = 1110, accuracy = 79.2288711522821, loss = 0.4523318252148508\n",
      "Iteration = 1120, accuracy = 79.2757399059868, loss = 0.4518949125013541\n",
      "Iteration = 1130, accuracy = 79.3102022248873, loss = 0.45146949531415004\n",
      "Iteration = 1140, accuracy = 79.34604303654385, loss = 0.45105517366180836\n",
      "Iteration = 1150, accuracy = 79.38050535544436, loss = 0.450651564304516\n",
      "Iteration = 1160, accuracy = 79.42186013812498, loss = 0.45025829997368466\n",
      "Iteration = 1170, accuracy = 79.44942999324539, loss = 0.4498750286295984\n",
      "Iteration = 1180, accuracy = 79.4838923121459, loss = 0.44950141275525657\n",
      "Iteration = 1190, accuracy = 79.51835463104642, loss = 0.4491371286846546\n",
      "Iteration = 1200, accuracy = 79.55557393545897, loss = 0.4487818659638197\n",
      "Iteration = 1210, accuracy = 79.57211584853121, loss = 0.44843532674299974\n",
      "Iteration = 1220, accuracy = 79.60382118191968, loss = 0.4480972251984764\n",
      "Iteration = 1230, accuracy = 79.63828350082021, loss = 0.44776728698254653\n",
      "Iteration = 1240, accuracy = 79.66033938491654, loss = 0.44744524870028457\n",
      "Iteration = 1250, accuracy = 79.690666225549, loss = 0.44713085741176284\n",
      "Iteration = 1260, accuracy = 79.71410060240134, loss = 0.4468238701584717\n",
      "Iteration = 1270, accuracy = 79.75545538508196, loss = 0.44652405351274144\n",
      "Iteration = 1280, accuracy = 79.78164674744635, loss = 0.44623118314902543\n",
      "Iteration = 1290, accuracy = 79.8119735880788, loss = 0.4459450434359591\n",
      "Iteration = 1300, accuracy = 79.8519498780034, loss = 0.445665427048163\n",
      "Iteration = 1310, accuracy = 79.87538425485575, loss = 0.4453921345968084\n",
      "Iteration = 1320, accuracy = 79.91122506651227, loss = 0.44512497427801173\n",
      "Iteration = 1330, accuracy = 79.93052396509657, loss = 0.44486376153817025\n",
      "Iteration = 1340, accuracy = 79.9525798491929, loss = 0.44460831875539497\n",
      "Iteration = 1350, accuracy = 79.98842066084943, loss = 0.4443584749362399\n",
      "Iteration = 1360, accuracy = 80.01185503770178, loss = 0.4441140654269646\n",
      "Iteration = 1370, accuracy = 80.01874750148188, loss = 0.44387493163860475\n",
      "Iteration = 1380, accuracy = 80.05320982038239, loss = 0.44364092078516554\n",
      "Iteration = 1390, accuracy = 80.07802268999077, loss = 0.44341188563428\n",
      "Iteration = 1400, accuracy = 80.10972802337923, loss = 0.44318768426971383\n",
      "Iteration = 1410, accuracy = 80.13316240023158, loss = 0.4429681798651247\n",
      "Iteration = 1420, accuracy = 80.13867637125567, loss = 0.44275324046851844\n",
      "Iteration = 1430, accuracy = 80.17038170464414, loss = 0.4425427387968569\n",
      "Iteration = 1440, accuracy = 80.1731386901562, loss = 0.4423365520403354\n",
      "Iteration = 1450, accuracy = 80.18554512496037, loss = 0.44213456167582627\n",
      "Iteration = 1460, accuracy = 80.21862895110486, loss = 0.4419366532890426\n",
      "Iteration = 1470, accuracy = 80.2406848352012, loss = 0.4417427164049854\n",
      "Iteration = 1480, accuracy = 80.24895579173732, loss = 0.4415526443262654\n",
      "Iteration = 1490, accuracy = 80.26274071929753, loss = 0.44136633397890496\n",
      "Iteration = 1500, accuracy = 80.27928263236976, loss = 0.4411836857652523\n",
      "Iteration = 1510, accuracy = 80.28479660339384, loss = 0.4410046034236506\n",
      "Iteration = 1520, accuracy = 80.3151234440263, loss = 0.44082899389453556\n",
      "Iteration = 1530, accuracy = 80.31788042953835, loss = 0.44065676719263086\n",
      "Iteration = 1540, accuracy = 80.32890837158651, loss = 0.4404878362849534\n",
      "Iteration = 1550, accuracy = 80.31925892229437, loss = 0.4403221169743352\n",
      "Iteration = 1560, accuracy = 80.31788042953835, loss = 0.44015952778818995\n",
      "Iteration = 1570, accuracy = 80.33166535709854, loss = 0.4399999898722673\n",
      "Iteration = 1580, accuracy = 80.33028686434253, loss = 0.4398434268891517\n",
      "Iteration = 1590, accuracy = 80.33580083536661, loss = 0.43968976492127376\n",
      "Iteration = 1600, accuracy = 80.35096425568284, loss = 0.43953893237820446\n",
      "Iteration = 1610, accuracy = 80.3550997339509, loss = 0.4393908599080417\n",
      "Iteration = 1620, accuracy = 80.36612767599907, loss = 0.43924548031267574\n",
      "Iteration = 1630, accuracy = 80.3812910963153, loss = 0.43910272846674986\n",
      "Iteration = 1640, accuracy = 80.3812910963153, loss = 0.4389625412401393\n",
      "Iteration = 1650, accuracy = 80.39231903836345, loss = 0.43882485742377686\n",
      "Iteration = 1660, accuracy = 80.4088609514357, loss = 0.43868961765866993\n",
      "Iteration = 1670, accuracy = 80.41299642970377, loss = 0.4385567643679415\n",
      "Iteration = 1680, accuracy = 80.42264587899591, loss = 0.43842624169177524\n",
      "Iteration = 1690, accuracy = 80.44470176309223, loss = 0.4382979954251076\n",
      "Iteration = 1700, accuracy = 80.45710819789642, loss = 0.4381719729579438\n",
      "Iteration = 1710, accuracy = 80.46537915443255, loss = 0.43804812321817393\n",
      "Iteration = 1720, accuracy = 80.48329956026082, loss = 0.4379263966167723\n",
      "Iteration = 1730, accuracy = 80.49019202404092, loss = 0.43780674499526884\n",
      "Iteration = 1740, accuracy = 80.4888135312849, loss = 0.4376891215753865\n",
      "Iteration = 1750, accuracy = 80.49708448782101, loss = 0.4375734809107457\n",
      "Iteration = 1760, accuracy = 80.50535544435714, loss = 0.43745977884053905\n",
      "Iteration = 1770, accuracy = 80.51500489364929, loss = 0.4373479724450878\n",
      "Iteration = 1780, accuracy = 80.51224790813724, loss = 0.43723802000319306\n",
      "Iteration = 1790, accuracy = 80.53568228498959, loss = 0.4371298809512035\n",
      "Iteration = 1800, accuracy = 80.53154680672154, loss = 0.43702351584371213\n",
      "Iteration = 1810, accuracy = 80.52603283569745, loss = 0.4369188863158228\n",
      "Iteration = 1820, accuracy = 80.51776187916133, loss = 0.4368159550469094\n",
      "Iteration = 1830, accuracy = 80.52603283569745, loss = 0.43671468572579897\n",
      "Iteration = 1840, accuracy = 80.51776187916133, loss = 0.4366150430173186\n",
      "Iteration = 1850, accuracy = 80.5301683139655, loss = 0.43651699253014664\n",
      "Iteration = 1860, accuracy = 80.53568228498959, loss = 0.4364205007859082\n",
      "Iteration = 1870, accuracy = 80.54395324152571, loss = 0.4363255351894644\n",
      "Iteration = 1880, accuracy = 80.54808871979378, loss = 0.4362320640003416\n",
      "Iteration = 1890, accuracy = 80.5770370676702, loss = 0.4361400563052418\n",
      "Iteration = 1900, accuracy = 80.60184993727859, loss = 0.4360494819916119\n",
      "Iteration = 1910, accuracy = 80.61563486483878, loss = 0.43596031172220173\n",
      "Iteration = 1920, accuracy = 80.62804129964297, loss = 0.4358725169105798\n",
      "Iteration = 1930, accuracy = 80.63906924169113, loss = 0.435786069697564\n",
      "Iteration = 1940, accuracy = 80.64182622720317, loss = 0.4357009429285287\n",
      "Iteration = 1950, accuracy = 80.63769074893511, loss = 0.4356171101315541\n",
      "Iteration = 1960, accuracy = 80.64871869098327, loss = 0.4355345454963772\n",
      "Iteration = 1970, accuracy = 80.64871869098327, loss = 0.4354532238541148\n",
      "Iteration = 1980, accuracy = 80.65285416925134, loss = 0.43537312065772416\n",
      "Iteration = 1990, accuracy = 80.66526060405552, loss = 0.43529421196317325\n",
      "Iteration = 2000, accuracy = 80.67215306783562, loss = 0.43521647441128825\n",
      "Iteration = 2010, accuracy = 80.67904553161573, loss = 0.4351398852102535\n",
      "Iteration = 2020, accuracy = 80.68593799539583, loss = 0.43506442211873514\n",
      "Iteration = 2030, accuracy = 80.69145196641992, loss = 0.43499006342960395\n",
      "Iteration = 2040, accuracy = 80.69145196641992, loss = 0.43491678795423283\n",
      "Iteration = 2050, accuracy = 80.69283045917594, loss = 0.43484457500734525\n",
      "Iteration = 2060, accuracy = 80.70110141571206, loss = 0.43477340439239426\n",
      "Iteration = 2070, accuracy = 80.70661538673615, loss = 0.434703256387449\n",
      "Iteration = 2080, accuracy = 80.7107508650042, loss = 0.4346341117315668\n",
      "Iteration = 2090, accuracy = 80.72315729980839, loss = 0.4345659516116379\n",
      "Iteration = 2100, accuracy = 80.72453579256441, loss = 0.43449875764967766\n",
      "Iteration = 2110, accuracy = 80.7300497635885, loss = 0.4344325118905512\n",
      "Iteration = 2120, accuracy = 80.72729277807646, loss = 0.43436719679011415\n",
      "Iteration = 2130, accuracy = 80.7300497635885, loss = 0.4343027952037512\n",
      "Iteration = 2140, accuracy = 80.74245619839267, loss = 0.4342392903752992\n",
      "Iteration = 2150, accuracy = 80.73418524185655, loss = 0.4341766659263393\n",
      "Iteration = 2160, accuracy = 80.73832072012462, loss = 0.4341149058458422\n",
      "Iteration = 2170, accuracy = 80.73969921288064, loss = 0.4340539944801565\n",
      "Iteration = 2180, accuracy = 80.74383469114869, loss = 0.4339939165233222\n",
      "Iteration = 2190, accuracy = 80.74797016941676, loss = 0.4339345900970558\n",
      "Iteration = 2200, accuracy = 80.74659167666074, loss = 0.4338760531833187\n",
      "Iteration = 2210, accuracy = 80.75486263319686, loss = 0.4338183065875862\n",
      "Iteration = 2220, accuracy = 80.75210564768483, loss = 0.43376133629750274\n",
      "Iteration = 2230, accuracy = 80.75486263319686, loss = 0.4337051286044859\n",
      "Iteration = 2240, accuracy = 80.75899811146492, loss = 0.4336496700957674\n",
      "Iteration = 2250, accuracy = 80.75899811146492, loss = 0.43359494764668316\n",
      "Iteration = 2260, accuracy = 80.764512082489, loss = 0.4335409484132061\n",
      "Iteration = 2270, accuracy = 80.77278303902513, loss = 0.4334876598247115\n",
      "Iteration = 2280, accuracy = 80.77554002453718, loss = 0.4334350695769661\n",
      "Iteration = 2290, accuracy = 80.77140454626911, loss = 0.433383165625334\n",
      "Iteration = 2300, accuracy = 80.76726906800104, loss = 0.4333319361781898\n",
      "Iteration = 2310, accuracy = 80.76726906800104, loss = 0.43328136969053677\n",
      "Iteration = 2320, accuracy = 80.77140454626911, loss = 0.4332314548578061\n",
      "Iteration = 2330, accuracy = 80.78656796658534, loss = 0.4331821806098603\n",
      "Iteration = 2340, accuracy = 80.79483892312146, loss = 0.4331335361051644\n",
      "Iteration = 2350, accuracy = 80.79621741587748, loss = 0.43308551072513535\n",
      "Iteration = 2360, accuracy = 80.80173138690157, loss = 0.4330380940686578\n",
      "Iteration = 2370, accuracy = 80.79897440138953, loss = 0.4329912759467619\n",
      "Iteration = 2380, accuracy = 80.80586686516963, loss = 0.4329450463774576\n",
      "Iteration = 2390, accuracy = 80.80724535792564, loss = 0.4328993955807188\n",
      "Iteration = 2400, accuracy = 80.81689480721779, loss = 0.4328543139736141\n",
      "Iteration = 2410, accuracy = 80.81689480721779, loss = 0.4328097921655765\n",
      "Iteration = 2420, accuracy = 80.82103028548585, loss = 0.43276582095381083\n",
      "Iteration = 2430, accuracy = 80.82930124202198, loss = 0.43272239131883083\n",
      "Iteration = 2440, accuracy = 80.83343672029002, loss = 0.4326794944201232\n",
      "Iteration = 2450, accuracy = 80.83757219855809, loss = 0.43263712159193435\n",
      "Iteration = 2460, accuracy = 80.83895069131411, loss = 0.43259526433917544\n",
      "Iteration = 2470, accuracy = 80.84170767682616, loss = 0.432553914333442\n",
      "Iteration = 2480, accuracy = 80.84584315509422, loss = 0.43251306340914325\n",
      "Iteration = 2490, accuracy = 80.84584315509422, loss = 0.43247270355974\n",
      "Iteration = 2500, accuracy = 80.84997863336228, loss = 0.4324328269340843\n",
      "Iteration = 2510, accuracy = 80.8513571261183, loss = 0.4323934258328605\n",
      "Iteration = 2520, accuracy = 80.85687109714237, loss = 0.4323544927051222\n",
      "Iteration = 2530, accuracy = 80.86100657541044, loss = 0.4323160201449244\n",
      "Iteration = 2540, accuracy = 80.86514205367851, loss = 0.43227800088804397\n",
      "Iteration = 2550, accuracy = 80.86652054643453, loss = 0.43224042780879185\n",
      "Iteration = 2560, accuracy = 80.86927753194657, loss = 0.4322032939169043\n",
      "Iteration = 2570, accuracy = 80.87616999572667, loss = 0.43216659235452165\n",
      "Iteration = 2580, accuracy = 80.87203451745862, loss = 0.43213031639324545\n",
      "Iteration = 2590, accuracy = 80.87616999572667, loss = 0.43209445943127317\n",
      "Iteration = 2600, accuracy = 80.88168396675076, loss = 0.4320590149906062\n",
      "Iteration = 2610, accuracy = 80.88306245950677, loss = 0.4320239767143318\n",
      "Iteration = 2620, accuracy = 80.88306245950677, loss = 0.43198933836397496\n",
      "Iteration = 2630, accuracy = 80.88168396675076, loss = 0.43195509381691866\n",
      "Iteration = 2640, accuracy = 80.88719793777483, loss = 0.43192123706388974\n",
      "Iteration = 2650, accuracy = 80.8913334160429, loss = 0.4318877622065091\n",
      "Iteration = 2660, accuracy = 80.8913334160429, loss = 0.4318546634549046\n",
      "Iteration = 2670, accuracy = 80.8913334160429, loss = 0.43182193512538264\n",
      "Iteration = 2680, accuracy = 80.88719793777483, loss = 0.4317895716381612\n",
      "Iteration = 2690, accuracy = 80.88995492328688, loss = 0.43175756751515515\n",
      "Iteration = 2700, accuracy = 80.88719793777483, loss = 0.43172591737782234\n",
      "Iteration = 2710, accuracy = 80.8913334160429, loss = 0.43169461594505704\n",
      "Iteration = 2720, accuracy = 80.89684738706698, loss = 0.43166365803114054\n",
      "Iteration = 2730, accuracy = 80.898225879823, loss = 0.4316330385437386\n",
      "Iteration = 2740, accuracy = 80.90098286533505, loss = 0.43160275248194957\n",
      "Iteration = 2750, accuracy = 80.90787532911514, loss = 0.43157279493439993\n",
      "Iteration = 2760, accuracy = 80.90925382187116, loss = 0.4315431610773846\n",
      "Iteration = 2770, accuracy = 80.91614628565127, loss = 0.43151384617305405\n",
      "Iteration = 2780, accuracy = 80.91476779289525, loss = 0.43148484556764394\n",
      "Iteration = 2790, accuracy = 80.91476779289525, loss = 0.4314561546897464\n",
      "Iteration = 2800, accuracy = 80.91752477840728, loss = 0.43142776904862434\n",
      "Iteration = 2810, accuracy = 80.92028176391933, loss = 0.4313996842325638\n",
      "Iteration = 2820, accuracy = 80.91338930013923, loss = 0.4313718959072663\n",
      "Iteration = 2830, accuracy = 80.92303874943137, loss = 0.43134439981428024\n",
      "Iteration = 2840, accuracy = 80.92993121321147, loss = 0.4313171917694656\n",
      "Iteration = 2850, accuracy = 80.9313097059675, loss = 0.43129026766149503\n",
      "Iteration = 2860, accuracy = 80.93406669147953, loss = 0.4312636234503946\n",
      "Iteration = 2870, accuracy = 80.92993121321147, loss = 0.4312372551661129\n",
      "Iteration = 2880, accuracy = 80.93682367699158, loss = 0.43121115890712536\n",
      "Iteration = 2890, accuracy = 80.93544518423556, loss = 0.43118533083907085\n",
      "Iteration = 2900, accuracy = 80.93406669147953, loss = 0.4311597671934178\n",
      "Iteration = 2910, accuracy = 80.94095915525963, loss = 0.4311344642661628\n",
      "Iteration = 2920, accuracy = 80.94095915525963, loss = 0.431109418416557\n",
      "Iteration = 2930, accuracy = 80.93682367699158, loss = 0.4310846260658621\n",
      "Iteration = 2940, accuracy = 80.92993121321147, loss = 0.4310600836961348\n",
      "Iteration = 2950, accuracy = 80.9313097059675, loss = 0.43103578784903734\n",
      "Iteration = 2960, accuracy = 80.93682367699158, loss = 0.4310117351246755\n",
      "Iteration = 2970, accuracy = 80.93544518423556, loss = 0.43098792218046206\n",
      "Iteration = 2980, accuracy = 80.93268819872351, loss = 0.43096434573000586\n",
      "Iteration = 2990, accuracy = 80.93406669147953, loss = 0.43094100254202516\n"
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
     ]
    }
   ],
   "source": [
    "y_pred_many, weights_many, loss_many = train_predict(X_train_many, y_train_many, X_test_many, max_iters=5000, degree=degree, lambda_=0.1, imputable_th=0.5)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 39,
=======
   "execution_count": 10,
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.vstack([y_pred_zero, y_pred_one, y_pred_many])\n",
    "ids_test = np.hstack([ids_test_zero, ids_test_one, ids_test_many])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 40,
=======
   "execution_count": 11,
>>>>>>> 5a564ce05bf8c7bf59e1b373bdc2cf75fcce20d5
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "method = 'reg_logistic_regression_by_jet'\n",
    "time = datetime.now().strftime('%Y%m%dH%H%M%S')\n",
    "OUTPUT_PATH = f'../submissions/submission_{method}_{time}'\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lambda_': np.logspace(-4, 0, 5),\n",
    "    'degree': list(range(1, 4)),\n",
    "    'max_iters': 100,\n",
    "    'gamma': [0.01, 0.05, 0.1],\n",
    "    'cont_features': [cont_features]\n",
    "}\n",
    "metrics, params = logistic_regression_cv(ty_train, tX_train, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.4160482322829052,\n",
       " 'accuracy': 81.97132616487455,\n",
       " 'f1_score': 0.7979288473644017}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lambda_': 0.001,\n",
       " 'degree': 3,\n",
       " 'gamma': 0.1,\n",
       " 'max_iters': 1000,\n",
       " 'cont_features': (1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train, ty_train, tX_test, _, cont_features = preprocess(X_train_many, y_train_many, X_test_many, imputable_th=0, encodable_th=1, switch_encoding=True)\n",
    "tX_train_poly = build_poly(tX_train, degree=degree, cont_features=cont_features)\n",
    "ty_train_pred = predict_logistic(weights_many, tX_train_poly)\n",
    "train_accuracy = compute_accuracy(ty_train, ty_train_pred)\n",
    "train_f1 = compute_f1(ty_train, ty_train_pred)\n",
    "train_accuracy, train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = read_feature_names(DATA_TRAIN_PATH)\n",
    "one_jet_features = [f for f in features if f not in ['PRI_jet_num', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet',\n",
    "                                                     'DER_lep_eta_centrality', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = read_feature_names(DATA_TRAIN_PATH)\n",
    "many_jet_features = [f for f in features if f not in ['PRI_jet_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_distrib(tX):\n",
    "    plt.figure(figsize=(15,30))\n",
    "    for i in range(tX.shape[1]):\n",
    "        plt.subplot(10,3,i+1)\n",
    "        plt.hist(tX[:,i], bins=50)\n",
    "        plt.yscale('log')\n",
    "        plt.title(\"Feature {}: {}\".format(i, many_jet_features[i]))\n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "plot_features_distrib(X_train_many[(ty_train_pred != ty_train).reshape((-1,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_val_poly = build_poly(tX_test, degree=params['degree'], cont_features=cont_features)\n",
    "ty_val_pred = predict_logistic(weights, tX_val_poly)\n",
    "val_accuracy = compute_accuracy(ty_test, ty_val_pred)\n",
    "val_f1 = compute_f1(ty_test, ty_val_pred)\n",
    "val_accuracy, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tmp = np.hstack([X_train_val, ty_val_pred, ty_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tmp_mis = val_tmp[val_tmp[:, -1] != val_tmp[:, -2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tval_tmp = replace_values(val_tmp_mis, -999, np.nan)\n",
    "col_nan_ratio = compute_nan_ratio(tval_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_nan_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_distrib(tX):\n",
    "    plt.figure(figsize=(15,30))\n",
    "    for i in range(tX.shape[1]):\n",
    "        plt.subplot(10,3,i+1)\n",
    "        plt.hist(tX[:,i], bins=50);\n",
    "        # plt.title(\"Feature {}: {}\".format(i, labels[i]))\n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "plot_features_distrib(tval_tmp[:, :-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_poly = build_poly(tX_test, params['degree'], cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "method = 'reg_logistic_regression_by_jet'\n",
    "time = datetime.now().strftime('%Y%m%dH%H%M%S')\n",
    "OUTPUT_PATH = f'../submissions/submission_{method}_{time}'\n",
    "# y_pred = predict_logistic(weights, tX_test_poly)\n",
    "# y_pred = replace_values(y_pred, from_val=0, to_val=-1)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90e2f873873f461a8f9df4e28d8f94c04c094c3b23f4c528ba0c1f04a076d4dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
