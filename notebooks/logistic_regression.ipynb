{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y_train, X_train, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_zero, y_train_zero, X_train_one, y_train_one, X_train_many, y_train_many = split_by_jet_num(DATA_TRAIN_PATH, X_train, y_train)\n",
    "X_test_zero, ids_test_zero, X_test_one, ids_test_one, X_test_many, ids_test_many = split_by_jet_num(DATA_TRAIN_PATH, X_test, ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(X_train, y_train, X_test, max_iters=3000, degree=2, lambda_=0.01, gamma=0.01, imputable_th=0, encodable_th=1, verbose=True):\n",
    "    tX_train, ty_train, tX_test, _, cont_features = preprocess(X_train, y_train, X_test, imputable_th=imputable_th, encodable_th=encodable_th, switch_encoding=True)\n",
    "    tX_train_poly = build_poly(tX_train, degree=degree, cont_features=cont_features)\n",
    "    weights, loss = reg_logistic_regression(ty_train, tX_train_poly, max_iters=max_iters, lambda_=lambda_, gamma=gamma, verbose=verbose)\n",
    "    tX_test_poly = build_poly(tX_test, degree=degree, cont_features=cont_features)\n",
    "    y_pred = predict_logistic(weights, tX_test_poly)\n",
    "    y_pred = replace_values(y_pred, from_val=0, to_val=-1)\n",
    "    return y_pred, weights, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "lambda_ = 0.01\n",
    "gamma = 0.01\n",
    "max_iters = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 0, accuracy = 27.355799545604675, loss = 4.873196092496017\n",
      "Iteration = 10, accuracy = 29.419595047691494, loss = 4.179022439433003\n",
      "Iteration = 20, accuracy = 32.112938256282966, loss = 3.5307493515776085\n",
      "Iteration = 30, accuracy = 35.33374035410807, loss = 2.9554653891757177\n",
      "Iteration = 40, accuracy = 39.20510844434658, loss = 2.471933032252379\n",
      "Iteration = 50, accuracy = 43.218600182158475, loss = 2.0869651973936225\n",
      "Iteration = 60, accuracy = 46.84975929058281, loss = 1.792437810426043\n",
      "Iteration = 70, accuracy = 49.800326283866966, loss = 1.5678536064320647\n",
      "Iteration = 80, accuracy = 52.171389108524416, loss = 1.3918634433277213\n",
      "Iteration = 90, accuracy = 54.28622901924675, loss = 1.2524443198047621\n",
      "Iteration = 100, accuracy = 56.14184340376127, loss = 1.1415184683864295\n",
      "Iteration = 110, accuracy = 57.933402059792016, loss = 1.0528169781656658\n",
      "Iteration = 120, accuracy = 59.49776305385686, loss = 0.9817662595270782\n",
      "Iteration = 130, accuracy = 60.91199343428782, loss = 0.924545034758663\n",
      "Iteration = 140, accuracy = 62.105031377298246, loss = 0.8778040266465021\n",
      "Iteration = 150, accuracy = 63.312081510914496, loss = 0.8392441779976371\n",
      "Iteration = 160, accuracy = 64.36299580635153, loss = 0.8067874181247235\n",
      "Iteration = 170, accuracy = 65.33384044118384, loss = 0.7792586817843337\n",
      "Iteration = 180, accuracy = 66.14754836707935, loss = 0.7555344714463337\n",
      "Iteration = 190, accuracy = 66.99128241570166, loss = 0.7346243665777735\n",
      "Iteration = 200, accuracy = 67.72792329326514, loss = 0.7160481347696029\n",
      "Iteration = 210, accuracy = 68.41251889143555, loss = 0.6993721107469252\n",
      "Iteration = 220, accuracy = 69.02104831203147, loss = 0.6842591986861406\n",
      "Iteration = 230, accuracy = 69.64158818171809, loss = 0.6704379672831328\n",
      "Iteration = 240, accuracy = 70.19206709837559, loss = 0.6576517029084076\n",
      "Iteration = 250, accuracy = 70.72352947063946, loss = 0.6457694614708015\n",
      "Iteration = 260, accuracy = 71.22296397866144, loss = 0.6347663638748344\n",
      "Iteration = 270, accuracy = 71.64933492138161, loss = 0.6244458366568703\n",
      "Iteration = 280, accuracy = 72.07670673485933, loss = 0.614793414889828\n",
      "Iteration = 290, accuracy = 72.48906548697367, loss = 0.605690918676423\n",
      "Iteration = 300, accuracy = 72.90542772211825, loss = 0.5971399350670156\n",
      "Iteration = 310, accuracy = 73.2587350995366, loss = 0.5891064921006048\n",
      "Iteration = 320, accuracy = 73.64407034119684, loss = 0.5814489180509589\n",
      "Iteration = 330, accuracy = 73.99337423558497, loss = 0.5742637762036752\n",
      "Iteration = 340, accuracy = 74.28763023830733, loss = 0.5674210583753382\n",
      "Iteration = 350, accuracy = 74.60090278542332, loss = 0.5609331616677514\n",
      "Iteration = 360, accuracy = 74.87414050223694, loss = 0.554767798632203\n",
      "Iteration = 370, accuracy = 75.19441914465585, loss = 0.5489148772226019\n",
      "Iteration = 380, accuracy = 75.47866643980264, loss = 0.5433478783633393\n",
      "Iteration = 390, accuracy = 75.73989370752555, loss = 0.5380536184876982\n",
      "Iteration = 400, accuracy = 76.02814448570257, loss = 0.5330093302504914\n",
      "Iteration = 410, accuracy = 76.29337523645572, loss = 0.5282035621390011\n",
      "Iteration = 420, accuracy = 76.51156506160359, loss = 0.5236120450750756\n",
      "Iteration = 430, accuracy = 76.73876272356951, loss = 0.519242034073431\n",
      "Iteration = 440, accuracy = 76.95795341947495, loss = 0.5150480815078662\n",
      "Iteration = 450, accuracy = 77.18515108144085, loss = 0.5110462892442186\n",
      "Iteration = 460, accuracy = 77.40834526037654, loss = 0.5072228676598021\n",
      "Iteration = 470, accuracy = 77.63254031006977, loss = 0.5035582038395581\n",
      "Iteration = 480, accuracy = 77.81870227097575, loss = 0.5000507412036534\n",
      "Iteration = 490, accuracy = 78.01086945642709, loss = 0.49669396195939935\n",
      "Iteration = 500, accuracy = 78.20704012490867, loss = 0.4934689859103535\n",
      "Iteration = 510, accuracy = 78.3871968612693, loss = 0.4903806009400736\n",
      "Iteration = 520, accuracy = 78.51630918899443, loss = 0.48741684272494024\n",
      "Iteration = 530, accuracy = 78.66643980262829, loss = 0.4845723681979898\n",
      "Iteration = 540, accuracy = 78.81356780398947, loss = 0.4818408860881468\n",
      "Iteration = 550, accuracy = 78.94768448550239, loss = 0.4792169232264559\n",
      "Iteration = 560, accuracy = 79.07179245943972, loss = 0.47669331044176383\n",
      "Iteration = 570, accuracy = 79.2019056579224, loss = 0.47426834866972956\n",
      "Iteration = 580, accuracy = 79.34202756398066, loss = 0.47193629832905093\n",
      "Iteration = 590, accuracy = 79.48515208231161, loss = 0.4696923943294748\n",
      "Iteration = 600, accuracy = 79.60325483170358, loss = 0.4675320167797844\n",
      "Iteration = 610, accuracy = 79.720356710338, loss = 0.46545184137901013\n",
      "Iteration = 620, accuracy = 79.84446468427532, loss = 0.46344938234737243\n",
      "Iteration = 630, accuracy = 79.93154044018296, loss = 0.4615194026959633\n",
      "Iteration = 640, accuracy = 80.05464754336272, loss = 0.45965993403726546\n",
      "Iteration = 650, accuracy = 80.14372504078548, loss = 0.4578676577255851\n",
      "Iteration = 660, accuracy = 80.2338034089658, loss = 0.45613916832568463\n",
      "Iteration = 670, accuracy = 80.3118713280554, loss = 0.45447247214314024\n",
      "Iteration = 680, accuracy = 80.39794621320549, loss = 0.4528643136652048\n",
      "Iteration = 690, accuracy = 80.47801587381022, loss = 0.4513126619493677\n",
      "Iteration = 700, accuracy = 80.56709337123297, loss = 0.44981495059995585\n",
      "Iteration = 710, accuracy = 80.6401569365348, loss = 0.4483690191822646\n",
      "Iteration = 720, accuracy = 80.68219350835226, loss = 0.44697272959358564\n",
      "Iteration = 730, accuracy = 80.71422137259415, loss = 0.44562405703530505\n",
      "Iteration = 740, accuracy = 80.79629277471399, loss = 0.4443210702032916\n",
      "Iteration = 750, accuracy = 80.85334240789487, loss = 0.4430619265342545\n",
      "Iteration = 760, accuracy = 80.90738942880306, loss = 0.4418448671101895\n",
      "Iteration = 770, accuracy = 80.96944341577172, loss = 0.4406682120160917\n",
      "Iteration = 780, accuracy = 81.00347302152872, loss = 0.43953013026213766\n",
      "Iteration = 790, accuracy = 81.06752875001251, loss = 0.43842914220315626\n",
      "Iteration = 800, accuracy = 81.12357751243582, loss = 0.4373639516867359\n",
      "Iteration = 810, accuracy = 81.15860798895038, loss = 0.43633315263262507\n",
      "Iteration = 820, accuracy = 81.19564020698007, loss = 0.4353353995052291\n",
      "Iteration = 830, accuracy = 81.25669332319117, loss = 0.4343694050429444\n",
      "Iteration = 840, accuracy = 81.2927246704633, loss = 0.43343393694784743\n",
      "Iteration = 850, accuracy = 81.32875601773543, loss = 0.43252781505904236\n",
      "Iteration = 860, accuracy = 81.36078388197731, loss = 0.431649908930286\n",
      "Iteration = 870, accuracy = 81.38980913394653, loss = 0.4307991355818373\n",
      "Iteration = 880, accuracy = 81.41583177364306, loss = 0.42997445738340234\n",
      "Iteration = 890, accuracy = 81.45286399167276, loss = 0.42917488005412985\n",
      "Iteration = 900, accuracy = 81.48188924364197, loss = 0.4283994507685921\n",
      "Iteration = 910, accuracy = 81.49790317576291, loss = 0.427647256359227\n",
      "Iteration = 920, accuracy = 81.5349353937926, loss = 0.4269174216069705\n",
      "Iteration = 930, accuracy = 81.56596238727694, loss = 0.4262091076129629\n",
      "Iteration = 940, accuracy = 81.60199373454905, loss = 0.42552151024514284\n",
      "Iteration = 950, accuracy = 81.64302943560898, loss = 0.4248538586544266\n",
      "Iteration = 960, accuracy = 81.6670503337904, loss = 0.4242054138558811\n",
      "Iteration = 970, accuracy = 81.68506600742647, loss = 0.42357546737090557\n",
      "Iteration = 980, accuracy = 81.71409125939567, loss = 0.42296333992696944\n",
      "Iteration = 990, accuracy = 81.74611912363756, loss = 0.4223683802118793\n",
      "Iteration = 1000, accuracy = 81.76713740954631, loss = 0.4217899636799032\n",
      "Iteration = 1010, accuracy = 81.79015743697016, loss = 0.42122749140738125\n",
      "Iteration = 1020, accuracy = 81.80316875681844, loss = 0.4206803889956831\n",
      "Iteration = 1030, accuracy = 81.82718965499986, loss = 0.4201481055195767\n",
      "Iteration = 1040, accuracy = 81.84920881166616, loss = 0.4196301125192164\n",
      "Iteration = 1050, accuracy = 81.87823406363536, loss = 0.419125903034092\n",
      "Iteration = 1060, accuracy = 81.89524886651387, loss = 0.4186349906773748\n",
      "Iteration = 1070, accuracy = 81.90926105711969, loss = 0.4181569087491789\n",
      "Iteration = 1080, accuracy = 81.92027063545284, loss = 0.41769120938732424\n",
      "Iteration = 1090, accuracy = 81.9432906628767, loss = 0.41723746275423\n",
      "Iteration = 1100, accuracy = 81.96430894878544, loss = 0.41679525625862196\n",
      "Iteration = 1110, accuracy = 81.98232462242152, loss = 0.4163641938107611\n",
      "Iteration = 1120, accuracy = 81.99933942530001, loss = 0.4159438951099533\n",
      "Iteration = 1130, accuracy = 82.00634552060292, loss = 0.41553399496311166\n",
      "Iteration = 1140, accuracy = 82.0193568404512, loss = 0.4151341426331677\n",
      "Iteration = 1150, accuracy = 82.04037512635993, loss = 0.4147439109871019\n",
      "Iteration = 1160, accuracy = 82.06339515378379, loss = 0.4143628083378656\n",
      "Iteration = 1170, accuracy = 82.09041866423789, loss = 0.41399078632032893\n",
      "Iteration = 1180, accuracy = 82.10743346711638, loss = 0.4136275459995771\n",
      "Iteration = 1190, accuracy = 82.1294526237827, loss = 0.4132728000477174\n",
      "Iteration = 1200, accuracy = 82.15447439272167, loss = 0.41292627224791384\n",
      "Iteration = 1210, accuracy = 82.17349093711529, loss = 0.4125876970206863\n",
      "Iteration = 1220, accuracy = 82.19651096453914, loss = 0.4122568189715086\n",
      "Iteration = 1230, accuracy = 82.20651967211474, loss = 0.4119333924587721\n",
      "Iteration = 1240, accuracy = 82.22453534575081, loss = 0.4116171811812032\n",
      "Iteration = 1250, accuracy = 82.2415501486293, loss = 0.4113079577838712\n",
      "Iteration = 1260, accuracy = 82.24855624393223, loss = 0.411005503481933\n",
      "Iteration = 1270, accuracy = 82.25956582226537, loss = 0.41070960770131615\n",
      "Iteration = 1280, accuracy = 82.27958323741656, loss = 0.41042006773555295\n",
      "Iteration = 1290, accuracy = 82.28658933271946, loss = 0.4101366884180205\n",
      "Iteration = 1300, accuracy = 82.29759891105262, loss = 0.40985928180886794\n",
      "Iteration = 1310, accuracy = 82.31961806771892, loss = 0.4095876668959419\n",
      "Iteration = 1320, accuracy = 82.3346311290823, loss = 0.4093216693090496\n",
      "Iteration = 1330, accuracy = 82.35264680271837, loss = 0.40906112104692555\n",
      "Iteration = 1340, accuracy = 82.3836737962027, loss = 0.40880586021629955\n",
      "Iteration = 1350, accuracy = 82.40168946983876, loss = 0.40855573078247454\n",
      "Iteration = 1360, accuracy = 82.42871298029286, loss = 0.4083105823308589\n",
      "Iteration = 1370, accuracy = 82.43571907559577, loss = 0.4080702698389153\n",
      "Iteration = 1380, accuracy = 82.44272517089868, loss = 0.40783465345799447\n",
      "Iteration = 1390, accuracy = 82.4617417152923, loss = 0.4076035983045461\n",
      "Iteration = 1400, accuracy = 82.47275129362546, loss = 0.4073769742602031\n",
      "Iteration = 1410, accuracy = 82.48476174271616, loss = 0.40715465578022897\n",
      "Iteration = 1420, accuracy = 82.49376957953419, loss = 0.40693652170981176\n",
      "Iteration = 1430, accuracy = 82.50377828710978, loss = 0.40672245510765265\n",
      "Iteration = 1440, accuracy = 82.51578873620049, loss = 0.4065123430762462\n",
      "Iteration = 1450, accuracy = 82.532803539079, loss = 0.4063060765981542\n",
      "Iteration = 1460, accuracy = 82.55682443726042, loss = 0.4061035503774388\n",
      "Iteration = 1470, accuracy = 82.56883488635113, loss = 0.4059046626852006\n",
      "Iteration = 1480, accuracy = 82.57183749862381, loss = 0.40570931520789555\n",
      "Iteration = 1490, accuracy = 82.58785143074475, loss = 0.40551741289678966\n",
      "Iteration = 1500, accuracy = 82.59986187983546, loss = 0.40532886381676647\n",
      "Iteration = 1510, accuracy = 82.61287319968372, loss = 0.4051435789932333\n",
      "Iteration = 1520, accuracy = 82.61887842422908, loss = 0.4049614722581491\n",
      "Iteration = 1530, accuracy = 82.62688539028954, loss = 0.40478246010159513\n",
      "Iteration = 1540, accuracy = 82.63689409786514, loss = 0.4046064615437306\n",
      "Iteration = 1550, accuracy = 82.65290802998608, loss = 0.4044333980478549\n",
      "Iteration = 1560, accuracy = 82.65390890074364, loss = 0.404263193487082\n",
      "Iteration = 1570, accuracy = 82.66291673756167, loss = 0.4040957741503883\n",
      "Iteration = 1580, accuracy = 82.6699228328646, loss = 0.4039310687487586\n",
      "Iteration = 1590, accuracy = 82.67792979892506, loss = 0.4037690083868728\n",
      "Iteration = 1600, accuracy = 82.68293415271286, loss = 0.40360952649524096\n",
      "Iteration = 1610, accuracy = 82.68793850650066, loss = 0.40345255873998886\n",
      "Iteration = 1620, accuracy = 82.68793850650066, loss = 0.4032980429283667\n",
      "Iteration = 1630, accuracy = 82.69794721407625, loss = 0.4031459189187738\n",
      "Iteration = 1640, accuracy = 82.7039524386216, loss = 0.40299612853697864\n",
      "Iteration = 1650, accuracy = 82.70695505089428, loss = 0.4028486154976642\n",
      "Iteration = 1660, accuracy = 82.70695505089428, loss = 0.4027033253304666\n",
      "Iteration = 1670, accuracy = 82.71596288771231, loss = 0.40256020530955394\n",
      "Iteration = 1680, accuracy = 82.72497072453035, loss = 0.40241920438663065\n",
      "Iteration = 1690, accuracy = 82.72797333680302, loss = 0.4022802731270051\n",
      "Iteration = 1700, accuracy = 82.73197681983325, loss = 0.40214336364852\n",
      "Iteration = 1710, accuracy = 82.73998378589373, loss = 0.40200842956317945\n",
      "Iteration = 1720, accuracy = 82.74598901043908, loss = 0.4018754259213131\n",
      "Iteration = 1730, accuracy = 82.7579994595298, loss = 0.4017443091581395\n",
      "Iteration = 1740, accuracy = 82.76200294256003, loss = 0.4016150370426006\n",
      "Iteration = 1750, accuracy = 82.76800816710538, loss = 0.4014875686283504\n",
      "Iteration = 1760, accuracy = 82.77401339165074, loss = 0.4013618642067937\n",
      "Iteration = 1770, accuracy = 82.77301252089318, loss = 0.40123788526206733\n",
      "Iteration = 1780, accuracy = 82.77501426240829, loss = 0.4011155944278796\n",
      "Iteration = 1790, accuracy = 82.78302122846877, loss = 0.4009949554461123\n",
      "Iteration = 1800, accuracy = 82.78001861619609, loss = 0.40087593312711145\n",
      "Iteration = 1810, accuracy = 82.7820203577112, loss = 0.40075849331158087\n",
      "Iteration = 1820, accuracy = 82.78502296998388, loss = 0.4006426028340146\n",
      "Iteration = 1830, accuracy = 82.78902645301413, loss = 0.4005282294875922\n",
      "Iteration = 1840, accuracy = 82.7920290652868, loss = 0.4004153419904776\n",
      "Iteration = 1850, accuracy = 82.79403080680193, loss = 0.4003039099534538\n",
      "Iteration = 1860, accuracy = 82.80504038513506, loss = 0.40019390384884\n",
      "Iteration = 1870, accuracy = 82.80203777286239, loss = 0.4000852949806337\n",
      "Iteration = 1880, accuracy = 82.80804299740775, loss = 0.39997805545582255\n",
      "Iteration = 1890, accuracy = 82.81104560968042, loss = 0.39987215815681915\n",
      "Iteration = 1900, accuracy = 82.81604996346822, loss = 0.3997675767149679\n",
      "Iteration = 1910, accuracy = 82.82305605877113, loss = 0.3996642854850797\n",
      "Iteration = 1920, accuracy = 82.83106302483161, loss = 0.3995622595209507\n",
      "Iteration = 1930, accuracy = 82.83606737861939, loss = 0.39946147455182235\n",
      "Iteration = 1940, accuracy = 82.84007086164964, loss = 0.3993619069597447\n",
      "Iteration = 1950, accuracy = 82.84207260316475, loss = 0.3992635337578048\n",
      "Iteration = 1960, accuracy = 82.85007956922522, loss = 0.3991663325691818\n",
      "Iteration = 1970, accuracy = 82.84707695695255, loss = 0.3990702816069983\n",
      "Iteration = 1980, accuracy = 82.84707695695255, loss = 0.39897535965493075\n",
      "Iteration = 1990, accuracy = 82.85007956922522, loss = 0.39888154604855114\n",
      "Iteration = 2000, accuracy = 82.85408305225546, loss = 0.39878882065736687\n",
      "Iteration = 2010, accuracy = 82.85708566452814, loss = 0.3986971638675319\n",
      "Iteration = 2020, accuracy = 82.85608479377058, loss = 0.3986065565652019\n",
      "Iteration = 2030, accuracy = 82.85108043998278, loss = 0.3985169801205048\n",
      "Iteration = 2040, accuracy = 82.85508392301301, loss = 0.398428416372105\n",
      "Iteration = 2050, accuracy = 82.8650926305886, loss = 0.39834084761233446\n",
      "Iteration = 2060, accuracy = 82.86409175983106, loss = 0.398254256572869\n",
      "Iteration = 2070, accuracy = 82.8700969843764, loss = 0.3981686264109281\n",
      "Iteration = 2080, accuracy = 82.87109785513397, loss = 0.39808394069597625\n",
      "Iteration = 2090, accuracy = 82.87710307967933, loss = 0.39800018339690646\n",
      "Iteration = 2100, accuracy = 82.88210743346711, loss = 0.39791733886968667\n",
      "Iteration = 2110, accuracy = 82.88310830422468, loss = 0.39783539184544975\n",
      "Iteration = 2120, accuracy = 82.88811265801247, loss = 0.3977543274190107\n",
      "Iteration = 2130, accuracy = 82.89311701180027, loss = 0.39767408144618516\n",
      "Iteration = 2140, accuracy = 82.89311701180027, loss = 0.3975945756589952\n",
      "Iteration = 2150, accuracy = 82.90012310710318, loss = 0.39751591201683123\n",
      "Iteration = 2160, accuracy = 82.90312571937586, loss = 0.3974380769366478\n",
      "Iteration = 2170, accuracy = 82.90412659013342, loss = 0.39736105714763637\n",
      "Iteration = 2180, accuracy = 82.90813007316366, loss = 0.39728483968197825\n",
      "Iteration = 2190, accuracy = 82.9121335561939, loss = 0.39720941186591746\n",
      "Iteration = 2200, accuracy = 82.9121335561939, loss = 0.397134761311146\n",
      "Iteration = 2210, accuracy = 82.92014052225436, loss = 0.39706087590648476\n",
      "Iteration = 2220, accuracy = 82.92214226376949, loss = 0.3969877438098523\n",
      "Iteration = 2230, accuracy = 82.92714661755728, loss = 0.3969153534405087\n",
      "Iteration = 2240, accuracy = 82.93115010058752, loss = 0.39684369347156456\n",
      "Iteration = 2250, accuracy = 82.93315184210263, loss = 0.39677275282274443\n",
      "Iteration = 2260, accuracy = 82.93415271286018, loss = 0.3967025206533954\n",
      "Iteration = 2270, accuracy = 82.93315184210263, loss = 0.39663298635573224\n",
      "Iteration = 2280, accuracy = 82.92914835907239, loss = 0.3965641395483094\n",
      "Iteration = 2290, accuracy = 82.92914835907239, loss = 0.3964959700697107\n",
      "Iteration = 2300, accuracy = 82.92914835907239, loss = 0.39642846797244924\n",
      "Iteration = 2310, accuracy = 82.93415271286018, loss = 0.39636162351707166\n",
      "Iteration = 2320, accuracy = 82.93115010058752, loss = 0.39629542716645444\n",
      "Iteration = 2330, accuracy = 82.93515358361775, loss = 0.3962298695802905\n",
      "Iteration = 2340, accuracy = 82.93915706664798, loss = 0.3961649416097538\n",
      "Iteration = 2350, accuracy = 82.94316054967823, loss = 0.3961006342923411\n",
      "Iteration = 2360, accuracy = 82.94716403270846, loss = 0.3960369388468776\n",
      "Iteration = 2370, accuracy = 82.95216838649625, loss = 0.39597384666868735\n",
      "Iteration = 2380, accuracy = 82.9581736110416, loss = 0.39591134932491734\n",
      "Iteration = 2390, accuracy = 82.96618057710208, loss = 0.39584943855001226\n",
      "Iteration = 2400, accuracy = 82.97418754316254, loss = 0.395788106241333\n",
      "Iteration = 2410, accuracy = 82.97819102619279, loss = 0.3957273444549155\n",
      "Iteration = 2420, accuracy = 82.97919189695034, loss = 0.3956671454013624\n",
      "Iteration = 2430, accuracy = 82.98219450922304, loss = 0.39560750144186546\n",
      "Iteration = 2440, accuracy = 82.9801927677079, loss = 0.39554840508435124\n",
      "Iteration = 2450, accuracy = 82.98319537998057, loss = 0.39548984897974876\n",
      "Iteration = 2460, accuracy = 82.98819973376837, loss = 0.3954318259183715\n",
      "Iteration = 2470, accuracy = 82.99120234604106, loss = 0.39537432882641227\n",
      "Iteration = 2480, accuracy = 82.9902014752835, loss = 0.3953173507625461\n",
      "Iteration = 2490, accuracy = 82.9952058290713, loss = 0.3952608849146363\n",
      "Iteration = 2500, accuracy = 83.00021018285909, loss = 0.39520492459654233\n",
      "Iteration = 2510, accuracy = 83.00021018285909, loss = 0.39514946324502326\n",
      "Iteration = 2520, accuracy = 83.00521453664689, loss = 0.3950944944167361\n",
      "Iteration = 2530, accuracy = 83.00421366588932, loss = 0.3950400117853234\n",
      "Iteration = 2540, accuracy = 83.01021889043467, loss = 0.3949860091385886\n",
      "Iteration = 2550, accuracy = 83.01522324422247, loss = 0.3949324803757557\n",
      "Iteration = 2560, accuracy = 83.01922672725271, loss = 0.3948794195048092\n",
      "Iteration = 2570, accuracy = 83.01922672725271, loss = 0.3948268206399144\n",
      "Iteration = 2580, accuracy = 83.02122846876783, loss = 0.3947746779989119\n",
      "Iteration = 2590, accuracy = 83.02222933952538, loss = 0.39472298590088567\n",
      "Iteration = 2600, accuracy = 83.02623282255563, loss = 0.39467173876380274\n",
      "Iteration = 2610, accuracy = 83.02723369331318, loss = 0.3946209311022201\n",
      "Iteration = 2620, accuracy = 83.02723369331318, loss = 0.3945705575250586\n",
      "Iteration = 2630, accuracy = 83.0242310810405, loss = 0.3945206127334407\n",
      "Iteration = 2640, accuracy = 83.02323021028295, loss = 0.39447109151858845\n",
      "Iteration = 2650, accuracy = 83.02323021028295, loss = 0.3944219887597838\n",
      "Iteration = 2660, accuracy = 83.0172249857376, loss = 0.39437329942238386\n",
      "Iteration = 2670, accuracy = 83.01822585649515, loss = 0.39432501855589436\n",
      "Iteration = 2680, accuracy = 83.0242310810405, loss = 0.39427714129209496\n",
      "Iteration = 2690, accuracy = 83.03023630558586, loss = 0.39422966284321864\n",
      "Iteration = 2700, accuracy = 83.02823456407074, loss = 0.3941825785001799\n",
      "Iteration = 2710, accuracy = 83.0292354348283, loss = 0.39413588363085317\n",
      "Iteration = 2720, accuracy = 83.03223804710098, loss = 0.39408957367839775\n",
      "Iteration = 2730, accuracy = 83.03323891785853, loss = 0.3940436441596285\n",
      "Iteration = 2740, accuracy = 83.03724240088877, loss = 0.3939980906634309\n",
      "Iteration = 2750, accuracy = 83.03524065937366, loss = 0.39395290884921963\n",
      "Iteration = 2760, accuracy = 83.03824327164632, loss = 0.3939080944454371\n",
      "Iteration = 2770, accuracy = 83.04124588391902, loss = 0.393863643248094\n",
      "Iteration = 2780, accuracy = 83.04124588391902, loss = 0.39381955111934724\n",
      "Iteration = 2790, accuracy = 83.04024501316145, loss = 0.3937758139861162\n",
      "Iteration = 2800, accuracy = 83.04625023770681, loss = 0.3937324278387361\n",
      "Iteration = 2810, accuracy = 83.04324762543412, loss = 0.3936893887296446\n",
      "Iteration = 2820, accuracy = 83.04424849619168, loss = 0.3936466927721059\n",
      "Iteration = 2830, accuracy = 83.04725110846437, loss = 0.3936043361389645\n",
      "Iteration = 2840, accuracy = 83.04725110846437, loss = 0.39356231506143413\n",
      "Iteration = 2850, accuracy = 83.05125459149461, loss = 0.39352062582791614\n",
      "Iteration = 2860, accuracy = 83.05225546225216, loss = 0.393479264782848\n",
      "Iteration = 2870, accuracy = 83.05425720376728, loss = 0.39343822832558234\n",
      "Iteration = 2880, accuracy = 83.05325633300971, loss = 0.39339751290929326\n",
      "Iteration = 2890, accuracy = 83.05525807452484, loss = 0.3933571150399111\n",
      "Iteration = 2900, accuracy = 83.06026242831264, loss = 0.39331703127508283\n",
      "Iteration = 2910, accuracy = 83.06226416982774, loss = 0.39327725822315923\n",
      "Iteration = 2920, accuracy = 83.06526678210042, loss = 0.39323779254220675\n",
      "Iteration = 2930, accuracy = 83.06526678210042, loss = 0.39319863093904467\n",
      "Iteration = 2940, accuracy = 83.06426591134287, loss = 0.39315977016830406\n",
      "Iteration = 2950, accuracy = 83.0632650405853, loss = 0.39312120703151154\n",
      "Iteration = 2960, accuracy = 83.06626765285799, loss = 0.3930829383761951\n",
      "Iteration = 2970, accuracy = 83.0682693943731, loss = 0.39304496109500997\n",
      "Iteration = 2980, accuracy = 83.07027113588822, loss = 0.3930072721248873\n",
      "Iteration = 2990, accuracy = 83.07527548967602, loss = 0.39296986844620285\n"
     ]
    }
   ],
   "source": [
    "y_pred_zero, weights_zero, loss_zero = train_predict(X_train_zero, y_train_zero, X_test_zero, max_iters=max_iters, degree=degree, lambda_=lambda_, imputable_th=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 0, accuracy = 35.73841947797379, loss = 7.117980078706898\n",
      "Iteration = 10, accuracy = 35.77710719075621, loss = 6.381175450079116\n",
      "Iteration = 20, accuracy = 36.0053646961725, loss = 5.613945176883278\n",
      "Iteration = 30, accuracy = 36.668214175177965, loss = 4.834021723739728\n",
      "Iteration = 40, accuracy = 38.21701227690086, loss = 4.067727630503282\n",
      "Iteration = 50, accuracy = 40.4763747033942, loss = 3.353485689659376\n",
      "Iteration = 60, accuracy = 43.497885071701226, loss = 2.7298565810200697\n",
      "Iteration = 70, accuracy = 46.92561642422366, loss = 2.219677830034287\n",
      "Iteration = 80, accuracy = 50.17925306922522, loss = 1.8283380728348302\n",
      "Iteration = 90, accuracy = 53.363251831218406, loss = 1.5460220453499545\n",
      "Iteration = 100, accuracy = 55.79412978438049, loss = 1.3470902878271502\n",
      "Iteration = 110, accuracy = 57.66403590219746, loss = 1.2050737017061655\n",
      "Iteration = 120, accuracy = 59.243784174146285, loss = 1.1038578342219219\n",
      "Iteration = 130, accuracy = 60.48952852574022, loss = 1.032455481960418\n",
      "Iteration = 140, accuracy = 61.49282987723099, loss = 0.9821066623636382\n",
      "Iteration = 150, accuracy = 62.33751160631383, loss = 0.9454979516031984\n",
      "Iteration = 160, accuracy = 62.947487877849994, loss = 0.9174613572526007\n",
      "Iteration = 170, accuracy = 63.53554111214278, loss = 0.8946271669552018\n",
      "Iteration = 180, accuracy = 64.06298359640978, loss = 0.8749652266565582\n",
      "Iteration = 190, accuracy = 64.4756525327556, loss = 0.857376224565091\n",
      "Iteration = 200, accuracy = 64.83673785205819, loss = 0.841255081506296\n",
      "Iteration = 210, accuracy = 65.19911276178686, loss = 0.8262532032583684\n",
      "Iteration = 220, accuracy = 65.49442896935933, loss = 0.8121780078364961\n",
      "Iteration = 230, accuracy = 65.79490353863613, loss = 0.7989016970488224\n",
      "Iteration = 240, accuracy = 66.08248220365212, loss = 0.7863365849330959\n",
      "Iteration = 250, accuracy = 66.32750438460745, loss = 0.7744148086141494\n",
      "Iteration = 260, accuracy = 66.60218714536263, loss = 0.7630757788715739\n",
      "Iteration = 270, accuracy = 66.85881564015268, loss = 0.7522605985395976\n",
      "Iteration = 280, accuracy = 67.11544413494273, loss = 0.7419503984853519\n",
      "Iteration = 290, accuracy = 67.38625812441968, loss = 0.732088463433122\n",
      "Iteration = 300, accuracy = 67.61580522026205, loss = 0.7226454212414629\n",
      "Iteration = 310, accuracy = 67.80795419374806, loss = 0.713605511058275\n",
      "Iteration = 320, accuracy = 68.01041989064274, loss = 0.704975450387736\n",
      "Iteration = 330, accuracy = 68.25673166202414, loss = 0.6967320464432596\n",
      "Iteration = 340, accuracy = 68.43211595997111, loss = 0.6888532194820945\n",
      "Iteration = 350, accuracy = 68.60492107706592, loss = 0.6813175824834158\n",
      "Iteration = 360, accuracy = 68.80738677396059, loss = 0.6741043639981611\n",
      "Iteration = 370, accuracy = 69.01114206128133, loss = 0.6671936905771899\n",
      "Iteration = 380, accuracy = 69.1942639017848, loss = 0.6605647448183168\n",
      "Iteration = 390, accuracy = 69.42896935933148, loss = 0.6542017989827901\n",
      "Iteration = 400, accuracy = 69.61724956153925, loss = 0.6480915084529222\n",
      "Iteration = 410, accuracy = 69.80810894459918, loss = 0.6422206235789326\n",
      "Iteration = 420, accuracy = 69.99252037552873, loss = 0.6365766066240461\n",
      "Iteration = 430, accuracy = 70.16403590219747, loss = 0.6311469008206152\n",
      "Iteration = 440, accuracy = 70.3690807799443, loss = 0.6259191147271002\n",
      "Iteration = 450, accuracy = 70.588311152378, loss = 0.6208817825066909\n",
      "Iteration = 460, accuracy = 70.74306200350769, loss = 0.6160247856749055\n",
      "Iteration = 470, accuracy = 70.91328793975033, loss = 0.6113392724942872\n",
      "Iteration = 480, accuracy = 71.06545961002786, loss = 0.6068173890874206\n",
      "Iteration = 490, accuracy = 71.23052718456618, loss = 0.6024508123866792\n",
      "Iteration = 500, accuracy = 71.39043639740018, loss = 0.5982331208288709\n",
      "Iteration = 510, accuracy = 71.53100175384299, loss = 0.59415717239087\n",
      "Iteration = 520, accuracy = 71.67285670071185, loss = 0.5902183531634065\n",
      "Iteration = 530, accuracy = 71.81987000928504, loss = 0.5864097415856931\n",
      "Iteration = 540, accuracy = 71.94753946146703, loss = 0.5827287856466787\n",
      "Iteration = 550, accuracy = 72.09197358918807, loss = 0.5791706441769641\n",
      "Iteration = 560, accuracy = 72.21577427009181, loss = 0.575729716006215\n",
      "Iteration = 570, accuracy = 72.3434437222738, loss = 0.5724013606663764\n",
      "Iteration = 580, accuracy = 72.48400907871661, loss = 0.5691823544820712\n",
      "Iteration = 590, accuracy = 72.61554730217684, loss = 0.5660690238685543\n",
      "Iteration = 600, accuracy = 72.75611265861961, loss = 0.5630575003367568\n",
      "Iteration = 610, accuracy = 72.86314866398432, loss = 0.5601440768260705\n",
      "Iteration = 620, accuracy = 72.98694934488806, loss = 0.5573252003479903\n",
      "Iteration = 630, accuracy = 73.1107500257918, loss = 0.554597465410731\n",
      "Iteration = 640, accuracy = 73.22294439286082, loss = 0.5519576078368322\n",
      "Iteration = 650, accuracy = 73.35319302589498, loss = 0.549402498698579\n",
      "Iteration = 660, accuracy = 73.4640978025379, loss = 0.5469285421536701\n",
      "Iteration = 670, accuracy = 73.56210667492005, loss = 0.5445306310687106\n",
      "Iteration = 680, accuracy = 73.67688022284122, loss = 0.5422088273388073\n",
      "Iteration = 690, accuracy = 73.77617868564944, loss = 0.539960481934641\n",
      "Iteration = 700, accuracy = 73.88966264314453, loss = 0.5377830483181596\n",
      "Iteration = 710, accuracy = 74.01346332404827, loss = 0.5356740753723249\n",
      "Iteration = 720, accuracy = 74.1127617868565, loss = 0.5336312004892224\n",
      "Iteration = 730, accuracy = 74.21077065923862, loss = 0.531652143175891\n",
      "Iteration = 740, accuracy = 74.31651707417724, loss = 0.5297346995679941\n",
      "Iteration = 750, accuracy = 74.392602909316, loss = 0.527876738150345\n",
      "Iteration = 760, accuracy = 74.46739915402868, loss = 0.5260761967824784\n",
      "Iteration = 770, accuracy = 74.58991024450634, loss = 0.5243310808874246\n",
      "Iteration = 780, accuracy = 74.68791911688848, loss = 0.522639462472961\n",
      "Iteration = 790, accuracy = 74.77819044671412, loss = 0.5209994795759482\n",
      "Iteration = 800, accuracy = 74.89812235633963, loss = 0.5194093357548207\n",
      "Iteration = 810, accuracy = 74.97162901062623, loss = 0.5178628811773984\n",
      "Iteration = 820, accuracy = 75.06061075002579, loss = 0.5163621681428302\n",
      "Iteration = 830, accuracy = 75.14443412772104, loss = 0.514903607792853\n",
      "Iteration = 840, accuracy = 75.23986381925101, loss = 0.5134872822384204\n",
      "Iteration = 850, accuracy = 75.33529351078097, loss = 0.51211059969868\n",
      "Iteration = 860, accuracy = 75.40622098421541, loss = 0.5107743694769517\n",
      "Iteration = 870, accuracy = 75.48617559063243, loss = 0.5094772174085829\n",
      "Iteration = 880, accuracy = 75.54420715980605, loss = 0.5082178206219806\n",
      "Iteration = 890, accuracy = 75.60997627153615, loss = 0.5069949052976828\n",
      "Iteration = 900, accuracy = 75.68993087795316, loss = 0.505807244522869\n",
      "Iteration = 910, accuracy = 75.76214794181368, loss = 0.5046536562620277\n",
      "Iteration = 920, accuracy = 75.82662746311772, loss = 0.5035330014531141\n",
      "Iteration = 930, accuracy = 75.89239657484784, loss = 0.5024441822302214\n",
      "Iteration = 940, accuracy = 75.94140101103889, loss = 0.5013861402680632\n",
      "Iteration = 950, accuracy = 75.98653667595173, loss = 0.5003578552399047\n",
      "Iteration = 960, accuracy = 76.05746414938615, loss = 0.49935834337845014\n",
      "Iteration = 970, accuracy = 76.10646858557722, loss = 0.49838665612827104\n",
      "Iteration = 980, accuracy = 76.15934179304654, loss = 0.49744187887828917\n",
      "Iteration = 990, accuracy = 76.22124213349841, loss = 0.4965223475843269\n",
      "Iteration = 1000, accuracy = 76.28443206437635, loss = 0.49562729113583304\n",
      "Iteration = 1010, accuracy = 76.31409264417621, loss = 0.4947566040458985\n",
      "Iteration = 1020, accuracy = 76.36567626121943, loss = 0.49390949631504216\n",
      "Iteration = 1030, accuracy = 76.41210151655834, loss = 0.49308520638734105\n",
      "Iteration = 1040, accuracy = 76.46239554317549, loss = 0.4922826648587897\n",
      "Iteration = 1050, accuracy = 76.50366243681007, loss = 0.49150142156727855\n",
      "Iteration = 1060, accuracy = 76.56040441555761, loss = 0.4907408678591924\n",
      "Iteration = 1070, accuracy = 76.62875270813988, loss = 0.49000034684562643\n",
      "Iteration = 1080, accuracy = 76.65712369751367, loss = 0.48927922545034247\n",
      "Iteration = 1090, accuracy = 76.71644485711339, loss = 0.4885768935571964\n",
      "Iteration = 1100, accuracy = 76.76287011245229, loss = 0.48789276317932057\n",
      "Iteration = 1110, accuracy = 76.79639946353038, loss = 0.48722626765015387\n",
      "Iteration = 1120, accuracy = 76.83895594759105, loss = 0.4865768608365205\n",
      "Iteration = 1130, accuracy = 76.86603734653875, loss = 0.48594401637405166\n",
      "Iteration = 1140, accuracy = 76.92406891571237, loss = 0.4853272269252677\n",
      "Iteration = 1150, accuracy = 76.95759826679047, loss = 0.4847260034606303\n",
      "Iteration = 1160, accuracy = 76.99886516042504, loss = 0.4841398745628294\n",
      "Iteration = 1170, accuracy = 77.02078819766842, loss = 0.4835683857545195\n",
      "Iteration = 1180, accuracy = 77.03884246363356, loss = 0.48301109884964954\n",
      "Iteration = 1190, accuracy = 77.07624058598988, loss = 0.482467591328467\n",
      "Iteration = 1200, accuracy = 77.10590116578975, loss = 0.48193745573620683\n",
      "Iteration = 1210, accuracy = 77.14458887857216, loss = 0.48142029910540507\n",
      "Iteration = 1220, accuracy = 77.18714536263282, loss = 0.4809157424017151\n",
      "Iteration = 1230, accuracy = 77.2026204477458, loss = 0.4804234199930324\n",
      "Iteration = 1240, accuracy = 77.23486020839782, loss = 0.47994297914166617\n",
      "Iteration = 1250, accuracy = 77.26709996904984, loss = 0.47947407951921023\n",
      "Iteration = 1260, accuracy = 77.28644382544104, loss = 0.4790163927436818\n",
      "Iteration = 1270, accuracy = 77.29031259671928, loss = 0.4785696019383718\n",
      "Iteration = 1280, accuracy = 77.30965645311049, loss = 0.47813340131171367\n",
      "Iteration = 1290, accuracy = 77.33157949035386, loss = 0.47770749575729404\n",
      "Iteration = 1300, accuracy = 77.37155679356236, loss = 0.4772916004728919\n",
      "Iteration = 1310, accuracy = 77.38574228824925, loss = 0.4768854405971441\n",
      "Iteration = 1320, accuracy = 77.41927163932735, loss = 0.47648875086207887\n",
      "Iteration = 1330, accuracy = 77.46182812338802, loss = 0.4761012752593225\n",
      "Iteration = 1340, accuracy = 77.49148870318787, loss = 0.4757227667172984\n",
      "Iteration = 1350, accuracy = 77.50696378830084, loss = 0.47535298678619364\n",
      "Iteration = 1360, accuracy = 77.53017641597029, loss = 0.474991705326931\n",
      "Iteration = 1370, accuracy = 77.55725781491797, loss = 0.47463870019989757\n",
      "Iteration = 1380, accuracy = 77.58433921386568, loss = 0.47429375694886244\n",
      "Iteration = 1390, accuracy = 77.60884143196121, loss = 0.4739566684754753\n",
      "Iteration = 1400, accuracy = 77.62818528835243, loss = 0.4736272347001414\n",
      "Iteration = 1410, accuracy = 77.67461054369133, loss = 0.47330526220605934\n",
      "Iteration = 1420, accuracy = 77.69911276178686, loss = 0.47299056386490457\n",
      "Iteration = 1430, accuracy = 77.70813989476942, loss = 0.47268295844508884\n",
      "Iteration = 1440, accuracy = 77.72490457030847, loss = 0.4723822702066088\n",
      "Iteration = 1450, accuracy = 77.74811719797792, loss = 0.4720883284899372\n",
      "Iteration = 1460, accuracy = 77.75972351181265, loss = 0.4718009673097361\n",
      "Iteration = 1470, accuracy = 77.77906736820385, loss = 0.4715200249667656\n",
      "Iteration = 1480, accuracy = 77.79970081502114, loss = 0.4712453436925945\n",
      "Iteration = 1490, accuracy = 77.829361394821, loss = 0.47097676934110455\n",
      "Iteration = 1500, accuracy = 77.86289074589911, loss = 0.4707141511381281\n",
      "Iteration = 1510, accuracy = 77.86933869802951, loss = 0.4704573414961319\n",
      "Iteration = 1520, accuracy = 77.87965542143816, loss = 0.4702061958952879\n",
      "Iteration = 1530, accuracy = 77.90157845868151, loss = 0.46996057282649345\n",
      "Iteration = 1540, accuracy = 77.92865985762921, loss = 0.4697203337868706\n",
      "Iteration = 1550, accuracy = 77.94671412359435, loss = 0.4694853433147529\n",
      "Iteration = 1560, accuracy = 77.96863716083772, loss = 0.4692554690495434\n",
      "Iteration = 1570, accuracy = 78.0034561023419, loss = 0.46903058180209983\n",
      "Iteration = 1580, accuracy = 78.0292479108635, loss = 0.4688105556230817\n",
      "Iteration = 1590, accuracy = 78.02795832043743, loss = 0.4685952678594585\n",
      "Iteration = 1600, accuracy = 78.05890849066337, loss = 0.46838459919251246\n",
      "Iteration = 1610, accuracy = 78.06922521407202, loss = 0.46817843365371\n",
      "Iteration = 1620, accuracy = 78.09372743216755, loss = 0.467976658617392\n",
      "Iteration = 1630, accuracy = 78.10920251728051, loss = 0.4677791647712205\n",
      "Iteration = 1640, accuracy = 78.12209842154132, loss = 0.46758584606663567\n",
      "Iteration = 1650, accuracy = 78.15433818219334, loss = 0.4673965996523537\n",
      "Iteration = 1660, accuracy = 78.17497162901063, loss = 0.46721132579421576\n",
      "Iteration = 1670, accuracy = 78.18141958114103, loss = 0.46702992778468194\n",
      "Iteration = 1680, accuracy = 78.19818425668008, loss = 0.4668523118450091\n",
      "Iteration = 1690, accuracy = 78.21881770349736, loss = 0.46667838702279435\n",
      "Iteration = 1700, accuracy = 78.22139688434953, loss = 0.4665080650871639\n",
      "Iteration = 1710, accuracy = 78.2368719694625, loss = 0.46634126042347707\n",
      "Iteration = 1720, accuracy = 78.25621582585372, loss = 0.46617788992904996\n",
      "Iteration = 1730, accuracy = 78.27427009181885, loss = 0.4660178729110667\n",
      "Iteration = 1740, accuracy = 78.2897451769318, loss = 0.46586113098758003\n",
      "Iteration = 1750, accuracy = 78.3103786237491, loss = 0.4657075879922512\n",
      "Iteration = 1760, accuracy = 78.31166821417519, loss = 0.4655571698832971\n",
      "Iteration = 1770, accuracy = 78.32456411843597, loss = 0.4654098046569476\n",
      "Iteration = 1780, accuracy = 78.33101207056639, loss = 0.4652654222655847\n",
      "Iteration = 1790, accuracy = 78.35293510780976, loss = 0.4651239545406273\n",
      "Iteration = 1800, accuracy = 78.36583101207056, loss = 0.46498533512012685\n",
      "Iteration = 1810, accuracy = 78.38130609718354, loss = 0.46484949938096615\n",
      "Iteration = 1820, accuracy = 78.38904363974002, loss = 0.46471638437549506\n",
      "Iteration = 1830, accuracy = 78.40451872485299, loss = 0.4645859287723745\n",
      "Iteration = 1840, accuracy = 78.42515217167028, loss = 0.46445807280137114\n",
      "Iteration = 1850, accuracy = 78.42902094294853, loss = 0.4643327582018075\n",
      "Iteration = 1860, accuracy = 78.44836479933973, loss = 0.4642099281743654\n",
      "Iteration = 1870, accuracy = 78.45481275147014, loss = 0.4640895273359212\n",
      "Iteration = 1880, accuracy = 78.44965438976581, loss = 0.46397150167710605\n",
      "Iteration = 1890, accuracy = 78.46899824615701, loss = 0.4638557985222861\n",
      "Iteration = 1900, accuracy = 78.48447333126998, loss = 0.46374236649167905\n",
      "Iteration = 1910, accuracy = 78.48705251212215, loss = 0.46363115546534617\n",
      "Iteration = 1920, accuracy = 78.49865882595688, loss = 0.463522116548824\n",
      "Iteration = 1930, accuracy = 78.50510677808728, loss = 0.46341520204019115\n",
      "Iteration = 1940, accuracy = 78.51155473021768, loss = 0.4633103653983887\n",
      "Iteration = 1950, accuracy = 78.52702981533065, loss = 0.46320756121264983\n",
      "Iteration = 1960, accuracy = 78.53218817703498, loss = 0.4631067451729093\n",
      "Iteration = 1970, accuracy = 78.53863612916537, loss = 0.463007874041098\n",
      "Iteration = 1980, accuracy = 78.55282162385227, loss = 0.46291090562324294\n",
      "Iteration = 1990, accuracy = 78.56829670896524, loss = 0.4628157987423103\n",
      "Iteration = 2000, accuracy = 78.5773238419478, loss = 0.4627225132117471\n",
      "Iteration = 2010, accuracy = 78.59537810791292, loss = 0.46263100980968774\n",
      "Iteration = 2020, accuracy = 78.60956360259982, loss = 0.4625412502538015\n",
      "Iteration = 2030, accuracy = 78.62245950686062, loss = 0.4624531971767599\n",
      "Iteration = 2040, accuracy = 78.6108531930259, loss = 0.4623668141023151\n",
      "Iteration = 2050, accuracy = 78.6301970494171, loss = 0.4622820654219762\n",
      "Iteration = 2060, accuracy = 78.63406582069534, loss = 0.46219891637227845\n",
      "Iteration = 2070, accuracy = 78.6366450015475, loss = 0.46211733301263536\n",
      "Iteration = 2080, accuracy = 78.64051377282576, loss = 0.4620372822037704\n",
      "Iteration = 2090, accuracy = 78.64954090580831, loss = 0.46195873158671946\n",
      "Iteration = 2100, accuracy = 78.65469926751264, loss = 0.46188164956239697\n",
      "Iteration = 2110, accuracy = 78.66759517177344, loss = 0.461806005271718\n",
      "Iteration = 2120, accuracy = 78.67146394305169, loss = 0.46173176857626724\n",
      "Iteration = 2130, accuracy = 78.67662230475601, loss = 0.4616589100395046\n",
      "Iteration = 2140, accuracy = 78.67920148560817, loss = 0.46158740090849676\n",
      "Iteration = 2150, accuracy = 78.67404312390384, loss = 0.4615172130961648\n",
      "Iteration = 2160, accuracy = 78.68178066646033, loss = 0.4614483191640343\n",
      "Iteration = 2170, accuracy = 78.68564943773858, loss = 0.46138069230547757\n",
      "Iteration = 2180, accuracy = 78.68822861859074, loss = 0.4613143063294331\n",
      "Iteration = 2190, accuracy = 78.68693902816466, loss = 0.4612491356445921\n",
      "Iteration = 2200, accuracy = 78.68693902816466, loss = 0.46118515524403775\n",
      "Iteration = 2210, accuracy = 78.68693902816466, loss = 0.4611223406903247\n",
      "Iteration = 2220, accuracy = 78.68693902816466, loss = 0.4610606681009849\n",
      "Iteration = 2230, accuracy = 78.6908077994429, loss = 0.46100011413445\n",
      "Iteration = 2240, accuracy = 78.70370370370371, loss = 0.46094065597637457\n",
      "Iteration = 2250, accuracy = 78.71144124626018, loss = 0.4608822713263493\n",
      "Iteration = 2260, accuracy = 78.71531001753843, loss = 0.4608249383849939\n",
      "Iteration = 2270, accuracy = 78.71402042711236, loss = 0.460768635841413\n",
      "Iteration = 2280, accuracy = 78.724337150521, loss = 0.46071334286101057\n",
      "Iteration = 2290, accuracy = 78.73336428350356, loss = 0.46065903907364475\n",
      "Iteration = 2300, accuracy = 78.74239141648613, loss = 0.46060570456211863\n",
      "Iteration = 2310, accuracy = 78.7578665015991, loss = 0.4605533198509906\n",
      "Iteration = 2320, accuracy = 78.76689363458166, loss = 0.46050186589569947\n",
      "Iteration = 2330, accuracy = 78.76173527287733, loss = 0.46045132407199085\n",
      "Iteration = 2340, accuracy = 78.76173527287733, loss = 0.4604016761656369\n",
      "Iteration = 2350, accuracy = 78.7578665015991, loss = 0.46035290436243953\n",
      "Iteration = 2360, accuracy = 78.76560404415558, loss = 0.4603049912385102\n",
      "Iteration = 2370, accuracy = 78.77334158671206, loss = 0.4602579197508142\n",
      "Iteration = 2380, accuracy = 78.77978953884246, loss = 0.460211673227975\n",
      "Iteration = 2390, accuracy = 78.78752708139895, loss = 0.460166235361327\n",
      "Iteration = 2400, accuracy = 78.79526462395543, loss = 0.4601215901962122\n",
      "Iteration = 2410, accuracy = 78.79913339523368, loss = 0.46007772212351167\n",
      "Iteration = 2420, accuracy = 78.79655421438152, loss = 0.46003461587140454\n",
      "Iteration = 2430, accuracy = 78.8107397090684, loss = 0.45999225649734765\n",
      "Iteration = 2440, accuracy = 78.80687093779017, loss = 0.45995062938026987\n",
      "Iteration = 2450, accuracy = 78.81202929949448, loss = 0.45990972021297377\n",
      "Iteration = 2460, accuracy = 78.81202929949448, loss = 0.4598695149947396\n",
      "Iteration = 2470, accuracy = 78.82105643247705, loss = 0.459830000024123\n",
      "Iteration = 2480, accuracy = 78.82234602290312, loss = 0.45979116189194363\n",
      "Iteration = 2490, accuracy = 78.8300835654596, loss = 0.4597529874744567\n",
      "Iteration = 2500, accuracy = 78.83653151759002, loss = 0.45971546392670337\n",
      "Iteration = 2510, accuracy = 78.83911069844217, loss = 0.45967857867603407\n",
      "Iteration = 2520, accuracy = 78.83266274631178, loss = 0.4596423194157994\n",
      "Iteration = 2530, accuracy = 78.83266274631178, loss = 0.45960667409920497\n",
      "Iteration = 2540, accuracy = 78.83653151759002, loss = 0.4595716309333229\n",
      "Iteration = 2550, accuracy = 78.84040028886825, loss = 0.45953717837325836\n",
      "Iteration = 2560, accuracy = 78.83653151759002, loss = 0.45950330511646453\n",
      "Iteration = 2570, accuracy = 78.83782110801609, loss = 0.45947000009720307\n",
      "Iteration = 2580, accuracy = 78.8442690601465, loss = 0.4594372524811438\n",
      "Iteration = 2590, accuracy = 78.83524192716393, loss = 0.4594050516601021\n",
      "Iteration = 2600, accuracy = 78.84297946972042, loss = 0.4593733872469087\n",
      "Iteration = 2610, accuracy = 78.84684824099865, loss = 0.4593422490704082\n",
      "Iteration = 2620, accuracy = 78.85329619312907, loss = 0.45931162717058305\n",
      "Iteration = 2630, accuracy = 78.85587537398122, loss = 0.45928151179379795\n",
      "Iteration = 2640, accuracy = 78.8571649644073, loss = 0.45925189338816474\n",
      "Iteration = 2650, accuracy = 78.86619209738987, loss = 0.4592227625990205\n",
      "Iteration = 2660, accuracy = 78.85845455483339, loss = 0.4591941102645164\n",
      "Iteration = 2670, accuracy = 78.8571649644073, loss = 0.45916592741131834\n",
      "Iteration = 2680, accuracy = 78.86232332611162, loss = 0.45913820525040955\n",
      "Iteration = 2690, accuracy = 78.8636129165377, loss = 0.4591109351729979\n",
      "Iteration = 2700, accuracy = 78.86619209738987, loss = 0.4590841087465226\n",
      "Iteration = 2710, accuracy = 78.86232332611162, loss = 0.4590577177107578\n",
      "Iteration = 2720, accuracy = 78.8636129165377, loss = 0.4590317539740098\n",
      "Iteration = 2730, accuracy = 78.86619209738987, loss = 0.4590062096094073\n",
      "Iteration = 2740, accuracy = 78.8636129165377, loss = 0.4589810768512807\n",
      "Iteration = 2750, accuracy = 78.86232332611162, loss = 0.45895634809162705\n",
      "Iteration = 2760, accuracy = 78.86232332611162, loss = 0.4589320158766618\n",
      "Iteration = 2770, accuracy = 78.85845455483339, loss = 0.45890807290345076\n",
      "Iteration = 2780, accuracy = 78.87006086866812, loss = 0.45888451201662234\n",
      "Iteration = 2790, accuracy = 78.87521923037242, loss = 0.45886132620515974\n",
      "Iteration = 2800, accuracy = 78.88037759207675, loss = 0.45883850859926617\n",
      "Iteration = 2810, accuracy = 78.88166718250284, loss = 0.45881605246730656\n",
      "Iteration = 2820, accuracy = 78.87521923037242, loss = 0.45879395121281896\n",
      "Iteration = 2830, accuracy = 78.87392963994635, loss = 0.45877219837159794\n",
      "Iteration = 2840, accuracy = 78.8777984112246, loss = 0.45875078760884447\n",
      "Iteration = 2850, accuracy = 78.87908800165067, loss = 0.4587297127163833\n",
      "Iteration = 2860, accuracy = 78.88553595378109, loss = 0.45870896760994506\n",
      "Iteration = 2870, accuracy = 78.88682554420716, loss = 0.45868854632651124\n",
      "Iteration = 2880, accuracy = 78.88166718250284, loss = 0.4586684430217198\n",
      "Iteration = 2890, accuracy = 78.88166718250284, loss = 0.45864865196733223\n",
      "Iteration = 2900, accuracy = 78.88037759207675, loss = 0.45862916754875743\n",
      "Iteration = 2910, accuracy = 78.88682554420716, loss = 0.4586099842626327\n",
      "Iteration = 2920, accuracy = 78.89456308676365, loss = 0.4585910967144618\n",
      "Iteration = 2930, accuracy = 78.89585267718972, loss = 0.45857249961630364\n",
      "Iteration = 2940, accuracy = 78.90745899102444, loss = 0.4585541877845178\n",
      "Iteration = 2950, accuracy = 78.91132776230269, loss = 0.4585361561375575\n",
      "Iteration = 2960, accuracy = 78.91777571443309, loss = 0.4585183996938153\n",
      "Iteration = 2970, accuracy = 78.92680284741566, loss = 0.45850091356951583\n",
      "Iteration = 2980, accuracy = 78.91777571443309, loss = 0.4584836929766575\n",
      "Iteration = 2990, accuracy = 78.92422366656349, loss = 0.4584667332209992\n"
     ]
    }
   ],
   "source": [
    "y_pred_one, weights_one, loss_one = train_predict(X_train_one, y_train_one, X_test_one, max_iters=max_iters, degree=degree, lambda_=lambda_, imputable_th=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 0, accuracy = 44.75276732420771, loss = 7.463962918553235\n",
      "Iteration = 10, accuracy = 44.75276732420771, loss = 6.76497306533056\n",
      "Iteration = 20, accuracy = 44.763795266255876, loss = 5.999796782206942\n",
      "Iteration = 30, accuracy = 44.863046744689356, loss = 5.181476801591986\n",
      "Iteration = 40, accuracy = 45.31932784693217, loss = 4.3375341144432955\n",
      "Iteration = 50, accuracy = 46.67300773334436, loss = 3.5238261422359143\n",
      "Iteration = 60, accuracy = 48.61806101208938, loss = 2.8127262234623656\n",
      "Iteration = 70, accuracy = 50.83192037825841, loss = 2.2469706893787076\n",
      "Iteration = 80, accuracy = 52.936878816701814, loss = 1.8293049709843736\n",
      "Iteration = 90, accuracy = 54.93982879119971, loss = 1.5384764890333331\n",
      "Iteration = 100, accuracy = 56.778738127731145, loss = 1.341011821729397\n",
      "Iteration = 110, accuracy = 58.30610810140193, loss = 1.205505013260816\n",
      "Iteration = 120, accuracy = 59.67495140813035, loss = 1.1095992005247624\n",
      "Iteration = 130, accuracy = 60.76809616365466, loss = 1.0392104453731303\n",
      "Iteration = 140, accuracy = 61.753718484209365, loss = 0.9852179729829903\n",
      "Iteration = 150, accuracy = 62.580814137821704, loss = 0.941940359430649\n",
      "Iteration = 160, accuracy = 63.308658313000564, loss = 0.9056217795500174\n",
      "Iteration = 170, accuracy = 63.996526198254834, loss = 0.8741831888636844\n",
      "Iteration = 180, accuracy = 64.56584370649132, loss = 0.8464917952496698\n",
      "Iteration = 190, accuracy = 65.12826875094771, loss = 0.8219371656747655\n",
      "Iteration = 200, accuracy = 65.66450243303971, loss = 0.8000092387798066\n",
      "Iteration = 210, accuracy = 66.11251257874639, loss = 0.7803882634008614\n",
      "Iteration = 220, accuracy = 66.5219249272845, loss = 0.7626110402814655\n",
      "Iteration = 230, accuracy = 66.97544904401528, loss = 0.7463697996952681\n",
      "Iteration = 240, accuracy = 67.36004852294502, loss = 0.7314027076887989\n",
      "Iteration = 250, accuracy = 67.75705443667893, loss = 0.7175375317108856\n",
      "Iteration = 260, accuracy = 68.14578939387673, loss = 0.7046442051314178\n",
      "Iteration = 270, accuracy = 68.44905780020126, loss = 0.6926125831114418\n",
      "Iteration = 280, accuracy = 68.7867885254263, loss = 0.6813499989628299\n",
      "Iteration = 290, accuracy = 69.08729994623877, loss = 0.6707773303182801\n",
      "Iteration = 300, accuracy = 69.44432957004811, loss = 0.660828379240589\n",
      "Iteration = 310, accuracy = 69.74621948361661, loss = 0.6514470178290174\n",
      "Iteration = 320, accuracy = 70.00399762899247, loss = 0.642584226981309\n",
      "Iteration = 330, accuracy = 70.27280371641646, loss = 0.6341966103045641\n",
      "Iteration = 340, accuracy = 70.51403994872007, loss = 0.6262459302852748\n",
      "Iteration = 350, accuracy = 70.81317287677653, loss = 0.6186986140748497\n",
      "Iteration = 360, accuracy = 71.06681554388432, loss = 0.6115251156519939\n",
      "Iteration = 370, accuracy = 71.31218725445598, loss = 0.6046993233057161\n",
      "Iteration = 380, accuracy = 71.57961484912396, loss = 0.598198046923367\n",
      "Iteration = 390, accuracy = 71.84704244379195, loss = 0.5920005681492682\n",
      "Iteration = 400, accuracy = 72.10068511089975, loss = 0.586088253590605\n",
      "Iteration = 410, accuracy = 72.32675792288713, loss = 0.5804442341727757\n",
      "Iteration = 420, accuracy = 72.50182650290172, loss = 0.5750531456424457\n",
      "Iteration = 430, accuracy = 72.69068001047656, loss = 0.569900919803475\n",
      "Iteration = 440, accuracy = 72.89745392387962, loss = 0.5649746154100113\n",
      "Iteration = 450, accuracy = 73.08079346043036, loss = 0.5602612868431708\n",
      "Iteration = 460, accuracy = 73.26137601146905, loss = 0.5557479544417709\n",
      "Iteration = 470, accuracy = 73.4378230842397, loss = 0.5514274944297388\n",
      "Iteration = 480, accuracy = 73.59083578015797, loss = 0.5472902788048571\n",
      "Iteration = 490, accuracy = 73.77693230222076, loss = 0.5433273026836779\n",
      "Iteration = 500, accuracy = 73.98232772286781, loss = 0.5395301233084554\n",
      "Iteration = 510, accuracy = 74.128447955006, loss = 0.5358891148312418\n",
      "Iteration = 520, accuracy = 74.29111010021643, loss = 0.5323984976186446\n",
      "Iteration = 530, accuracy = 74.4441227961347, loss = 0.5290512944997996\n",
      "Iteration = 540, accuracy = 74.60678494134513, loss = 0.5258408456333528\n",
      "Iteration = 550, accuracy = 74.80115241994403, loss = 0.5227608500743389\n",
      "Iteration = 560, accuracy = 74.96795004342253, loss = 0.5198053388887702\n",
      "Iteration = 570, accuracy = 75.11544876831671, loss = 0.5169686513510483\n",
      "Iteration = 580, accuracy = 75.25329804391878, loss = 0.5142454137698542\n",
      "Iteration = 590, accuracy = 75.38839033400879, loss = 0.5116305205609788\n",
      "Iteration = 600, accuracy = 75.53037508787891, loss = 0.5091191172469645\n",
      "Iteration = 610, accuracy = 75.61584163875219, loss = 0.5067065851149504\n",
      "Iteration = 620, accuracy = 75.76885433467048, loss = 0.5043885273070753\n",
      "Iteration = 630, accuracy = 75.91359607405263, loss = 0.5021607561535781\n",
      "Iteration = 640, accuracy = 76.00181961043795, loss = 0.5000192815883372\n",
      "Iteration = 650, accuracy = 76.12450546572379, loss = 0.49795959377065696\n",
      "Iteration = 660, accuracy = 76.22789242242533, loss = 0.4959787860245703\n",
      "Iteration = 670, accuracy = 76.32852239361482, loss = 0.49407341665600024\n",
      "Iteration = 680, accuracy = 76.43466633582841, loss = 0.4922401863944908\n",
      "Iteration = 690, accuracy = 76.55045972733413, loss = 0.49047594781838066\n",
      "Iteration = 700, accuracy = 76.66625311883986, loss = 0.48877769801360527\n",
      "Iteration = 710, accuracy = 76.7820465103456, loss = 0.48714257168910596\n",
      "Iteration = 720, accuracy = 76.87854100326703, loss = 0.48556783470101605\n",
      "Iteration = 730, accuracy = 76.97779248170052, loss = 0.48405087794409946\n",
      "Iteration = 740, accuracy = 77.08255793115806, loss = 0.48258921157449297\n",
      "Iteration = 750, accuracy = 77.17078146754339, loss = 0.4811804595327577\n",
      "Iteration = 760, accuracy = 77.23694911983237, loss = 0.4798223543406435\n",
      "Iteration = 770, accuracy = 77.34171456928995, loss = 0.4785127321488596\n",
      "Iteration = 780, accuracy = 77.4409660477234, loss = 0.4772495280165314\n",
      "Iteration = 790, accuracy = 77.50851219276844, loss = 0.47603077140594857\n",
      "Iteration = 800, accuracy = 77.59949271466579, loss = 0.474854581878671\n",
      "Iteration = 810, accuracy = 77.70563665687938, loss = 0.4737191649811224\n",
      "Iteration = 820, accuracy = 77.76491184538826, loss = 0.47262280830947523\n",
      "Iteration = 830, accuracy = 77.8476214107495, loss = 0.4715638777449792\n",
      "Iteration = 840, accuracy = 77.89311167169816, loss = 0.47054081385194263\n",
      "Iteration = 850, accuracy = 77.94411590367093, loss = 0.4695521284314027\n",
      "Iteration = 860, accuracy = 78.02268999076409, loss = 0.46859640122415325\n",
      "Iteration = 870, accuracy = 78.07920819376093, loss = 0.4676722767572754\n",
      "Iteration = 880, accuracy = 78.15088981707402, loss = 0.46677846132868295\n",
      "Iteration = 890, accuracy = 78.217057469363, loss = 0.46591372012446947\n",
      "Iteration = 900, accuracy = 78.28598210716403, loss = 0.4650768744640645\n",
      "Iteration = 910, accuracy = 78.35352825220903, loss = 0.4642667991683725\n",
      "Iteration = 920, accuracy = 78.39626152764568, loss = 0.46348242004622003\n",
      "Iteration = 930, accuracy = 78.44588726686241, loss = 0.4627227114945517\n",
      "Iteration = 940, accuracy = 78.5051624553713, loss = 0.46198669420793925\n",
      "Iteration = 950, accuracy = 78.58787202073253, loss = 0.46127343299307944\n",
      "Iteration = 960, accuracy = 78.63611926719325, loss = 0.4605820346840538\n",
      "Iteration = 970, accuracy = 78.67333857160581, loss = 0.4599116461542461\n",
      "Iteration = 980, accuracy = 78.72158581806653, loss = 0.45926145242090466\n",
      "Iteration = 990, accuracy = 78.74915567318693, loss = 0.45863067483848086\n",
      "Iteration = 1000, accuracy = 78.80980935445184, loss = 0.4580185693769463\n",
      "Iteration = 1010, accuracy = 78.86632755744868, loss = 0.457424424981443\n",
      "Iteration = 1020, accuracy = 78.90492535461726, loss = 0.4568475620097167\n",
      "Iteration = 1030, accuracy = 78.9476586300539, loss = 0.45628733074391103\n",
      "Iteration = 1040, accuracy = 79.00004135478268, loss = 0.4557431099734117\n",
      "Iteration = 1050, accuracy = 79.03588216643922, loss = 0.45521430564555165\n",
      "Iteration = 1060, accuracy = 79.07172297809575, loss = 0.45470034958109906\n",
      "Iteration = 1070, accuracy = 79.10480680424024, loss = 0.4542006982515729\n",
      "Iteration = 1080, accuracy = 79.15305405070097, loss = 0.4537148316155369\n",
      "Iteration = 1090, accuracy = 79.1820023985774, loss = 0.45324182590871653\n",
      "Iteration = 1100, accuracy = 79.20267978991771, loss = 0.4527806510180545\n",
      "Iteration = 1110, accuracy = 79.2288711522821, loss = 0.4523318252148508\n",
      "Iteration = 1120, accuracy = 79.2757399059868, loss = 0.4518949125013541\n",
      "Iteration = 1130, accuracy = 79.3102022248873, loss = 0.45146949531415004\n",
      "Iteration = 1140, accuracy = 79.34604303654385, loss = 0.45105517366180836\n",
      "Iteration = 1150, accuracy = 79.38050535544436, loss = 0.450651564304516\n",
      "Iteration = 1160, accuracy = 79.42186013812498, loss = 0.45025829997368466\n",
      "Iteration = 1170, accuracy = 79.44942999324539, loss = 0.4498750286295984\n",
      "Iteration = 1180, accuracy = 79.4838923121459, loss = 0.44950141275525657\n",
      "Iteration = 1190, accuracy = 79.51835463104642, loss = 0.4491371286846546\n",
      "Iteration = 1200, accuracy = 79.55557393545897, loss = 0.4487818659638197\n",
      "Iteration = 1210, accuracy = 79.57211584853121, loss = 0.44843532674299974\n",
      "Iteration = 1220, accuracy = 79.60382118191968, loss = 0.4480972251984764\n",
      "Iteration = 1230, accuracy = 79.63828350082021, loss = 0.44776728698254653\n",
      "Iteration = 1240, accuracy = 79.66033938491654, loss = 0.44744524870028457\n",
      "Iteration = 1250, accuracy = 79.690666225549, loss = 0.44713085741176284\n",
      "Iteration = 1260, accuracy = 79.71410060240134, loss = 0.4468238701584717\n",
      "Iteration = 1270, accuracy = 79.75545538508196, loss = 0.44652405351274144\n",
      "Iteration = 1280, accuracy = 79.78164674744635, loss = 0.44623118314902543\n",
      "Iteration = 1290, accuracy = 79.8119735880788, loss = 0.4459450434359591\n",
      "Iteration = 1300, accuracy = 79.8519498780034, loss = 0.445665427048163\n",
      "Iteration = 1310, accuracy = 79.87538425485575, loss = 0.4453921345968084\n",
      "Iteration = 1320, accuracy = 79.91122506651227, loss = 0.44512497427801173\n",
      "Iteration = 1330, accuracy = 79.93052396509657, loss = 0.44486376153817025\n",
      "Iteration = 1340, accuracy = 79.9525798491929, loss = 0.44460831875539497\n",
      "Iteration = 1350, accuracy = 79.98842066084943, loss = 0.4443584749362399\n",
      "Iteration = 1360, accuracy = 80.01185503770178, loss = 0.4441140654269646\n",
      "Iteration = 1370, accuracy = 80.01874750148188, loss = 0.44387493163860475\n",
      "Iteration = 1380, accuracy = 80.05320982038239, loss = 0.44364092078516554\n",
      "Iteration = 1390, accuracy = 80.07802268999077, loss = 0.44341188563428\n",
      "Iteration = 1400, accuracy = 80.10972802337923, loss = 0.44318768426971383\n",
      "Iteration = 1410, accuracy = 80.13316240023158, loss = 0.4429681798651247\n",
      "Iteration = 1420, accuracy = 80.13867637125567, loss = 0.44275324046851844\n",
      "Iteration = 1430, accuracy = 80.17038170464414, loss = 0.4425427387968569\n",
      "Iteration = 1440, accuracy = 80.1731386901562, loss = 0.4423365520403354\n",
      "Iteration = 1450, accuracy = 80.18554512496037, loss = 0.44213456167582627\n",
      "Iteration = 1460, accuracy = 80.21862895110486, loss = 0.4419366532890426\n",
      "Iteration = 1470, accuracy = 80.2406848352012, loss = 0.4417427164049854\n",
      "Iteration = 1480, accuracy = 80.24895579173732, loss = 0.4415526443262654\n",
      "Iteration = 1490, accuracy = 80.26274071929753, loss = 0.44136633397890496\n",
      "Iteration = 1500, accuracy = 80.27928263236976, loss = 0.4411836857652523\n",
      "Iteration = 1510, accuracy = 80.28479660339384, loss = 0.4410046034236506\n",
      "Iteration = 1520, accuracy = 80.3151234440263, loss = 0.44082899389453556\n",
      "Iteration = 1530, accuracy = 80.31788042953835, loss = 0.44065676719263086\n",
      "Iteration = 1540, accuracy = 80.32890837158651, loss = 0.4404878362849534\n",
      "Iteration = 1550, accuracy = 80.31925892229437, loss = 0.4403221169743352\n",
      "Iteration = 1560, accuracy = 80.31788042953835, loss = 0.44015952778818995\n",
      "Iteration = 1570, accuracy = 80.33166535709854, loss = 0.4399999898722673\n",
      "Iteration = 1580, accuracy = 80.33028686434253, loss = 0.4398434268891517\n",
      "Iteration = 1590, accuracy = 80.33580083536661, loss = 0.43968976492127376\n",
      "Iteration = 1600, accuracy = 80.35096425568284, loss = 0.43953893237820446\n",
      "Iteration = 1610, accuracy = 80.3550997339509, loss = 0.4393908599080417\n",
      "Iteration = 1620, accuracy = 80.36612767599907, loss = 0.43924548031267574\n",
      "Iteration = 1630, accuracy = 80.3812910963153, loss = 0.43910272846674986\n",
      "Iteration = 1640, accuracy = 80.3812910963153, loss = 0.4389625412401393\n",
      "Iteration = 1650, accuracy = 80.39231903836345, loss = 0.43882485742377686\n",
      "Iteration = 1660, accuracy = 80.4088609514357, loss = 0.43868961765866993\n",
      "Iteration = 1670, accuracy = 80.41299642970377, loss = 0.4385567643679415\n",
      "Iteration = 1680, accuracy = 80.42264587899591, loss = 0.43842624169177524\n",
      "Iteration = 1690, accuracy = 80.44470176309223, loss = 0.4382979954251076\n",
      "Iteration = 1700, accuracy = 80.45710819789642, loss = 0.4381719729579438\n",
      "Iteration = 1710, accuracy = 80.46537915443255, loss = 0.43804812321817393\n",
      "Iteration = 1720, accuracy = 80.48329956026082, loss = 0.4379263966167723\n",
      "Iteration = 1730, accuracy = 80.49019202404092, loss = 0.43780674499526884\n",
      "Iteration = 1740, accuracy = 80.4888135312849, loss = 0.4376891215753865\n",
      "Iteration = 1750, accuracy = 80.49708448782101, loss = 0.4375734809107457\n",
      "Iteration = 1760, accuracy = 80.50535544435714, loss = 0.43745977884053905\n",
      "Iteration = 1770, accuracy = 80.51500489364929, loss = 0.4373479724450878\n",
      "Iteration = 1780, accuracy = 80.51224790813724, loss = 0.43723802000319306\n",
      "Iteration = 1790, accuracy = 80.53568228498959, loss = 0.4371298809512035\n",
      "Iteration = 1800, accuracy = 80.53154680672154, loss = 0.43702351584371213\n",
      "Iteration = 1810, accuracy = 80.52603283569745, loss = 0.4369188863158228\n",
      "Iteration = 1820, accuracy = 80.51776187916133, loss = 0.4368159550469094\n",
      "Iteration = 1830, accuracy = 80.52603283569745, loss = 0.43671468572579897\n",
      "Iteration = 1840, accuracy = 80.51776187916133, loss = 0.4366150430173186\n",
      "Iteration = 1850, accuracy = 80.5301683139655, loss = 0.43651699253014664\n",
      "Iteration = 1860, accuracy = 80.53568228498959, loss = 0.4364205007859082\n",
      "Iteration = 1870, accuracy = 80.54395324152571, loss = 0.4363255351894644\n",
      "Iteration = 1880, accuracy = 80.54808871979378, loss = 0.4362320640003416\n",
      "Iteration = 1890, accuracy = 80.5770370676702, loss = 0.4361400563052418\n",
      "Iteration = 1900, accuracy = 80.60184993727859, loss = 0.4360494819916119\n",
      "Iteration = 1910, accuracy = 80.61563486483878, loss = 0.43596031172220173\n",
      "Iteration = 1920, accuracy = 80.62804129964297, loss = 0.4358725169105798\n",
      "Iteration = 1930, accuracy = 80.63906924169113, loss = 0.435786069697564\n",
      "Iteration = 1940, accuracy = 80.64182622720317, loss = 0.4357009429285287\n",
      "Iteration = 1950, accuracy = 80.63769074893511, loss = 0.4356171101315541\n",
      "Iteration = 1960, accuracy = 80.64871869098327, loss = 0.4355345454963772\n",
      "Iteration = 1970, accuracy = 80.64871869098327, loss = 0.4354532238541148\n",
      "Iteration = 1980, accuracy = 80.65285416925134, loss = 0.43537312065772416\n",
      "Iteration = 1990, accuracy = 80.66526060405552, loss = 0.43529421196317325\n",
      "Iteration = 2000, accuracy = 80.67215306783562, loss = 0.43521647441128825\n",
      "Iteration = 2010, accuracy = 80.67904553161573, loss = 0.4351398852102535\n",
      "Iteration = 2020, accuracy = 80.68593799539583, loss = 0.43506442211873514\n",
      "Iteration = 2030, accuracy = 80.69145196641992, loss = 0.43499006342960395\n",
      "Iteration = 2040, accuracy = 80.69145196641992, loss = 0.43491678795423283\n",
      "Iteration = 2050, accuracy = 80.69283045917594, loss = 0.43484457500734525\n",
      "Iteration = 2060, accuracy = 80.70110141571206, loss = 0.43477340439239426\n",
      "Iteration = 2070, accuracy = 80.70661538673615, loss = 0.434703256387449\n",
      "Iteration = 2080, accuracy = 80.7107508650042, loss = 0.4346341117315668\n",
      "Iteration = 2090, accuracy = 80.72315729980839, loss = 0.4345659516116379\n",
      "Iteration = 2100, accuracy = 80.72453579256441, loss = 0.43449875764967766\n",
      "Iteration = 2110, accuracy = 80.7300497635885, loss = 0.4344325118905512\n",
      "Iteration = 2120, accuracy = 80.72729277807646, loss = 0.43436719679011415\n",
      "Iteration = 2130, accuracy = 80.7300497635885, loss = 0.4343027952037512\n",
      "Iteration = 2140, accuracy = 80.74245619839267, loss = 0.4342392903752992\n",
      "Iteration = 2150, accuracy = 80.73418524185655, loss = 0.4341766659263393\n",
      "Iteration = 2160, accuracy = 80.73832072012462, loss = 0.4341149058458422\n",
      "Iteration = 2170, accuracy = 80.73969921288064, loss = 0.4340539944801565\n",
      "Iteration = 2180, accuracy = 80.74383469114869, loss = 0.4339939165233222\n",
      "Iteration = 2190, accuracy = 80.74797016941676, loss = 0.4339345900970558\n",
      "Iteration = 2200, accuracy = 80.74659167666074, loss = 0.4338760531833187\n",
      "Iteration = 2210, accuracy = 80.75486263319686, loss = 0.4338183065875862\n",
      "Iteration = 2220, accuracy = 80.75210564768483, loss = 0.43376133629750274\n",
      "Iteration = 2230, accuracy = 80.75486263319686, loss = 0.4337051286044859\n",
      "Iteration = 2240, accuracy = 80.75899811146492, loss = 0.4336496700957674\n",
      "Iteration = 2250, accuracy = 80.75899811146492, loss = 0.43359494764668316\n",
      "Iteration = 2260, accuracy = 80.764512082489, loss = 0.4335409484132061\n",
      "Iteration = 2270, accuracy = 80.77278303902513, loss = 0.4334876598247115\n",
      "Iteration = 2280, accuracy = 80.77554002453718, loss = 0.4334350695769661\n",
      "Iteration = 2290, accuracy = 80.77140454626911, loss = 0.433383165625334\n",
      "Iteration = 2300, accuracy = 80.76726906800104, loss = 0.4333319361781898\n",
      "Iteration = 2310, accuracy = 80.76726906800104, loss = 0.43328136969053677\n",
      "Iteration = 2320, accuracy = 80.77140454626911, loss = 0.4332314548578061\n",
      "Iteration = 2330, accuracy = 80.78656796658534, loss = 0.4331821806098603\n",
      "Iteration = 2340, accuracy = 80.79483892312146, loss = 0.4331335361051644\n",
      "Iteration = 2350, accuracy = 80.79621741587748, loss = 0.43308551072513535\n",
      "Iteration = 2360, accuracy = 80.80173138690157, loss = 0.4330380940686578\n",
      "Iteration = 2370, accuracy = 80.79897440138953, loss = 0.4329912759467619\n",
      "Iteration = 2380, accuracy = 80.80586686516963, loss = 0.4329450463774576\n",
      "Iteration = 2390, accuracy = 80.80724535792564, loss = 0.4328993955807188\n",
      "Iteration = 2400, accuracy = 80.81689480721779, loss = 0.4328543139736141\n",
      "Iteration = 2410, accuracy = 80.81689480721779, loss = 0.4328097921655765\n",
      "Iteration = 2420, accuracy = 80.82103028548585, loss = 0.43276582095381083\n",
      "Iteration = 2430, accuracy = 80.82930124202198, loss = 0.43272239131883083\n",
      "Iteration = 2440, accuracy = 80.83343672029002, loss = 0.4326794944201232\n",
      "Iteration = 2450, accuracy = 80.83757219855809, loss = 0.43263712159193435\n",
      "Iteration = 2460, accuracy = 80.83895069131411, loss = 0.43259526433917544\n",
      "Iteration = 2470, accuracy = 80.84170767682616, loss = 0.432553914333442\n",
      "Iteration = 2480, accuracy = 80.84584315509422, loss = 0.43251306340914325\n",
      "Iteration = 2490, accuracy = 80.84584315509422, loss = 0.43247270355974\n",
      "Iteration = 2500, accuracy = 80.84997863336228, loss = 0.4324328269340843\n",
      "Iteration = 2510, accuracy = 80.8513571261183, loss = 0.4323934258328605\n",
      "Iteration = 2520, accuracy = 80.85687109714237, loss = 0.4323544927051222\n",
      "Iteration = 2530, accuracy = 80.86100657541044, loss = 0.4323160201449244\n",
      "Iteration = 2540, accuracy = 80.86514205367851, loss = 0.43227800088804397\n",
      "Iteration = 2550, accuracy = 80.86652054643453, loss = 0.43224042780879185\n",
      "Iteration = 2560, accuracy = 80.86927753194657, loss = 0.4322032939169043\n",
      "Iteration = 2570, accuracy = 80.87616999572667, loss = 0.43216659235452165\n",
      "Iteration = 2580, accuracy = 80.87203451745862, loss = 0.43213031639324545\n",
      "Iteration = 2590, accuracy = 80.87616999572667, loss = 0.43209445943127317\n",
      "Iteration = 2600, accuracy = 80.88168396675076, loss = 0.4320590149906062\n",
      "Iteration = 2610, accuracy = 80.88306245950677, loss = 0.4320239767143318\n",
      "Iteration = 2620, accuracy = 80.88306245950677, loss = 0.43198933836397496\n",
      "Iteration = 2630, accuracy = 80.88168396675076, loss = 0.43195509381691866\n",
      "Iteration = 2640, accuracy = 80.88719793777483, loss = 0.43192123706388974\n",
      "Iteration = 2650, accuracy = 80.8913334160429, loss = 0.4318877622065091\n",
      "Iteration = 2660, accuracy = 80.8913334160429, loss = 0.4318546634549046\n",
      "Iteration = 2670, accuracy = 80.8913334160429, loss = 0.43182193512538264\n",
      "Iteration = 2680, accuracy = 80.88719793777483, loss = 0.4317895716381612\n",
      "Iteration = 2690, accuracy = 80.88995492328688, loss = 0.43175756751515515\n",
      "Iteration = 2700, accuracy = 80.88719793777483, loss = 0.43172591737782234\n",
      "Iteration = 2710, accuracy = 80.8913334160429, loss = 0.43169461594505704\n",
      "Iteration = 2720, accuracy = 80.89684738706698, loss = 0.43166365803114054\n",
      "Iteration = 2730, accuracy = 80.898225879823, loss = 0.4316330385437386\n",
      "Iteration = 2740, accuracy = 80.90098286533505, loss = 0.43160275248194957\n",
      "Iteration = 2750, accuracy = 80.90787532911514, loss = 0.43157279493439993\n",
      "Iteration = 2760, accuracy = 80.90925382187116, loss = 0.4315431610773846\n",
      "Iteration = 2770, accuracy = 80.91614628565127, loss = 0.43151384617305405\n",
      "Iteration = 2780, accuracy = 80.91476779289525, loss = 0.43148484556764394\n",
      "Iteration = 2790, accuracy = 80.91476779289525, loss = 0.4314561546897464\n",
      "Iteration = 2800, accuracy = 80.91752477840728, loss = 0.43142776904862434\n",
      "Iteration = 2810, accuracy = 80.92028176391933, loss = 0.4313996842325638\n",
      "Iteration = 2820, accuracy = 80.91338930013923, loss = 0.4313718959072663\n",
      "Iteration = 2830, accuracy = 80.92303874943137, loss = 0.43134439981428024\n",
      "Iteration = 2840, accuracy = 80.92993121321147, loss = 0.4313171917694656\n",
      "Iteration = 2850, accuracy = 80.9313097059675, loss = 0.43129026766149503\n",
      "Iteration = 2860, accuracy = 80.93406669147953, loss = 0.4312636234503946\n",
      "Iteration = 2870, accuracy = 80.92993121321147, loss = 0.4312372551661129\n",
      "Iteration = 2880, accuracy = 80.93682367699158, loss = 0.43121115890712536\n",
      "Iteration = 2890, accuracy = 80.93544518423556, loss = 0.43118533083907085\n",
      "Iteration = 2900, accuracy = 80.93406669147953, loss = 0.4311597671934178\n",
      "Iteration = 2910, accuracy = 80.94095915525963, loss = 0.4311344642661628\n",
      "Iteration = 2920, accuracy = 80.94095915525963, loss = 0.431109418416557\n",
      "Iteration = 2930, accuracy = 80.93682367699158, loss = 0.4310846260658621\n",
      "Iteration = 2940, accuracy = 80.92993121321147, loss = 0.4310600836961348\n",
      "Iteration = 2950, accuracy = 80.9313097059675, loss = 0.43103578784903734\n",
      "Iteration = 2960, accuracy = 80.93682367699158, loss = 0.4310117351246755\n",
      "Iteration = 2970, accuracy = 80.93544518423556, loss = 0.43098792218046206\n",
      "Iteration = 2980, accuracy = 80.93268819872351, loss = 0.43096434573000586\n",
      "Iteration = 2990, accuracy = 80.93406669147953, loss = 0.43094100254202516\n"
     ]
    }
   ],
   "source": [
    "y_pred_many, weights_many, loss_many = train_predict(X_train_many, y_train_many, X_test_many, max_iters=max_iters, degree=degree, lambda_=lambda_, imputable_th=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.vstack([y_pred_zero, y_pred_one, y_pred_many])\n",
    "ids_test = np.hstack([ids_test_zero, ids_test_one, ids_test_many])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "method = 'reg_logistic_regression_by_jet'\n",
    "time = datetime.now().strftime('%Y%m%dH%H%M%S')\n",
    "OUTPUT_PATH = f'../submissions/submission_{method}_{time}'\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lambda_': np.logspace(-4, 0, 5),\n",
    "    'degree': list(range(1, 4)),\n",
    "    'max_iters': 100,\n",
    "    'gamma': [0.01, 0.05, 0.1],\n",
    "    'cont_features': [cont_features]\n",
    "}\n",
    "metrics, params = logistic_regression_cv(ty_train, tX_train, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.4160482322829052,\n",
       " 'accuracy': 81.97132616487455,\n",
       " 'f1_score': 0.7979288473644017}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lambda_': 0.001,\n",
       " 'degree': 3,\n",
       " 'gamma': 0.1,\n",
       " 'max_iters': 1000,\n",
       " 'cont_features': (1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train, ty_train, tX_test, _, cont_features = preprocess(X_train_many, y_train_many, X_test_many, imputable_th=0, encodable_th=1, switch_encoding=True)\n",
    "tX_train_poly = build_poly(tX_train, degree=degree, cont_features=cont_features)\n",
    "ty_train_pred = predict_logistic(weights_many, tX_train_poly)\n",
    "train_accuracy = compute_accuracy(ty_train, ty_train_pred)\n",
    "train_f1 = compute_f1(ty_train, ty_train_pred)\n",
    "train_accuracy, train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = read_feature_names(DATA_TRAIN_PATH)\n",
    "one_jet_features = [f for f in features if f not in ['PRI_jet_num', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet',\n",
    "                                                     'DER_lep_eta_centrality', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = read_feature_names(DATA_TRAIN_PATH)\n",
    "many_jet_features = [f for f in features if f not in ['PRI_jet_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_distrib(tX):\n",
    "    plt.figure(figsize=(15,30))\n",
    "    for i in range(tX.shape[1]):\n",
    "        plt.subplot(10,3,i+1)\n",
    "        plt.hist(tX[:,i], bins=50)\n",
    "        plt.yscale('log')\n",
    "        plt.title(\"Feature {}: {}\".format(i, many_jet_features[i]))\n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "plot_features_distrib(X_train_many[(ty_train_pred != ty_train).reshape((-1,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_val_poly = build_poly(tX_test, degree=params['degree'], cont_features=cont_features)\n",
    "ty_val_pred = predict_logistic(weights, tX_val_poly)\n",
    "val_accuracy = compute_accuracy(ty_test, ty_val_pred)\n",
    "val_f1 = compute_f1(ty_test, ty_val_pred)\n",
    "val_accuracy, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tmp = np.hstack([X_train_val, ty_val_pred, ty_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tmp_mis = val_tmp[val_tmp[:, -1] != val_tmp[:, -2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tval_tmp = replace_values(val_tmp_mis, -999, np.nan)\n",
    "col_nan_ratio = compute_nan_ratio(tval_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_nan_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_distrib(tX):\n",
    "    plt.figure(figsize=(15,30))\n",
    "    for i in range(tX.shape[1]):\n",
    "        plt.subplot(10,3,i+1)\n",
    "        plt.hist(tX[:,i], bins=50);\n",
    "        # plt.title(\"Feature {}: {}\".format(i, labels[i]))\n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "plot_features_distrib(tval_tmp[:, :-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_poly = build_poly(tX_test, params['degree'], cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "method = 'reg_logistic_regression_by_jet'\n",
    "time = datetime.now().strftime('%Y%m%dH%H%M%S')\n",
    "OUTPUT_PATH = f'../submissions/submission_{method}_{time}'\n",
    "# y_pred = predict_logistic(weights, tX_test_poly)\n",
    "# y_pred = replace_values(y_pred, from_val=0, to_val=-1)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34e9dc8c9cd4c2e3341692c7f5472da17e27c062a0f2ac63648b60e63867ef4a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
