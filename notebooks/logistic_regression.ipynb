{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y_train, X_train, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import preprocess\n",
    "tX_train, ty_train, tX_test, ty_test, cont_features = preprocess(X_train, y_train, X_test, imputable_th=0.3, encodable_min_th=0.3, encodable_max_th=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.06833197,  0.40768027, ...,  1.        ,\n",
       "         1.        ,  1.        ],\n",
       "       [ 1.        ,  0.55250482,  0.54013641, ...,  1.        ,\n",
       "         1.        ,  1.        ],\n",
       "       [ 1.        ,  3.19515553,  1.09655998, ...,  1.        ,\n",
       "         1.        ,  1.        ],\n",
       "       ...,\n",
       "       [ 1.        ,  0.31931645, -0.13086367, ...,  1.        ,\n",
       "         1.        ,  1.        ],\n",
       "       [ 1.        , -0.84532397, -0.30297338, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.        ,  0.66533608, -0.25352276, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 24), (568238, 24))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train.shape, tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=71.828\n",
      "Accuracy=70.63199999999999\n",
      "Accuracy=72.114\n",
      "Accuracy=71.172\n",
      "Accuracy=70.562\n",
      "Accuracy=75.90400000000001\n",
      "Accuracy=75.166\n",
      "Accuracy=77.28399999999999\n",
      "Accuracy=75.614\n",
      "Accuracy=77.064\n",
      "Accuracy=71.922\n",
      "Accuracy=73.678\n",
      "Accuracy=76.214\n",
      "Accuracy=75.42999999999999\n",
      "Accuracy=73.024\n",
      "Accuracy=71.846\n",
      "Accuracy=70.638\n",
      "Accuracy=72.134\n",
      "Accuracy=71.182\n",
      "Accuracy=70.594\n",
      "Accuracy=75.94999999999999\n",
      "Accuracy=75.21799999999999\n",
      "Accuracy=77.3\n",
      "Accuracy=75.664\n",
      "Accuracy=77.084\n",
      "Accuracy=71.93\n",
      "Accuracy=72.684\n",
      "Accuracy=76.25\n",
      "Accuracy=75.51400000000001\n",
      "Accuracy=74.274\n",
      "Accuracy=72.028\n",
      "Accuracy=70.842\n",
      "Accuracy=72.36200000000001\n",
      "Accuracy=71.398\n",
      "Accuracy=70.794\n",
      "Accuracy=76.356\n",
      "Accuracy=75.71799999999999\n",
      "Accuracy=77.718\n",
      "Accuracy=76.042\n",
      "Accuracy=77.248\n",
      "Accuracy=75.11\n",
      "Accuracy=73.91799999999999\n",
      "Accuracy=76.36800000000001\n",
      "Accuracy=75.77199999999999\n",
      "Accuracy=74.698\n",
      "Accuracy=71.72\n",
      "Accuracy=71.334\n",
      "Accuracy=71.8\n",
      "Accuracy=71.65\n",
      "Accuracy=71.296\n",
      "Accuracy=76.146\n",
      "Accuracy=75.952\n",
      "Accuracy=76.582\n",
      "Accuracy=75.83800000000001\n",
      "Accuracy=76.226\n",
      "Accuracy=71.164\n",
      "Accuracy=75.128\n",
      "Accuracy=74.478\n",
      "Accuracy=74.534\n",
      "Accuracy=74.262\n",
      "Accuracy=69.966\n",
      "Accuracy=69.868\n",
      "Accuracy=70.114\n",
      "Accuracy=70.072\n",
      "Accuracy=69.796\n",
      "Accuracy=68.738\n",
      "Accuracy=68.542\n",
      "Accuracy=68.382\n",
      "Accuracy=68.352\n",
      "Accuracy=68.22200000000001\n",
      "Accuracy=62.166\n",
      "Accuracy=65.364\n",
      "Accuracy=66.758\n",
      "Accuracy=62.514\n",
      "Accuracy=63.782000000000004\n"
     ]
    }
   ],
   "source": [
    "from implementations import logistic_regression_cv\n",
    "param_grid = {\n",
    "    'lambda_': np.logspace(-4, 0, 5),\n",
    "    'degree': list(range(1, 4)),\n",
    "    'max_iters': 100,\n",
    "    'cont_features': [cont_features]\n",
    "}\n",
    "weights, metrics, params = logistic_regression_cv(ty_train, tX_train, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lambda_': 0.01,\n",
       " 'degree': 2,\n",
       " 'max_iters': 100,\n",
       " 'cont_features': [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/1000\n",
      "Accuracy = 34.322399999999995%\n",
      "Loss = 5.817646820262799\n",
      "\n",
      "\n",
      "Iteration 10/1000\n",
      "Accuracy = 52.796%\n",
      "Loss = 1.562288105956384\n",
      "\n",
      "\n",
      "Iteration 20/1000\n",
      "Accuracy = 62.1908%\n",
      "Loss = 0.9133277554101196\n",
      "\n",
      "\n",
      "Iteration 30/1000\n",
      "Accuracy = 66.34%\n",
      "Loss = 0.7413411065494199\n",
      "\n",
      "\n",
      "Iteration 40/1000\n",
      "Accuracy = 68.6472%\n",
      "Loss = 0.6586529654155584\n",
      "\n",
      "\n",
      "Iteration 50/1000\n",
      "Accuracy = 70.6052%\n",
      "Loss = 0.602033807153642\n",
      "\n",
      "\n",
      "Iteration 60/1000\n",
      "Accuracy = 72.2724%\n",
      "Loss = 0.5632419896768377\n",
      "\n",
      "\n",
      "Iteration 70/1000\n",
      "Accuracy = 73.586%\n",
      "Loss = 0.5365621899595447\n",
      "\n",
      "\n",
      "Iteration 80/1000\n",
      "Accuracy = 74.6692%\n",
      "Loss = 0.5177877004996777\n",
      "\n",
      "\n",
      "Iteration 90/1000\n",
      "Accuracy = 75.5716%\n",
      "Loss = 0.5043884068137361\n",
      "\n",
      "\n",
      "Iteration 100/1000\n",
      "Accuracy = 76.24600000000001%\n",
      "Loss = 0.4947060358829992\n",
      "\n",
      "\n",
      "Iteration 110/1000\n",
      "Accuracy = 76.75319999999999%\n",
      "Loss = 0.4876053860621903\n",
      "\n",
      "\n",
      "Iteration 120/1000\n",
      "Accuracy = 77.1572%\n",
      "Loss = 0.4823343512015901\n",
      "\n",
      "\n",
      "Iteration 130/1000\n",
      "Accuracy = 77.4648%\n",
      "Loss = 0.47839048264930734\n",
      "\n",
      "\n",
      "Iteration 140/1000\n",
      "Accuracy = 77.66999999999999%\n",
      "Loss = 0.47541229199431473\n",
      "\n",
      "\n",
      "Iteration 150/1000\n",
      "Accuracy = 77.8476%\n",
      "Loss = 0.47312444389194597\n",
      "\n",
      "\n",
      "Iteration 160/1000\n",
      "Accuracy = 77.9952%\n",
      "Loss = 0.4713351689220755\n",
      "\n",
      "\n",
      "Iteration 170/1000\n",
      "Accuracy = 78.0916%\n",
      "Loss = 0.46991131535618097\n",
      "\n",
      "\n",
      "Iteration 180/1000\n",
      "Accuracy = 78.1768%\n",
      "Loss = 0.468756633640096\n",
      "\n",
      "\n",
      "Iteration 190/1000\n",
      "Accuracy = 78.228%\n",
      "Loss = 0.4678066835442091\n",
      "\n",
      "\n",
      "Iteration 200/1000\n",
      "Accuracy = 78.2824%\n",
      "Loss = 0.46701505470139526\n",
      "\n",
      "\n",
      "Iteration 210/1000\n",
      "Accuracy = 78.3396%\n",
      "Loss = 0.46634662043504277\n",
      "\n",
      "\n",
      "Iteration 220/1000\n",
      "Accuracy = 78.374%\n",
      "Loss = 0.4657765162969389\n",
      "\n",
      "\n",
      "Iteration 230/1000\n",
      "Accuracy = 78.4092%\n",
      "Loss = 0.46528490762424046\n",
      "\n",
      "\n",
      "Iteration 240/1000\n",
      "Accuracy = 78.4408%\n",
      "Loss = 0.464856494205193\n",
      "\n",
      "\n",
      "Iteration 250/1000\n",
      "Accuracy = 78.474%\n",
      "Loss = 0.46448056095008705\n",
      "\n",
      "\n",
      "Iteration 260/1000\n",
      "Accuracy = 78.49080000000001%\n",
      "Loss = 0.4641477886710582\n",
      "\n",
      "\n",
      "Iteration 270/1000\n",
      "Accuracy = 78.50160000000001%\n",
      "Loss = 0.4638508587205741\n",
      "\n",
      "\n",
      "Iteration 280/1000\n",
      "Accuracy = 78.5128%\n",
      "Loss = 0.4635842297113896\n",
      "\n",
      "\n",
      "Iteration 290/1000\n",
      "Accuracy = 78.52799999999999%\n",
      "Loss = 0.4633430008042617\n",
      "\n",
      "\n",
      "Iteration 300/1000\n",
      "Accuracy = 78.5496%\n",
      "Loss = 0.4631236303131673\n",
      "\n",
      "\n",
      "Iteration 310/1000\n",
      "Accuracy = 78.554%\n",
      "Loss = 0.462922574447469\n",
      "\n",
      "\n",
      "Iteration 320/1000\n",
      "Accuracy = 78.56%\n",
      "Loss = 0.46273764604653844\n",
      "\n",
      "\n",
      "Iteration 330/1000\n",
      "Accuracy = 78.5628%\n",
      "Loss = 0.46256663738187925\n",
      "\n",
      "\n",
      "Iteration 340/1000\n",
      "Accuracy = 78.56840000000001%\n",
      "Loss = 0.4624075023839309\n",
      "\n",
      "\n",
      "Iteration 350/1000\n",
      "Accuracy = 78.5864%\n",
      "Loss = 0.4622582543860817\n",
      "\n",
      "\n",
      "Iteration 360/1000\n",
      "Accuracy = 78.58800000000001%\n",
      "Loss = 0.4621182019007311\n",
      "\n",
      "\n",
      "Iteration 370/1000\n",
      "Accuracy = 78.5908%\n",
      "Loss = 0.46198620413219676\n",
      "\n",
      "\n",
      "Iteration 380/1000\n",
      "Accuracy = 78.5872%\n",
      "Loss = 0.46186128447035707\n",
      "\n",
      "\n",
      "Iteration 390/1000\n",
      "Accuracy = 78.594%\n",
      "Loss = 0.4617426043684007\n",
      "\n",
      "\n",
      "Iteration 400/1000\n",
      "Accuracy = 78.59479999999999%\n",
      "Loss = 0.4616294419383137\n",
      "\n",
      "\n",
      "Iteration 410/1000\n",
      "Accuracy = 78.596%\n",
      "Loss = 0.4615206881797193\n",
      "\n",
      "\n",
      "Iteration 420/1000\n",
      "Accuracy = 78.5956%\n",
      "Loss = 0.4614159531857956\n",
      "\n",
      "\n",
      "Iteration 430/1000\n",
      "Accuracy = 78.6092%\n",
      "Loss = 0.4613150819849757\n",
      "\n",
      "\n",
      "Iteration 440/1000\n",
      "Accuracy = 78.6076%\n",
      "Loss = 0.46121774166317964\n",
      "\n",
      "\n",
      "Iteration 450/1000\n",
      "Accuracy = 78.606%\n",
      "Loss = 0.46112350990303125\n",
      "\n",
      "\n",
      "Iteration 460/1000\n",
      "Accuracy = 78.6104%\n",
      "Loss = 0.4610321184403606\n",
      "\n",
      "\n",
      "Iteration 470/1000\n",
      "Accuracy = 78.62360000000001%\n",
      "Loss = 0.46094325445700485\n",
      "\n",
      "\n",
      "Iteration 480/1000\n",
      "Accuracy = 78.6288%\n",
      "Loss = 0.46085644320799524\n",
      "\n",
      "\n",
      "Iteration 490/1000\n",
      "Accuracy = 78.6328%\n",
      "Loss = 0.46077147723485584\n",
      "\n",
      "\n",
      "Iteration 500/1000\n",
      "Accuracy = 78.6348%\n",
      "Loss = 0.46068840992935306\n",
      "\n",
      "\n",
      "Iteration 510/1000\n",
      "Accuracy = 78.63560000000001%\n",
      "Loss = 0.46060720788212295\n",
      "\n",
      "\n",
      "Iteration 520/1000\n",
      "Accuracy = 78.6436%\n",
      "Loss = 0.46052773066127045\n",
      "\n",
      "\n",
      "Iteration 530/1000\n",
      "Accuracy = 78.6412%\n",
      "Loss = 0.46044985476117395\n",
      "\n",
      "\n",
      "Iteration 540/1000\n",
      "Accuracy = 78.648%\n",
      "Loss = 0.4603734714186156\n",
      "\n",
      "\n",
      "Iteration 550/1000\n",
      "Accuracy = 78.6628%\n",
      "Loss = 0.4602984847194332\n",
      "\n",
      "\n",
      "Iteration 560/1000\n",
      "Accuracy = 78.6708%\n",
      "Loss = 0.46022480995496573\n",
      "\n",
      "\n",
      "Iteration 570/1000\n",
      "Accuracy = 78.67920000000001%\n",
      "Loss = 0.46015237219378774\n",
      "\n",
      "\n",
      "Iteration 580/1000\n",
      "Accuracy = 78.6856%\n",
      "Loss = 0.46008110503935123\n",
      "\n",
      "\n",
      "Iteration 590/1000\n",
      "Accuracy = 78.6852%\n",
      "Loss = 0.4600109495483015\n",
      "\n",
      "\n",
      "Iteration 600/1000\n",
      "Accuracy = 78.69080000000001%\n",
      "Loss = 0.4599418532881667\n",
      "\n",
      "\n",
      "Iteration 610/1000\n",
      "Accuracy = 78.69319999999999%\n",
      "Loss = 0.4598737695156793\n",
      "\n",
      "\n",
      "Iteration 620/1000\n",
      "Accuracy = 78.6916%\n",
      "Loss = 0.45980665646022423\n",
      "\n",
      "\n",
      "Iteration 630/1000\n",
      "Accuracy = 78.69279999999999%\n",
      "Loss = 0.4597404145458336\n",
      "\n",
      "\n",
      "Iteration 640/1000\n",
      "Accuracy = 78.69279999999999%\n",
      "Loss = 0.45967502826731355\n",
      "\n",
      "\n",
      "Iteration 650/1000\n",
      "Accuracy = 78.69800000000001%\n",
      "Loss = 0.45961051831462557\n",
      "\n",
      "\n",
      "Iteration 660/1000\n",
      "Accuracy = 78.6972%\n",
      "Loss = 0.45954685690270103\n",
      "\n",
      "\n",
      "Iteration 670/1000\n",
      "Accuracy = 78.7088%\n",
      "Loss = 0.45948401881274425\n",
      "\n",
      "\n",
      "Iteration 680/1000\n",
      "Accuracy = 78.7128%\n",
      "Loss = 0.45942198107277316\n",
      "\n",
      "\n",
      "Iteration 690/1000\n",
      "Accuracy = 78.7192%\n",
      "Loss = 0.45936072267858513\n",
      "\n",
      "\n",
      "Iteration 700/1000\n",
      "Accuracy = 78.7244%\n",
      "Loss = 0.45930022434961315\n",
      "\n",
      "\n",
      "Iteration 710/1000\n",
      "Accuracy = 78.7264%\n",
      "Loss = 0.4592404683156383\n",
      "\n",
      "\n",
      "Iteration 720/1000\n",
      "Accuracy = 78.7328%\n",
      "Loss = 0.4591814381303608\n",
      "\n",
      "\n",
      "Iteration 730/1000\n",
      "Accuracy = 78.73599999999999%\n",
      "Loss = 0.4591231185083831\n",
      "\n",
      "\n",
      "Iteration 740/1000\n",
      "Accuracy = 78.73639999999999%\n",
      "Loss = 0.45906549518282475\n",
      "\n",
      "\n",
      "Iteration 750/1000\n",
      "Accuracy = 78.7368%\n",
      "Loss = 0.45900855478115615\n",
      "\n",
      "\n",
      "Iteration 760/1000\n",
      "Accuracy = 78.7396%\n",
      "Loss = 0.45895228471642324\n",
      "\n",
      "\n",
      "Iteration 770/1000\n",
      "Accuracy = 78.7428%\n",
      "Loss = 0.458896673092551\n",
      "\n",
      "\n",
      "Iteration 780/1000\n",
      "Accuracy = 78.74600000000001%\n",
      "Loss = 0.4588417086219123\n",
      "\n",
      "\n",
      "Iteration 790/1000\n",
      "Accuracy = 78.74839999999999%\n",
      "Loss = 0.45878738055296797\n",
      "\n",
      "\n",
      "Iteration 800/1000\n",
      "Accuracy = 78.7532%\n",
      "Loss = 0.45873367860776254\n",
      "\n",
      "\n",
      "Iteration 810/1000\n",
      "Accuracy = 78.75280000000001%\n",
      "Loss = 0.45868059292715363\n",
      "\n",
      "\n",
      "Iteration 820/1000\n",
      "Accuracy = 78.75720000000001%\n",
      "Loss = 0.458628114023418\n",
      "\n",
      "\n",
      "Iteration 830/1000\n",
      "Accuracy = 78.75919999999999%\n",
      "Loss = 0.458576232738838\n",
      "\n",
      "\n",
      "Iteration 840/1000\n",
      "Accuracy = 78.762%\n",
      "Loss = 0.4585249402099957\n",
      "\n",
      "\n",
      "Iteration 850/1000\n",
      "Accuracy = 78.7628%\n",
      "Loss = 0.4584742278364859\n",
      "\n",
      "\n",
      "Iteration 860/1000\n",
      "Accuracy = 78.7676%\n",
      "Loss = 0.4584240872542546\n",
      "\n",
      "\n",
      "Iteration 870/1000\n",
      "Accuracy = 78.7736%\n",
      "Loss = 0.4583745103121169\n",
      "\n",
      "\n",
      "Iteration 880/1000\n",
      "Accuracy = 78.7744%\n",
      "Loss = 0.45832548905167647\n",
      "\n",
      "\n",
      "Iteration 890/1000\n",
      "Accuracy = 78.7796%\n",
      "Loss = 0.4582770156899395\n",
      "\n",
      "\n",
      "Iteration 900/1000\n",
      "Accuracy = 78.78399999999999%\n",
      "Loss = 0.4582290826045356\n",
      "\n",
      "\n",
      "Iteration 910/1000\n",
      "Accuracy = 78.79119999999999%\n",
      "Loss = 0.45818168232069495\n",
      "\n",
      "\n",
      "Iteration 920/1000\n",
      "Accuracy = 78.7932%\n",
      "Loss = 0.4581348075000536\n",
      "\n",
      "\n",
      "Iteration 930/1000\n",
      "Accuracy = 78.7948%\n",
      "Loss = 0.458088450931748\n",
      "\n",
      "\n",
      "Iteration 940/1000\n",
      "Accuracy = 78.7996%\n",
      "Loss = 0.45804260552373816\n",
      "\n",
      "\n",
      "Iteration 950/1000\n",
      "Accuracy = 78.8088%\n",
      "Loss = 0.4579972642960945\n",
      "\n",
      "\n",
      "Iteration 960/1000\n",
      "Accuracy = 78.8076%\n",
      "Loss = 0.45795242037545736\n",
      "\n",
      "\n",
      "Iteration 970/1000\n",
      "Accuracy = 78.8108%\n",
      "Loss = 0.4579080669894725\n",
      "\n",
      "\n",
      "Iteration 980/1000\n",
      "Accuracy = 78.8092%\n",
      "Loss = 0.4578641974629548\n",
      "\n",
      "\n",
      "Iteration 990/1000\n",
      "Accuracy = 78.8104%\n",
      "Loss = 0.4578208052140795\n",
      "\n",
      "\n",
      "Best Accuracy : 78.81519999999999% reached at epoch 999\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from implementations import reg_logistic_regression, build_poly\n",
    "tX_train_poly = build_poly(tX_train, degree=2, cont_features=cont_features)\n",
    "weights, loss = reg_logistic_regression(ty_train, tX_train_poly, max_iters=1000, lambda_=0.0001, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 24)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import build_poly\n",
    "tX_test_poly = build_poly(tX_test, degree, cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 42)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from implementations import predict_log_reg\n",
    "method = 'reg_logistic_regression'\n",
    "time = datetime.now().strftime('%Y%m%dH%H%M%S')\n",
    "OUTPUT_PATH = f'submissions/submission_{method}_{time}' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_log_reg(weights, tX_test_poly)\n",
    "y_pred[np.where(y_pred == 0)] = -1\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34e9dc8c9cd4c2e3341692c7f5472da17e27c062a0f2ac63648b60e63867ef4a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
