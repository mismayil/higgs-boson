{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y_train, X_train, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import preprocess\n",
    "tX_train, ty_train, tX_test, ty_test, cont_features = preprocess(X_train, y_train, X_test, encodable_threshold=0.5, imputable_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.06833197,  0.40768027, ...,  1.        ,\n",
       "         1.        ,  1.        ],\n",
       "       [ 1.        ,  0.55250482,  0.54013641, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.        ,  3.19515553,  1.09655998, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 1.        ,  0.31931645, -0.13086367, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.        , -0.84532397, -0.30297338, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.        ,  0.66533608, -0.25352276, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 31), (568238, 31))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train.shape, tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy : 71.3685% reached at epoch 99\n",
      "Best Accuracy : 70.804% reached at epoch 99\n",
      "Best Accuracy : 70.895% reached at epoch 99\n",
      "Best Accuracy : 70.48400000000001% reached at epoch 99\n",
      "Best Accuracy : 70.27% reached at epoch 99\n",
      "Lambda = 0.0001\n",
      "Degree = 1\n",
      "Accuracy = 70.82000000000001%\n",
      "F1 Score = 0.6742771435337176\n",
      "Loss = 0.558924704264603\n",
      "\n",
      "\n",
      "Best Accuracy : 71.3965% reached at epoch 99\n",
      "Best Accuracy : 70.8365% reached at epoch 99\n",
      "Best Accuracy : 70.9385% reached at epoch 99\n",
      "Best Accuracy : 70.537% reached at epoch 99\n",
      "Best Accuracy : 70.3045% reached at epoch 99\n",
      "Lambda = 0.001\n",
      "Degree = 1\n",
      "Accuracy = 70.85239999999999%\n",
      "F1 Score = 0.6741895537376316\n",
      "Loss = 0.5583631754453391\n",
      "\n",
      "\n",
      "Best Accuracy : 71.601% reached at epoch 99\n",
      "Best Accuracy : 71.14200000000001% reached at epoch 99\n",
      "Best Accuracy : 71.3315% reached at epoch 99\n",
      "Best Accuracy : 70.9405% reached at epoch 99\n",
      "Best Accuracy : 70.595% reached at epoch 99\n",
      "Lambda = 0.01\n",
      "Degree = 1\n",
      "Accuracy = 71.15799999999999%\n",
      "F1 Score = 0.6740515759507468\n",
      "Loss = 0.5546178846198453\n",
      "\n",
      "\n",
      "Best Accuracy : 71.653% reached at epoch 87\n",
      "Best Accuracy : 71.485% reached at epoch 97\n",
      "Best Accuracy : 71.4275% reached at epoch 89\n",
      "Best Accuracy : 71.509% reached at epoch 97\n",
      "Best Accuracy : 71.2655% reached at epoch 99\n",
      "Lambda = 0.1\n",
      "Degree = 1\n",
      "Accuracy = 71.47279999999999%\n",
      "F1 Score = 0.6814571926042151\n",
      "Loss = 0.5734012338665518\n",
      "\n",
      "\n",
      "Best Accuracy : 69.52000000000001% reached at epoch 22\n",
      "Best Accuracy : 69.475% reached at epoch 30\n",
      "Best Accuracy : 69.3925% reached at epoch 22\n",
      "Best Accuracy : 69.416% reached at epoch 24\n",
      "Best Accuracy : 69.49849999999999% reached at epoch 28\n",
      "Lambda = 1.0\n",
      "Degree = 1\n",
      "Accuracy = 69.4272%\n",
      "F1 Score = 0.6931602114605203\n",
      "Loss = 0.6534689964468281\n",
      "\n",
      "\n",
      "Best Accuracy : 75.7795% reached at epoch 99\n",
      "Best Accuracy : 75.81% reached at epoch 99\n",
      "Best Accuracy : 76.818% reached at epoch 99\n",
      "Best Accuracy : 75.3155% reached at epoch 99\n",
      "Best Accuracy : 77.602% reached at epoch 99\n",
      "Lambda = 0.0001\n",
      "Degree = 2\n",
      "Accuracy = 76.31000000000002%\n",
      "F1 Score = 0.7141854197885193\n",
      "Loss = 0.4971630405204944\n",
      "\n",
      "\n",
      "Best Accuracy : 75.84349999999999% reached at epoch 99\n",
      "Best Accuracy : 75.8605% reached at epoch 99\n",
      "Best Accuracy : 76.85499999999999% reached at epoch 99\n",
      "Best Accuracy : 75.3845% reached at epoch 99\n",
      "Best Accuracy : 77.6425% reached at epoch 99\n",
      "Lambda = 0.001\n",
      "Degree = 2\n",
      "Accuracy = 76.3572%\n",
      "F1 Score = 0.7141072753952591\n",
      "Loss = 0.4958384271479582\n",
      "\n",
      "\n",
      "Best Accuracy : 76.3665% reached at epoch 99\n",
      "Best Accuracy : 76.366% reached at epoch 99\n",
      "Best Accuracy : 77.266% reached at epoch 99\n",
      "Best Accuracy : 75.92750000000001% reached at epoch 99\n",
      "Best Accuracy : 77.8375% reached at epoch 99\n",
      "Lambda = 0.01\n",
      "Degree = 2\n",
      "Accuracy = 76.8172%\n",
      "F1 Score = 0.7128593450427909\n",
      "Loss = 0.4861309771697623\n",
      "\n",
      "\n",
      "Best Accuracy : 76.7725% reached at epoch 98\n",
      "Best Accuracy : 76.80799999999999% reached at epoch 98\n",
      "Best Accuracy : 77.0335% reached at epoch 85\n",
      "Best Accuracy : 76.738% reached at epoch 99\n",
      "Best Accuracy : 77.0275% reached at epoch 84\n",
      "Lambda = 0.1\n",
      "Degree = 2\n",
      "Accuracy = 76.8348%\n",
      "F1 Score = 0.6544426563444778\n",
      "Loss = 0.49998769245249075\n",
      "\n",
      "\n",
      "Best Accuracy : 70.858% reached at epoch 13\n",
      "Best Accuracy : 71.09299999999999% reached at epoch 12\n",
      "Best Accuracy : 70.967% reached at epoch 12\n",
      "Best Accuracy : 70.7015% reached at epoch 12\n",
      "Best Accuracy : 72.00450000000001% reached at epoch 12\n",
      "Lambda = 1.0\n",
      "Degree = 2\n",
      "Accuracy = 70.09759999999999%\n",
      "F1 Score = 0.29526539922912753\n",
      "Loss = 0.5919544300103982\n",
      "\n",
      "\n",
      "Best Accuracy : 74.505% reached at epoch 98\n",
      "Best Accuracy : 75.227% reached at epoch 99\n",
      "Best Accuracy : 76.6425% reached at epoch 99\n",
      "Best Accuracy : 75.315% reached at epoch 98\n",
      "Best Accuracy : 76.4% reached at epoch 99\n",
      "Lambda = 0.0001\n",
      "Degree = 3\n",
      "Accuracy = 74.6528%\n",
      "F1 Score = 0.7054556117351132\n",
      "Loss = 0.7203068427936465\n",
      "\n",
      "\n",
      "Best Accuracy : 74.386% reached at epoch 96\n",
      "Best Accuracy : 75.144% reached at epoch 97\n",
      "Best Accuracy : 76.7165% reached at epoch 99\n",
      "Best Accuracy : 75.6205% reached at epoch 98\n",
      "Best Accuracy : 76.447% reached at epoch 99\n",
      "Lambda = 0.001\n",
      "Degree = 3\n",
      "Accuracy = 74.82239999999999%\n",
      "F1 Score = 0.7045575057791909\n",
      "Loss = 0.720693767778495\n",
      "\n",
      "\n",
      "Best Accuracy : 74.938% reached at epoch 96\n",
      "Best Accuracy : 75.55% reached at epoch 99\n",
      "Best Accuracy : 76.8% reached at epoch 97\n",
      "Best Accuracy : 75.834% reached at epoch 96\n",
      "Best Accuracy : 76.4445% reached at epoch 93\n",
      "Lambda = 0.01\n",
      "Degree = 3\n",
      "Accuracy = 75.2952%\n",
      "F1 Score = 0.7004729484645495\n",
      "Loss = 0.700306999096803\n",
      "\n",
      "\n",
      "Best Accuracy : 75.3265% reached at epoch 95\n",
      "Best Accuracy : 75.81700000000001% reached at epoch 99\n",
      "Best Accuracy : 75.634% reached at epoch 75\n",
      "Best Accuracy : 75.50750000000001% reached at epoch 87\n",
      "Best Accuracy : 75.878% reached at epoch 85\n",
      "Lambda = 0.1\n",
      "Degree = 3\n",
      "Accuracy = 73.7708%\n",
      "F1 Score = 0.6634625842402235\n",
      "Loss = 0.8271099518792745\n",
      "\n",
      "\n",
      "Best Accuracy : 70.9655% reached at epoch 71\n",
      "Best Accuracy : 71.018% reached at epoch 51\n",
      "Best Accuracy : 72.036% reached at epoch 30\n",
      "Best Accuracy : 71.1435% reached at epoch 37\n",
      "Best Accuracy : 70.58250000000001% reached at epoch 14\n",
      "Lambda = 1.0\n",
      "Degree = 3\n",
      "Accuracy = 66.84760000000001%\n",
      "F1 Score = 0.472663486440598\n",
      "Loss = 1.089318594739536\n",
      "\n",
      "\n",
      "Best Accuracy : 67.806% reached at epoch 98\n",
      "Best Accuracy : 67.5995% reached at epoch 99\n",
      "Best Accuracy : 74.8965% reached at epoch 83\n",
      "Best Accuracy : 67.8245% reached at epoch 98\n",
      "Best Accuracy : 67.3305% reached at epoch 97\n",
      "Lambda = 0.0001\n",
      "Degree = 4\n",
      "Accuracy = 68.63879999999999%\n",
      "F1 Score = 0.654276095180681\n",
      "Loss = 3.408529178711563\n",
      "\n",
      "\n",
      "Best Accuracy : 67.9785% reached at epoch 97\n",
      "Best Accuracy : 67.61% reached at epoch 99\n",
      "Best Accuracy : 75.37549999999999% reached at epoch 99\n",
      "Best Accuracy : 67.766% reached at epoch 97\n",
      "Best Accuracy : 67.3545% reached at epoch 99\n",
      "Lambda = 0.001\n",
      "Degree = 4\n",
      "Accuracy = 68.39720000000001%\n",
      "F1 Score = 0.656977020378154\n",
      "Loss = 3.4019512971256836\n",
      "\n",
      "\n",
      "Best Accuracy : 69.9465% reached at epoch 99\n",
      "Best Accuracy : 68.856% reached at epoch 99\n",
      "Best Accuracy : 74.99749999999999% reached at epoch 98\n",
      "Best Accuracy : 69.33399999999999% reached at epoch 99\n",
      "Best Accuracy : 68.508% reached at epoch 97\n",
      "Lambda = 0.01\n",
      "Degree = 4\n",
      "Accuracy = 70.22720000000001%\n",
      "F1 Score = 0.6772505656512701\n",
      "Loss = 2.7688027281860785\n",
      "\n",
      "\n",
      "Best Accuracy : 71.9205% reached at epoch 71\n",
      "Best Accuracy : 74.207% reached at epoch 77\n",
      "Best Accuracy : 74.376% reached at epoch 54\n",
      "Best Accuracy : 73.4785% reached at epoch 74\n",
      "Best Accuracy : 72.096% reached at epoch 79\n",
      "Lambda = 0.1\n",
      "Degree = 4\n",
      "Accuracy = 60.93920000000001%\n",
      "F1 Score = 0.6889892472302994\n",
      "Loss = 4.66016783815148\n",
      "\n",
      "\n",
      "Best Accuracy : 66.9855% reached at epoch 84\n",
      "Best Accuracy : 68.735% reached at epoch 32\n",
      "Best Accuracy : 70.1835% reached at epoch 18\n",
      "Best Accuracy : 68.292% reached at epoch 16\n",
      "Best Accuracy : 68.67% reached at epoch 80\n",
      "Lambda = 1.0\n",
      "Degree = 4\n",
      "Accuracy = 54.2768%\n",
      "F1 Score = 0.609266416391889\n",
      "Loss = 5.175975729378495\n",
      "\n",
      "\n",
      "Best Accuracy : 56.381499999999996% reached at epoch 36\n",
      "Best Accuracy : 56.389500000000005% reached at epoch 30\n",
      "Best Accuracy : 72.1545% reached at epoch 39\n",
      "Best Accuracy : 56.293000000000006% reached at epoch 32\n",
      "Best Accuracy : 56.688% reached at epoch 32\n",
      "Lambda = 0.0001\n",
      "Degree = 5\n",
      "Accuracy = 57.178%\n",
      "F1 Score = 0.6344570104395849\n",
      "Loss = 7.893482763804839\n",
      "\n",
      "\n",
      "Best Accuracy : 56.4415% reached at epoch 36\n",
      "Best Accuracy : 56.428% reached at epoch 45\n",
      "Best Accuracy : 72.031% reached at epoch 66\n",
      "Best Accuracy : 56.277% reached at epoch 37\n",
      "Best Accuracy : 56.686% reached at epoch 32\n",
      "Lambda = 0.001\n",
      "Degree = 5\n",
      "Accuracy = 58.3768%\n",
      "F1 Score = 0.6376134851183084\n",
      "Loss = 7.735443645027587\n",
      "\n",
      "\n",
      "Best Accuracy : 56.4685% reached at epoch 35\n",
      "Best Accuracy : 56.407% reached at epoch 46\n",
      "Best Accuracy : 72.32300000000001% reached at epoch 72\n",
      "Best Accuracy : 56.305499999999995% reached at epoch 23\n",
      "Best Accuracy : 56.69499999999999% reached at epoch 31\n",
      "Lambda = 0.01\n",
      "Degree = 5\n",
      "Accuracy = 58.412%\n",
      "F1 Score = 0.6334294996592646\n",
      "Loss = 7.804412297095718\n",
      "\n",
      "\n",
      "Best Accuracy : 56.5225% reached at epoch 91\n",
      "Best Accuracy : 56.613% reached at epoch 96\n",
      "Best Accuracy : 71.721% reached at epoch 47\n",
      "Best Accuracy : 56.573499999999996% reached at epoch 87\n",
      "Best Accuracy : 56.759% reached at epoch 26\n",
      "Lambda = 0.1\n",
      "Degree = 5\n",
      "Accuracy = 59.19840000000001%\n",
      "F1 Score = 0.6447359548311726\n",
      "Loss = 7.2342090602629465\n",
      "\n",
      "\n",
      "Best Accuracy : 58.957% reached at epoch 50\n",
      "Best Accuracy : 65.9625% reached at epoch 28\n",
      "Best Accuracy : 66.6125% reached at epoch 95\n",
      "Best Accuracy : 61.906499999999994% reached at epoch 25\n",
      "Best Accuracy : 66.9225% reached at epoch 84\n",
      "Lambda = 1.0\n",
      "Degree = 5\n",
      "Accuracy = 54.79200000000001%\n",
      "F1 Score = 0.6301756118941325\n",
      "Loss = 6.95143976659693\n",
      "\n",
      "\n",
      "Best Accuracy : 65.8335% reached at epoch 1\n",
      "Best Accuracy : 65.632% reached at epoch 1\n",
      "Best Accuracy : 67.1975% reached at epoch 99\n",
      "Best Accuracy : 65.9735% reached at epoch 1\n",
      "Best Accuracy : 65.86% reached at epoch 1\n",
      "Lambda = 0.0001\n",
      "Degree = 6\n",
      "Accuracy = 47.428%\n",
      "F1 Score = 0.7366129989139241\n",
      "Loss = 10.420577022160797\n",
      "\n",
      "\n",
      "Best Accuracy : 65.8335% reached at epoch 1\n",
      "Best Accuracy : 65.6315% reached at epoch 1\n",
      "Best Accuracy : 66.656% reached at epoch 21\n",
      "Best Accuracy : 65.9735% reached at epoch 1\n",
      "Best Accuracy : 65.86% reached at epoch 1\n",
      "Lambda = 0.001\n",
      "Degree = 6\n",
      "Accuracy = 46.952%\n",
      "F1 Score = 0.739691943777701\n",
      "Loss = 10.51403333438668\n",
      "\n",
      "\n",
      "Best Accuracy : 65.8305% reached at epoch 1\n",
      "Best Accuracy : 65.629% reached at epoch 1\n",
      "Best Accuracy : 67.109% reached at epoch 64\n",
      "Best Accuracy : 65.971% reached at epoch 1\n",
      "Best Accuracy : 65.861% reached at epoch 1\n",
      "Lambda = 0.01\n",
      "Degree = 6\n",
      "Accuracy = 47.661199999999994%\n",
      "F1 Score = 0.7310685750240287\n",
      "Loss = 10.36060701392658\n",
      "\n",
      "\n",
      "Best Accuracy : 65.8285% reached at epoch 1\n",
      "Best Accuracy : 65.63% reached at epoch 1\n",
      "Best Accuracy : 67.23649999999999% reached at epoch 81\n",
      "Best Accuracy : 65.955% reached at epoch 1\n",
      "Best Accuracy : 65.857% reached at epoch 1\n",
      "Lambda = 0.1\n",
      "Degree = 6\n",
      "Accuracy = 49.8832%\n",
      "F1 Score = 0.7116444108328384\n",
      "Loss = 9.895309452049469\n",
      "\n",
      "\n",
      "Best Accuracy : 67.3525% reached at epoch 39\n",
      "Best Accuracy : 66.5035% reached at epoch 39\n",
      "Best Accuracy : 67.84400000000001% reached at epoch 10\n",
      "Best Accuracy : 65.827% reached at epoch 1\n",
      "Best Accuracy : 66.9005% reached at epoch 39\n",
      "Lambda = 1.0\n",
      "Degree = 6\n",
      "Accuracy = 52.384%\n",
      "F1 Score = 0.6653343369539633\n",
      "Loss = 9.37748368063637\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (5,) and (30,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rs/c9bqjyq95q59ngc5v1t0gz_00000gn/T/ipykernel_7773/503247270.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimplementations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogistic_regression_cv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mty_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcont_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/EPFL/cs-433/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression_cv\u001b[0;34m(y, tx, lambdas, degrees, k_fold, max_iters, gamma, degree, cont_features, seed, verbose)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_f1s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \"\"\"\n\u001b[1;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    502\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (5,) and (30,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from implementations import logistic_regression_cv\n",
    "weights, loss, lambda_, degree, accuracy, f1 = logistic_regression_cv(ty_train, tX_train, cont_features=cont_features, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.24183977],\n",
       "        [-0.26544805],\n",
       "        [ 0.37138584],\n",
       "        [ 0.30515328],\n",
       "        [ 0.91327956],\n",
       "        [ 0.17158336],\n",
       "        [ 0.53986589],\n",
       "        [-0.36120854],\n",
       "        [ 0.19929079],\n",
       "        [ 0.35773507],\n",
       "        [-0.06072972],\n",
       "        [ 0.29187704],\n",
       "        [ 0.12951111],\n",
       "        [ 0.11767485],\n",
       "        [ 0.33113117],\n",
       "        [ 0.20211105],\n",
       "        [ 0.23329362],\n",
       "        [-0.09106707],\n",
       "        [ 0.21601779],\n",
       "        [-0.04322265],\n",
       "        [-0.05206696],\n",
       "        [ 0.23535913],\n",
       "        [ 0.1280109 ],\n",
       "        [ 0.27909552],\n",
       "        [-0.22777135],\n",
       "        [ 0.03490579],\n",
       "        [-0.60286866],\n",
       "        [ 0.16207475],\n",
       "        [-0.32180454],\n",
       "        [-0.01908622],\n",
       "        [-0.02063272],\n",
       "        [-0.01574469],\n",
       "        [ 0.31183612],\n",
       "        [-0.03238049],\n",
       "        [-0.04039736],\n",
       "        [ 0.07057688],\n",
       "        [ 0.02495367],\n",
       "        [-0.17392018],\n",
       "        [ 0.02440933],\n",
       "        [-0.00554544],\n",
       "        [-0.10049804],\n",
       "        [-0.03536644],\n",
       "        [-0.26242361],\n",
       "        [-0.11389735],\n",
       "        [-0.24725652],\n",
       "        [-0.08161609],\n",
       "        [ 0.17138845],\n",
       "        [-0.14844606]]),\n",
       " 0.48650376615578345,\n",
       " 0.01,\n",
       " 2,\n",
       " 76.80879999999999,\n",
       " 0.7134537923821604)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, loss, lambda_, degree, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/1000\n",
      "Accuracy = 34.266799999999996%\n",
      "Loss = 7.512752747308656\n",
      "\n",
      "\n",
      "Iteration 10/1000\n",
      "Accuracy = 57.414%\n",
      "Loss = 1.3267212383098728\n",
      "\n",
      "\n",
      "Iteration 20/1000\n",
      "Accuracy = 65.9532%\n",
      "Loss = 0.7310928001903751\n",
      "\n",
      "\n",
      "Iteration 30/1000\n",
      "Accuracy = 69.5332%\n",
      "Loss = 0.605709180975117\n",
      "\n",
      "\n",
      "Iteration 40/1000\n",
      "Accuracy = 72.36359999999999%\n",
      "Loss = 0.5472410645509806\n",
      "\n",
      "\n",
      "Iteration 50/1000\n",
      "Accuracy = 74.422%\n",
      "Loss = 0.5174312370894415\n",
      "\n",
      "\n",
      "Iteration 60/1000\n",
      "Accuracy = 75.69839999999999%\n",
      "Loss = 0.5021529876141859\n",
      "\n",
      "\n",
      "Iteration 70/1000\n",
      "Accuracy = 76.472%\n",
      "Loss = 0.49377191008399235\n",
      "\n",
      "\n",
      "Iteration 80/1000\n",
      "Accuracy = 76.92999999999999%\n",
      "Loss = 0.48894913573336474\n",
      "\n",
      "\n",
      "Iteration 90/1000\n",
      "Accuracy = 77.216%\n",
      "Loss = 0.48605760412051635\n",
      "\n",
      "\n",
      "Iteration 100/1000\n",
      "Accuracy = 77.3864%\n",
      "Loss = 0.4842656778429128\n",
      "\n",
      "\n",
      "Iteration 110/1000\n",
      "Accuracy = 77.4828%\n",
      "Loss = 0.4831340905769937\n",
      "\n",
      "\n",
      "Iteration 120/1000\n",
      "Accuracy = 77.58040000000001%\n",
      "Loss = 0.4824060934123485\n",
      "\n",
      "\n",
      "Iteration 130/1000\n",
      "Accuracy = 77.6628%\n",
      "Loss = 0.48192887450566024\n",
      "\n",
      "\n",
      "Iteration 140/1000\n",
      "Accuracy = 77.69359999999999%\n",
      "Loss = 0.4816108880357878\n",
      "\n",
      "\n",
      "Iteration 150/1000\n",
      "Accuracy = 77.7012%\n",
      "Loss = 0.481396359416806\n",
      "\n",
      "\n",
      "Iteration 160/1000\n",
      "Accuracy = 77.7408%\n",
      "Loss = 0.48124931523880565\n",
      "\n",
      "\n",
      "Iteration 170/1000\n",
      "Accuracy = 77.7552%\n",
      "Loss = 0.48114670774063634\n",
      "\n",
      "\n",
      "Iteration 180/1000\n",
      "Accuracy = 77.7784%\n",
      "Loss = 0.4810736133378208\n",
      "\n",
      "\n",
      "Iteration 190/1000\n",
      "Accuracy = 77.7872%\n",
      "Loss = 0.48102029335616847\n",
      "\n",
      "\n",
      "Iteration 200/1000\n",
      "Accuracy = 77.8%\n",
      "Loss = 0.48098014862817634\n",
      "\n",
      "\n",
      "Iteration 210/1000\n",
      "Accuracy = 77.8064%\n",
      "Loss = 0.48094899292320836\n",
      "\n",
      "\n",
      "Iteration 220/1000\n",
      "Accuracy = 77.802%\n",
      "Loss = 0.4809242914482023\n",
      "\n",
      "\n",
      "Iteration 230/1000\n",
      "Accuracy = 77.8048%\n",
      "Loss = 0.48090418099694804\n",
      "\n",
      "\n",
      "Iteration 240/1000\n",
      "Accuracy = 77.8124%\n",
      "Loss = 0.4808874167556199\n",
      "\n",
      "\n",
      "Iteration 250/1000\n",
      "Accuracy = 77.8052%\n",
      "Loss = 0.4808731612705652\n",
      "\n",
      "\n",
      "Iteration 260/1000\n",
      "Accuracy = 77.8104%\n",
      "Loss = 0.4808608453840166\n",
      "\n",
      "\n",
      "Iteration 270/1000\n",
      "Accuracy = 77.812%\n",
      "Loss = 0.48085007656670825\n",
      "\n",
      "\n",
      "Iteration 280/1000\n",
      "Accuracy = 77.8092%\n",
      "Loss = 0.48084057855958234\n",
      "\n",
      "\n",
      "Iteration 290/1000\n",
      "Accuracy = 77.8088%\n",
      "Loss = 0.4808321517216563\n",
      "\n",
      "\n",
      "Iteration 300/1000\n",
      "Accuracy = 77.80799999999999%\n",
      "Loss = 0.48082464706679917\n",
      "\n",
      "\n",
      "Iteration 310/1000\n",
      "Accuracy = 77.8052%\n",
      "Loss = 0.480817949334588\n",
      "\n",
      "\n",
      "Iteration 320/1000\n",
      "Accuracy = 77.804%\n",
      "Loss = 0.48081196600511317\n",
      "\n",
      "\n",
      "Iteration 330/1000\n",
      "Accuracy = 77.8092%\n",
      "Loss = 0.48080662020743714\n",
      "\n",
      "\n",
      "Iteration 340/1000\n",
      "Accuracy = 77.81439999999999%\n",
      "Loss = 0.48080184616376076\n",
      "\n",
      "\n",
      "Iteration 350/1000\n",
      "Accuracy = 77.8184%\n",
      "Loss = 0.4807975862726314\n",
      "\n",
      "\n",
      "Iteration 360/1000\n",
      "Accuracy = 77.822%\n",
      "Loss = 0.4807937892417266\n",
      "\n",
      "\n",
      "Iteration 370/1000\n",
      "Accuracy = 77.8224%\n",
      "Loss = 0.48079040888502966\n",
      "\n",
      "\n",
      "Iteration 380/1000\n",
      "Accuracy = 77.8204%\n",
      "Loss = 0.48078740333469605\n",
      "\n",
      "\n",
      "Iteration 390/1000\n",
      "Accuracy = 77.8208%\n",
      "Loss = 0.4807847345074099\n",
      "\n",
      "\n",
      "Iteration 400/1000\n",
      "Accuracy = 77.8208%\n",
      "Loss = 0.48078236772383687\n",
      "\n",
      "\n",
      "Iteration 410/1000\n",
      "Accuracy = 77.8212%\n",
      "Loss = 0.48078027141814017\n",
      "\n",
      "\n",
      "Iteration 420/1000\n",
      "Accuracy = 77.8224%\n",
      "Loss = 0.4807784168992945\n",
      "\n",
      "\n",
      "Iteration 430/1000\n",
      "Accuracy = 77.8256%\n",
      "Loss = 0.4807767781417445\n",
      "\n",
      "\n",
      "Iteration 440/1000\n",
      "Accuracy = 77.82679999999999%\n",
      "Loss = 0.48077533159283975\n",
      "\n",
      "\n",
      "Iteration 450/1000\n",
      "Accuracy = 77.82639999999999%\n",
      "Loss = 0.48077405599054895\n",
      "\n",
      "\n",
      "Iteration 460/1000\n",
      "Accuracy = 77.82759999999999%\n",
      "Loss = 0.4807729321885317\n",
      "\n",
      "\n",
      "Iteration 470/1000\n",
      "Accuracy = 77.828%\n",
      "Loss = 0.4807719429876624\n",
      "\n",
      "\n",
      "Iteration 480/1000\n",
      "Accuracy = 77.82719999999999%\n",
      "Loss = 0.4807710729741304\n",
      "\n",
      "\n",
      "Iteration 490/1000\n",
      "Accuracy = 77.82679999999999%\n",
      "Loss = 0.48077030836469015\n",
      "\n",
      "\n",
      "Iteration 500/1000\n",
      "Accuracy = 77.82719999999999%\n",
      "Loss = 0.4807696368597349\n",
      "\n",
      "\n",
      "Iteration 510/1000\n",
      "Accuracy = 77.8284%\n",
      "Loss = 0.48076904750479255\n",
      "\n",
      "\n",
      "Iteration 520/1000\n",
      "Accuracy = 77.8296%\n",
      "Loss = 0.4807685305608721\n",
      "\n",
      "\n",
      "Iteration 530/1000\n",
      "Accuracy = 77.83%\n",
      "Loss = 0.4807680773838995\n",
      "\n",
      "\n",
      "Iteration 540/1000\n",
      "Accuracy = 77.8304%\n",
      "Loss = 0.48076768031329126\n",
      "\n",
      "\n",
      "Iteration 550/1000\n",
      "Accuracy = 77.8292%\n",
      "Loss = 0.4807673325695497\n",
      "\n",
      "\n",
      "Iteration 560/1000\n",
      "Accuracy = 77.8304%\n",
      "Loss = 0.4807670281606273\n",
      "\n",
      "\n",
      "Iteration 570/1000\n",
      "Accuracy = 77.83%\n",
      "Loss = 0.4807667617966976\n",
      "\n",
      "\n",
      "Iteration 580/1000\n",
      "Accuracy = 77.8304%\n",
      "Loss = 0.48076652881290177\n",
      "\n",
      "\n",
      "Iteration 590/1000\n",
      "Accuracy = 77.8316%\n",
      "Loss = 0.4807663250995812\n",
      "\n",
      "\n",
      "Iteration 600/1000\n",
      "Accuracy = 77.83200000000001%\n",
      "Loss = 0.4807661470394795\n",
      "\n",
      "\n",
      "Iteration 610/1000\n",
      "Accuracy = 77.8324%\n",
      "Loss = 0.48076599145139015\n",
      "\n",
      "\n",
      "Iteration 620/1000\n",
      "Accuracy = 77.8312%\n",
      "Loss = 0.48076585553972084\n",
      "\n",
      "\n",
      "Iteration 630/1000\n",
      "Accuracy = 77.8316%\n",
      "Loss = 0.4807657368494669\n",
      "\n",
      "\n",
      "Iteration 640/1000\n",
      "Accuracy = 77.8316%\n",
      "Loss = 0.480765633226097\n",
      "\n",
      "\n",
      "Iteration 650/1000\n",
      "Accuracy = 77.8316%\n",
      "Loss = 0.4807655427798895\n",
      "\n",
      "\n",
      "Iteration 660/1000\n",
      "Accuracy = 77.8316%\n",
      "Loss = 0.4807654638542788\n",
      "\n",
      "\n",
      "Iteration 670/1000\n",
      "Accuracy = 77.8308%\n",
      "Loss = 0.480765394997812\n",
      "\n",
      "\n",
      "Iteration 680/1000\n",
      "Accuracy = 77.8304%\n",
      "Loss = 0.4807653349393306\n",
      "\n",
      "\n",
      "Iteration 690/1000\n",
      "Accuracy = 77.8308%\n",
      "Loss = 0.4807652825660451\n",
      "\n",
      "\n",
      "Iteration 700/1000\n",
      "Accuracy = 77.8308%\n",
      "Loss = 0.4807652369041836\n",
      "\n",
      "\n",
      "Iteration 710/1000\n",
      "Accuracy = 77.8308%\n",
      "Loss = 0.4807651971019295\n",
      "\n",
      "\n",
      "Iteration 720/1000\n",
      "Accuracy = 77.8308%\n",
      "Loss = 0.4807651624143977\n",
      "\n",
      "\n",
      "Iteration 730/1000\n",
      "Accuracy = 77.8308%\n",
      "Loss = 0.4807651321904106\n",
      "\n",
      "\n",
      "Iteration 740/1000\n",
      "Accuracy = 77.8308%\n",
      "Loss = 0.480765105860874\n",
      "\n",
      "\n",
      "Iteration 750/1000\n",
      "Accuracy = 77.8308%\n",
      "Loss = 0.48076508292856446\n",
      "\n",
      "\n",
      "Iteration 760/1000\n",
      "Accuracy = 77.8308%\n",
      "Loss = 0.48076506295915983\n",
      "\n",
      "\n",
      "Iteration 770/1000\n",
      "Accuracy = 77.8308%\n",
      "Loss = 0.4807650455733732\n",
      "\n",
      "\n",
      "Iteration 780/1000\n",
      "Accuracy = 77.8312%\n",
      "Loss = 0.48076503044005164\n",
      "\n",
      "\n",
      "Iteration 790/1000\n",
      "Accuracy = 77.8312%\n",
      "Loss = 0.4807650172701287\n",
      "\n",
      "\n",
      "Iteration 800/1000\n",
      "Accuracy = 77.8308%\n",
      "Loss = 0.48076500581132614\n",
      "\n",
      "\n",
      "Iteration 810/1000\n",
      "Accuracy = 77.8308%\n",
      "Loss = 0.4807649958435137\n",
      "\n",
      "\n",
      "Iteration 820/1000\n",
      "Accuracy = 77.8308%\n",
      "Loss = 0.48076498717464866\n",
      "\n",
      "\n",
      "Iteration 830/1000\n",
      "Accuracy = 77.8312%\n",
      "Loss = 0.48076497963721976\n",
      "\n",
      "\n",
      "Iteration 840/1000\n",
      "Accuracy = 77.8312%\n",
      "Loss = 0.4807649730851404\n",
      "\n",
      "\n",
      "Iteration 850/1000\n",
      "Accuracy = 77.8312%\n",
      "Loss = 0.48076496739102825\n",
      "\n",
      "\n",
      "Iteration 860/1000\n",
      "Accuracy = 77.8312%\n",
      "Loss = 0.48076496244382966\n",
      "\n",
      "\n",
      "Iteration 870/1000\n",
      "Accuracy = 77.8312%\n",
      "Loss = 0.48076495814674186\n",
      "\n",
      "\n",
      "Iteration 880/1000\n",
      "Accuracy = 77.8312%\n",
      "Loss = 0.48076495441539724\n",
      "\n",
      "\n",
      "Iteration 890/1000\n",
      "Accuracy = 77.8312%\n",
      "Loss = 0.4807649511762785\n",
      "\n",
      "\n",
      "Iteration 900/1000\n",
      "Accuracy = 77.8312%\n",
      "Loss = 0.4807649483653326\n",
      "\n",
      "\n",
      "Iteration 910/1000\n",
      "Accuracy = 77.8312%\n",
      "Loss = 0.48076494592676206\n",
      "\n",
      "\n",
      "Iteration 920/1000\n",
      "Accuracy = 77.8316%\n",
      "Loss = 0.48076494381196827\n",
      "\n",
      "\n",
      "Iteration 930/1000\n",
      "Accuracy = 77.8316%\n",
      "Loss = 0.4807649419786306\n",
      "\n",
      "\n",
      "Iteration 940/1000\n",
      "Accuracy = 77.8316%\n",
      "Loss = 0.4807649403899002\n",
      "\n",
      "\n",
      "Iteration 950/1000\n",
      "Accuracy = 77.8316%\n",
      "Loss = 0.4807649390136998\n",
      "\n",
      "\n",
      "Iteration 960/1000\n",
      "Accuracy = 77.8316%\n",
      "Loss = 0.4807649378221094\n",
      "\n",
      "\n",
      "Iteration 970/1000\n",
      "Accuracy = 77.8316%\n",
      "Loss = 0.48076493679083265\n",
      "\n",
      "\n",
      "Iteration 980/1000\n",
      "Accuracy = 77.8316%\n",
      "Loss = 0.48076493589873043\n",
      "\n",
      "\n",
      "Iteration 990/1000\n",
      "Accuracy = 77.8316%\n",
      "Loss = 0.48076493512741425\n",
      "\n",
      "\n",
      "Best Accuracy : 77.8324% reached at epoch 601\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from implementations import logistic_regression, build_poly\n",
    "tX_train_poly = build_poly(tX_train, degree=2, cont_features=cont_features)\n",
    "loss, weights = logistic_regression(ty_train, tX_train_poly, max_iters=1000, lambda_=0.05, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 34)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import build_poly\n",
    "tX_test = build_poly(tX_test, degree, cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 48)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from implementations import predict\n",
    "method = 'logistic_regression'\n",
    "time = datetime.now().strftime('%Y%m%dH%H%M%S')\n",
    "OUTPUT_PATH = f'submissions/submission_{method}_{time}' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict(weights, tX_test)\n",
    "y_pred[np.where(y_pred == 0)] = -1\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34e9dc8c9cd4c2e3341692c7f5472da17e27c062a0f2ac63648b60e63867ef4a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
